+ [author](http://nsddd.top)

# ç¬¬66èŠ‚ Kubernetes äºŒæ¬¡å¼€å‘ CRD å­¦ä¹ 

<div><a href = '65.md' style='float:left'>â¬†ï¸ä¸Šä¸€èŠ‚ğŸ”—  </a><a href = '67.md' style='float: right'>  â¬‡ï¸ä¸‹ä¸€èŠ‚ğŸ”—</a></div>
<br>

> â¤ï¸ğŸ’•ğŸ’•æ–°æ—¶ä»£æ‹¥æŠ±äº‘åŸç”Ÿï¼Œäº‘åŸç”Ÿå…·æœ‰ç¯å¢ƒç»Ÿä¸€ã€æŒ‰éœ€ä»˜è´¹ã€å³å¼€å³ç”¨ã€ç¨³å®šæ€§å¼ºç‰¹ç‚¹ã€‚Myblog:[http://nsddd.top](http://nsddd.top/)

---
[TOC]

## å‰è¨€

sealos ä½¿ç”¨äº†å¤§é‡çš„ CRD å¯¹å…¶æ‰©å±•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ CRD æ¥ Kubernetes

+ [kubebuilder](https://github.com/kubernetes-sigs/kubebuilder)
+ [å®˜æ–¹å­¦ä¹ æ–‡æ¡£](https://book.kubebuilder.io/)

é™¤æ­¤ä¹‹å¤–ï¼Œè¿™é‡Œæœ‰ä¸€ä»½å®˜æ–¹ CRD  [æ¡ˆä¾‹ ~](https://github.com/kubernetes/sample-controller)

CRDæ˜¯Kubernetesä¸ºæé«˜å¯æ‰©å±•æ€§ï¼Œè®©å¼€å‘è€…å»è‡ªå®šä¹‰èµ„æºï¼ˆå¦‚Deploymentï¼ŒStatefulSetç­‰ï¼‰çš„ä¸€ç§æ–¹æ³•ã€‚

```
Operator=CRD+Controller
```

CRDä»…ä»…æ˜¯èµ„æºçš„å®šä¹‰ï¼Œè€ŒControllerå¯ä»¥å»ç›‘å¬CRDçš„CRUDäº‹ä»¶æ¥æ·»åŠ è‡ªå®šä¹‰ä¸šåŠ¡é€»è¾‘ã€‚

å¦‚æœè¯´åªæ˜¯å¯¹CRDå®ä¾‹è¿›è¡Œ `CRUD` çš„è¯ï¼Œä¸éœ€è¦ `Controller` ä¹Ÿæ˜¯å¯ä»¥å®ç°çš„ï¼Œåªæ˜¯åªæœ‰æ•°æ®ï¼Œæ²¡æœ‰é’ˆå¯¹æ•°æ®çš„æ“ä½œã€‚

å°±æ‹¿å®˜æ–¹çš„ CRD æ¡ˆä¾‹ï¼ˆsample-controllerï¼‰æ¥è¯´ï¼Œå¦‚æœæ²¡æœ‰è¿è¡Œè¿™ä¸ªç¨‹åºï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ examples æ¡ˆä¾‹ä¸­çš„ yaml æ–‡ä»¶ï¼Œåˆ›å»º CRDï¼Œä»¥åŠ CRã€‚åªä¸è¿‡æ²¡æœ‰åŠæ³•é’ˆå¯¹ Pod å’Œ deployment ç­‰ å†…ç½®çš„ API èµ„æºå¯¹è±¡è¿›è¡Œ CRUD æ“ä½œã€‚

```bash
â¯ k get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           9h
â¯ ./ctrl -kubeconfig ~/.kube/config  -logtostderr=true
â¯ k get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
example-foo        1/1     1            1           4s
nginx-deployment   3/3     3            3           9h
```



###  é¡¹ç›® demo

reviewï¼š[@muzi502](https://github.com/muzi502)

author:  [@cubxxw](https://github.com/cubxxw)

> è¿™ç¯‡æ–‡ç« å°†å‚è€ƒå„ä¸ªåšå®¢å’Œ kubebuilder å®˜æ–¹æ–‡æ¡£ ä»¥åŠ [kubernetes/sample-controller](https://github.com/kubernetes/sample-controller)  è¿›è¡Œå­¦ä¹ ï¼Œæœ€åå®è·µä¸€ä¸ªé¡¹ç›®çš„æ­¥éª¤ï¼Œå¯¹é™æ€åšå®¢ï¼ˆ[docker.nsddd.top](https://docker.nsddd.top/) æˆ–è€… [go.nsddd.top](https://go.nsddd.top/)) è¿›è¡Œ CRDï¼Œå½¢æˆå­¦ä¹ é—­ç¯~
>
> 1. åˆ›å»ºè‡ªå®šä¹‰APIå¯¹è±¡ï¼ˆCustom Resource Definitionï¼‰ï¼Œåä¸ºBlogï¼›
> 2. ç”¨ä»£ç ç”Ÿæˆå·¥å…·ç”Ÿæˆinformerå’Œclientç›¸å…³ä»£ç ï¼›
> 3. åˆ›å»ºå¹¶è¿è¡Œè‡ªå®šä¹‰æ§åˆ¶å™¨ï¼Œk8sç¯å¢ƒä¸­æ‰€æœ‰ Blog ç›¸å…³çš„"å¢ã€åˆ ã€æ”¹"æ“ä½œéƒ½ä¼šè¢«æ­¤æ§åˆ¶å™¨ç›‘å¬åˆ°ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚åœ¨æ§åˆ¶å™¨ä¸­ç¼–å†™ä¸šåŠ¡ä»£ç ï¼›



### Operator åŠŸèƒ½è®¾è®¡

å€ŸåŠ© Operator å®Œæˆå’Œä¼ä¸šå†…éƒ¨æ³¨å†Œä¸­å¿ƒçš„æ‰“é€š

**Operator å¼€å‘ SDK æœ‰ 2 ä¸ªé€‰æ‹©ï¼š**

+ kubebuilder
+ operator sdk

æ³¨æ„ï¼šåœ¨æœ¬è´¨ä¸Šå…¶å®éƒ½æ˜¯åœ¨ K8s æ§åˆ¶å™¨è¿è¡Œæ—¶ä¸Šçš„å°è£…ï¼Œä¸»è¦éƒ½æ˜¯è„šæ‰‹æ¶çš„ç”Ÿæˆï¼Œä½¿ç”¨ä½“éªŒç›¸å·®ä¸å¤§ã€‚

ä½†æ˜¯æœ‰æ„æ€çš„æ˜¯ï¼ŒKubebuilder çš„ç»´æŠ¤æ–¹æ˜¯ï¼škubernetes-sigsï¼Œæ‰€ä»¥æ›´å—äººå…³æ³¨ã€‚

åº•å±‚éƒ½æ˜¯åŸºäº k8s æ§åˆ¶å™¨è¿è¡Œæ—¶å°è£…ï¼Œä¸åŒçš„æ˜¯ kubebuilder æ—©æœŸåŒ…å« CRDå’Œ è‡ªå®šä¹‰ Controller å¼€å‘ã€‚ä½†æ˜¯ operator-sdk æ—©æœŸä¸åŒ…å« CRD å¼€å‘ï¼Œä½†æ˜¯ç°åœ¨ä¹Ÿæ˜¯èåˆäº†ã€‚

**CRD å…è®¸ä½ å®šä¹‰è‡ªå·±çš„ Kubernetes API å¯¹è±¡ï¼Œè€Œè‡ªå®šä¹‰æ§åˆ¶å™¨å¯ä»¥ç›‘å¬è¿™äº›å¯¹è±¡çš„äº‹ä»¶å¹¶æ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚**



### kubebuilder æ¶æ„å›¾

**å›¾ç‰‡æ¥è‡ªå®˜ç½‘ç«™ï¼š**

![image-20230408102740099](http://sm.nsddd.top/sm202304081027380.png)





## installation kubebuilder

å®‰è£…kubebuilderï¼š

```bash
â¯ curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH)

â¯ chmod +x kubebuilder && mv kubebuilder /usr/local/bin/
```

æˆ–è€…ï¼Œä½¿ç”¨ `make build` ï¼Œ è¿™æ˜¯æˆ‘å¾ˆå–œæ¬¢çš„ä¸€ç§æ–¹å¼ï¼Œæ–¹ä¾¿è´¡çŒ®å’Œé˜…è¯»æºç ï¼š

```bash
â¯ export KUBEBUILDER=$(pwd)/kubebuilder
â¯ git clone https://github.com/kubernetes-sigs/kubebuilder.git $KUBEBUILDER  && cd $KUBEBUILDER; make build && cd bin; ./kubebuilder; 
â¯ export PATH=$PATH:$KUBEBUILDER/bin; kubebuilder
```



## create a project

å¾ˆç®€å•çš„è¡Œä¸ºï¼Œæˆ‘ä»¬åªéœ€è¦åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œå¹¶ä¸”ä½¿ç”¨ å‘½ä»¤ åˆå§‹åŒ–å°±å¤Ÿäº†ï¼Œåœ¨åº•å±‚ä¸Š kubebuilder å¹¶ä¸å¸Œæœ›æˆ‘ä»¬çŸ¥é“å®ç°çš„ç»†èŠ‚~

```bash
â¯ mkdir /tmp/guestbook -p; cd /tmp/guestbook
â¯ kubebuilder init --domain my.domain --repo my.domain/guestbook
```

è¿™ä¸ªç›®å½•å°±æ˜¯å¾ˆç¥å¥‡äº†ï¼Œå°±åƒä»£ç ç”Ÿæˆå™¨ä¸€æ ·ç”Ÿæˆäº†ä¸€ä¸ªæ¨¡æ¿ï¼š

```bash
â¯ tree -L 3
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ PROJECT
â”œâ”€â”€ README.md
â”œâ”€â”€ cmd
â”‚   â””â”€â”€ main.go
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ default
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ manager_auth_proxy_patch.yaml
â”‚   â”‚   â””â”€â”€ manager_config_patch.yaml
â”‚   â”œâ”€â”€ manager
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ manager.yaml
â”‚   â”œâ”€â”€ prometheus
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ monitor.yaml
â”‚   â””â”€â”€ rbac
â”‚       â”œâ”€â”€ auth_proxy_client_clusterrole.yaml
â”‚       â”œâ”€â”€ auth_proxy_role.yaml
â”‚       â”œâ”€â”€ auth_proxy_role_binding.yaml
â”‚       â”œâ”€â”€ auth_proxy_service.yaml
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â”œâ”€â”€ leader_election_role.yaml
â”‚       â”œâ”€â”€ leader_election_role_binding.yaml
â”‚       â”œâ”€â”€ role_binding.yaml
â”‚       â””â”€â”€ service_account.yaml
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ hack
    â””â”€â”€ boilerplate.go.txt
```

åœ¨è¿™é‡Œæˆ‘å€ŸåŠ©ã€ŠKubernetes operator å¼€å‘è¿›é˜¶ã€‹è¿™æœ¬ä¹¦ï¼ˆä½†ä¸æ¨èï¼‰éƒ¨åˆ†è§£é‡Šï¼š

- `Dockerfile`: ç”¨äºæ„å»º Docker é•œåƒçš„æ–‡ä»¶ã€‚
- `Makefile`: ä¸€ä¸ª Makefileï¼Œå…¶ä¸­åŒ…å«äº†ç”¨äºæ„å»ºå’Œå‘å¸ƒ Operator çš„å¸¸ç”¨å‘½ä»¤ã€‚
- `PROJECT`: é¡¹ç›®åç§°ï¼Œä»¥åŠé¡¹ç›®ä¿¡æ¯ï¼Œè¿™é‡Œæ˜¯ä¸€äº› metadata ã€‚
- `README.md`: é¡¹ç›®çš„è¯´æ˜æ–‡æ¡£ã€‚
- `cmd/`: åŒ…å«äº† Operator çš„å…¥å£ç¨‹åº `main.go`ã€‚
- `config/`: åŒ…å«äº† Operator çš„é…ç½®æ–‡ä»¶ï¼ŒåŒ…æ‹¬ RBAC æƒé™ç›¸å…³çš„ YAML æ–‡ä»¶ã€Prometheus ç›‘æ§æœåŠ¡å‘ç°ï¼ˆServiceMonitorï¼‰ç›¸å…³çš„ Yaml æ–‡ä»¶ã€æ§åˆ¶å™¨ï¼ˆManagerï¼‰éƒ¨åˆ†éƒ¨ç½²çš„ Yaml æ–‡ä»¶ã€‚
    - `default/`: åŒ…å«äº†é»˜è®¤çš„é…ç½®æ–‡ä»¶ã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `manager_auth_proxy_patch.yaml`: åœ¨ manager å®¹å™¨ä¸­æ·»åŠ äº† auth-proxy å®¹å™¨çš„ç›¸å…³ä¿¡æ¯ã€‚
        - `manager_config_patch.yaml`: åœ¨ manager å®¹å™¨ä¸­æ·»åŠ äº†ä¸ Operator ç›¸å…³çš„é…ç½®ä¿¡æ¯ã€‚
    - `manager/`: åŒ…å«äº†éƒ¨ç½² Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `manager.yaml`: éƒ¨ç½² Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
    - `prometheus/`: åŒ…å«äº† Prometheus ç›‘æ§ Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `monitor.yaml`: éƒ¨ç½² Prometheus ç›‘æ§ Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
    - `rbac/`: åŒ…å«äº† Operator æ‰€éœ€çš„ RBAC èµ„æºæ–‡ä»¶ã€‚
        - `auth_proxy_client_clusterrole.yaml`: é…ç½®äº†ä¸å®¢æˆ·ç«¯æˆæƒç›¸å…³çš„ ClusterRoleã€‚
        - `auth_proxy_role.yaml`: é…ç½®äº†ä¸ auth-proxy ç›¸å…³çš„ Roleã€‚
        - `auth_proxy_role_binding.yaml`: é…ç½®äº†ä¸ auth-proxy ç›¸å…³çš„ RoleBindingã€‚
        - `auth_proxy_service.yaml`: é…ç½®äº†ä¸ auth-proxy ç›¸å…³çš„ Serviceã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `leader_election_role.yaml`: é…ç½®äº†ä¸ leader election ç›¸å…³çš„ Roleã€‚
        - `leader_election_role_binding.yaml`: é…ç½®äº†ä¸ leader election ç›¸å…³çš„ RoleBindingã€‚
        - `role_binding.yaml`: é…ç½®äº†ä¸ Operator ç›¸å…³çš„ RoleBindingã€‚
        - `service_account.yaml`: é…ç½®äº†ä¸ Operator ç›¸å…³çš„ ServiceAccountã€‚
- `go.mod`: Go é¡¹ç›®çš„æ¨¡å—æ–‡ä»¶ã€‚
- `go.sum`: Go é¡¹ç›®çš„æ¨¡å—ä¾èµ–æ–‡ä»¶ã€‚
- `hack/`: åŒ…å«äº†ç”Ÿæˆä»£ç å’Œæ–‡æ¡£ç­‰ç›¸å…³çš„è„šæœ¬å’Œæ–‡ä»¶ã€‚
    - `boilerplate.go.txt`: ç”¨äºç”Ÿæˆ Go é¡¹ç›®æ–‡ä»¶çš„ä»£ç æ¨¡æ¿ã€‚



ä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬åé¢çš„å­¦ä¹ ï¼Œæˆ‘è¿™é‡Œç”¨ git è¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼Œæ–¹ä¾¿è§‚å¯Ÿåé¢ç”Ÿæˆäº†å“ªäº›æ–‡ä»¶

```
â¯ git add .
â¯ git commit -a -s -m "kubebuilder init"
```



## create an API

è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„APIï¼ˆç»„/ç‰ˆæœ¬ï¼‰ä¸º `webapp/v1` ï¼Œå¹¶åœ¨å…¶ä¸Šåˆ›å»ºæ–°çš„Kindï¼ˆCRDï¼‰ `Guestbook` ï¼š

```bash
â¯ kubebuilder create api --group webapp --version v1 --kind Guestbook
Create Resource [y/n]
y
Create Controller [y/n]
y
```

ğŸ“œ å¯¹ä¸Šé¢çš„è§£é‡Šï¼š

> å¦‚æœä½ æŒ‰ä¸‹ `y` åˆ›å»ºèµ„æº[y/n]å’Œåˆ›å»ºæ§åˆ¶å™¨[y/n]ï¼Œåˆ™è¿™å°†åˆ›å»ºæ–‡ä»¶ `api/v1/guestbook_types.go` ï¼ˆå…¶ä¸­å®šä¹‰äº†APIï¼‰å’Œ `controllers/guestbook_controller.go` ï¼ˆå…¶ä¸­å®ç°äº†æ­¤ç±»ï¼ˆCRDï¼‰çš„åè°ƒä¸šåŠ¡é€»è¾‘ï¼‰ã€‚

è¿™ä¸ªæ—¶å€™ kubebuilder åˆå·å·çš„åšäº†ä»€ä¹ˆï¼Ÿæˆ‘ä»¬çœ‹ä¸€ä¸‹ git çš„å˜åŠ¨ï¼š

```bash
â¯ git status
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   PROJECT
        modified:   cmd/main.go
        modified:   go.mod
        modified:   go.sum

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        api/
        config/crd/
        config/rbac/guestbook_editor_role.yaml
        config/rbac/guestbook_viewer_role.yaml
        config/samples/
        internal/

no changes added to commit (use "git add" and/or "git commit -a")
```

æ–°å¢ç›®å½•ï¼š

+ `api`ï¼šåŒ…å«åˆšåˆšæ·»åŠ çš„ APIï¼Œåé¢ä¼šç»å¸¸ç¼–è¾‘è¿™é‡Œçš„ `guestbook_types.go` æ–‡ä»¶ã€‚è¿™ä¸ªæ–‡ä»¶æ˜¯ CRD ä»£ç çš„ä¸»è¦å®šä¹‰æ–‡ä»¶ã€‚
+ `config/crd`ï¼šå­˜æ”¾çš„æ˜¯ crd éƒ¨ç½²ç›¸å…³çš„ kustomize æ–‡ä»¶ã€‚
+ `config/rbac/`ï¼šåˆ†åˆ«æ˜¯ç¼–è¾‘æƒé™å’ŒæŸ¥è¯¢æƒé™çš„ `ClusterRole`
+ `samples`ï¼šå¾ˆå¥½ç†è§£ï¼ŒCR ç¤ºä¾‹æ–‡ä»¶
+ `internal` ï¼šå¾ˆå¥½ç†è§£ï¼Œå†…éƒ¨æ ¸å¿ƒä»£ç ï¼Œæˆ‘ä»¬æ‰“å¼€çœ‹çœ‹ `controllers`

```go
â¯ cat internal/controller/guestbook_controller.go

package controller
//...
// GuestbookReconciler reconciles a Guestbook object
type GuestbookReconciler struct {
        client.Client
        Scheme *runtime.Scheme
}
//...
func (r *GuestbookReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
        _ = log.FromContext(ctx)

        // TODO(user): your logic here

        return ctrl.Result{}, nil
}

// SetupWithManager sets up the controller with the Manager.
func (r *GuestbookReconciler) SetupWithManager(mgr ctrl.Manager) error {
        return ctrl.NewControllerManagedBy(mgr).
                For(&webappv1.Guestbook{}).
                Complete(r)
}
```

å¾ˆæ˜æ˜¾ï¼Œä¸Šé¢çš„ Reconcile å‡½æ•°æ˜¯ä¸€ä¸ªè°ƒè°å‡½æ•°ï¼Œè¿™æ˜¯è´¯ç©¿å§‹ç»ˆçš„ä¸€ä¸ªè¯è¯­ï¼Œåœ¨ æ˜¾çœ¼çš„æç¤º TODO ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è¡¥å……è‡ªå·±çš„é€»è¾‘ã€‚



æˆ‘ä»¬å…·ä½“è§‚å¯Ÿä¸€ä¸‹ PROJECT æ–‡ä»¶ï¼Œå…·ä½“çš„å…ƒæ•°æ®å˜åŒ–ï¼š

```bash
â¯ git diff PROJECT
diff --git a/PROJECT b/PROJECT
index a18434c..3ea38eb 100644
--- a/PROJECT
+++ b/PROJECT
@@ -7,4 +7,14 @@ layout:
 - go.kubebuilder.io/v4
 projectName: guestbook
 repo: my.domain/guestbook
+resources:
+- api:
+    crdVersion: v1
+    namespaced: true
+  controller: true
+  domain: my.domain
+  group: webapp
+  kind: Guestbook
+  path: my.domain/guestbook/api/v1
+  version: v1
 version: "3"
```

å¦‚æœè¦ç¼–è¾‘APIå®šä¹‰ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆæ¸…å•ï¼Œå¦‚è‡ªå®šä¹‰èµ„æºï¼ˆCRï¼‰æˆ–è‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰

::: details CRD å’Œ CR åŒºåˆ«
è‡ªå®šä¹‰èµ„æºï¼ˆCRï¼‰æ˜¯ Kubernetes ä¸­æ‰©å±• API èµ„æºçš„ä¸€ç§æ–¹å¼ï¼Œå®ƒå…è®¸ç”¨æˆ·è‡ªå®šä¹‰ Kubernetes ä¸­çš„èµ„æºç±»å‹ï¼Œå¹¶ä¸ºå…¶åˆ›å»ºè‡ªå·±çš„ API ç«¯ç‚¹ã€‚ç”¨æˆ·å¯ä»¥ä½¿ç”¨ kubectl å‘½ä»¤è¡Œå·¥å…·æˆ– Kubernetes API ä»¥ç¼–ç¨‹æ–¹å¼æ“ä½œ CRã€‚CR å¯ä»¥ç”¨äºä»»ä½•ä¸€ç§ Kubernetes èµ„æºçš„æ‰©å±•ï¼Œä¾‹å¦‚ Podã€Service æˆ– Deploymentã€‚

è‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰ç”¨äºå®šä¹‰è‡ªå®šä¹‰èµ„æºï¼ˆCRï¼‰ã€‚å®ƒå®šä¹‰äº† CR çš„ç»“æ„ï¼Œå³å®ƒçš„ API è§„èŒƒã€‚CRD ç”¨äºå®šä¹‰ Kubernetes ä¸­çš„æ–° API èµ„æºç±»å‹ã€‚è¿™äº›èµ„æºç±»å‹å¯ä»¥ç”¨äºæ‰©å±• Kubernetesï¼Œä½¿å…¶æ”¯æŒæ›´å¤šçš„èµ„æºç±»å‹å’Œæ“ä½œã€‚CRD æœ¬èº«æ˜¯ä¸€ä¸ª Kubernetes èµ„æºï¼Œå®ƒå¯ä»¥è¢« kubectl æˆ– Kubernetes API ç”¨äºåˆ›å»ºæ–°çš„ CR ç±»å‹ã€‚

å› æ­¤ï¼ŒCR æ˜¯ç”¨æˆ·åˆ›å»ºçš„ Kubernetes èµ„æºç±»å‹ï¼Œè€Œ CRD æ˜¯å®šä¹‰å’Œç®¡ç†è¿™äº›èµ„æºç±»å‹çš„æ–¹å¼ã€‚
:::



### API å®šä¹‰

Kubernetes çš„èµ„æºæœ¬è´¨å°±æ˜¯ä¸€ä¸ª API å¯¹è±¡ï¼Œä¸è¿‡è¿™ä¸ªå¯¹è±¡çš„ æœŸæœ›çŠ¶æ€ è¢« API Service ä¿å­˜åœ¨äº† ETCD ä¸­ï¼ˆæˆ–è€…æ˜¯å¯¹äº k3s æ¥è¯´å¯ä»¥ä¿å­˜åœ¨å…¶ä»–çš„æœ‰çŠ¶æ€æ•°æ®åº“ï¼ŒåŒ…æ‹¬ sqliteã€dqliteã€mysqlâ€¦)ï¼Œç„¶åæä¾› RESTful æ¥å£ç”¨äº æ›´æ–°è¿™äº›å¯¹è±¡ã€‚

æˆ‘ä»¬åœ¨ä¸Šé¢è®²è¿‡ï¼ŒCRD çš„ä»£ç å®šä¹‰ä¸»è¦åœ¨ `api/` ç›®å½•ä¸‹é¢ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹ä»£ç ç»“æ„ï¼š

```bash
â¯ tree api
api
â””â”€â”€ v1
    â”œâ”€â”€ groupversion_info.go
    â”œâ”€â”€ guestbook_types.go
    â””â”€â”€ zz_generated.deepcopy.go
```

`guestbook_types.go` æ–‡ä»¶ä¸»è¦çš„å®šä¹‰ï¼Œæˆ‘ä»¬çœ‹ä¸‹ spec ç»“æ„ã€‚

::: details Spec ç»“å°¾çš„ç»“æ„ä½“å«ä¹‰
åœ¨Goè¯­è¨€ä¸­ï¼Œç»“æ„ä½“ä»¥ spec ç»“å°¾è¡¨ç¤ºè¯¥ç»“æ„ä½“æ˜¯ç”¨äºç‰¹å®šç›®çš„çš„è§„èŒƒç»“æ„ä½“ã€‚è¿™ç§å‘½åçº¦å®šé€šå¸¸ç”¨äºæè¿°ä¸€ä¸ªç»“æ„ä½“çš„ç”¨é€”å’ŒåŠŸèƒ½ï¼Œä»¥ä¾¿å¼€å‘äººå‘˜æ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨å®ƒã€‚ä¾‹å¦‚ï¼Œ`GuestbookSpec`å®šä¹‰äº†æ‰€éœ€çš„ `Guestbook` çŠ¶æ€
:::

```go
// GuestbookSpec defines the desired state of Guestbook
type GuestbookSpec struct {
        // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
        // Important: Run "make" to regenerate code after modifying this file

        // Foo is an example field of Guestbook. Edit guestbook_types.go to remove/update
        Foo string `json:"foo,omitempty"`
}
```

ä¸Šé¢çš„æ³¨é‡Šå†™çš„å¾ˆæ¸…æ¥šï¼ŒFoo æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ é™¤æ‰ï¼Œç„¶åæ·»åŠ è‡ªå·±éœ€è¦çš„é…ç½®ã€‚

ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶ååˆ©ç”¨ Makefile é‡æ–°ç”Ÿæˆä»£ç ï¼ŒğŸ’¡ç®€å•çš„ä¸€ä¸ªæ¡ˆä¾‹å¦‚ä¸‹ï¼š

```go
import (
	corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

type GuestbookSpec struct {
	Replicas int32					`json:"replicas,omitempty"`
    Template corev1.PodTemplateSpec	`json:"template,omitempty"`
}
```

æˆ‘ä»¬åˆ†åˆ«å®šä¹‰äº† ç”¨äºå£°æ˜ Pod å‰¯æœ¬çš„æ•°é‡ã€å’Œç”¨äºç”Ÿæˆ Pod æ¨¡æ¿çš„é…ç½®ã€‚

æœ€åæˆ‘ä»¬ä¹Ÿè¦è®°å½•åˆ° git commit ä¸­ï¼š

```
â¯ git add .
â¯ git commit -a -s -m "create api"
```



### CRD éƒ¨ç½²

**æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ `make manifests` å‘½ä»¤ç”Ÿæˆ ClusterRole å’Œ CustomResourceDefinition é…ç½®ã€‚**



**æ£€æŸ¥ git çš„æ›´æ”¹ä¿¡æ¯ï¼š**

> å¦‚æœè¦ç¼–è¾‘APIå®šä¹‰ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆæ¸…å•ï¼Œå¦‚è‡ªå®šä¹‰èµ„æºï¼ˆCRï¼‰æˆ–è‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰

```bash
â¯ git status
On branch master
Untracked files:
  (use "git add <file>..." to include in what will be committed)
        config/crd/bases/
        config/rbac/role.yaml
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªç›®å½•æ–‡ä»¶çš„å˜åŒ–ï¼š

+ `config/crd/bases/` ç›®å½•ï¼šæ–°å¢ `webapp.my.domain_guestbooks.yaml` æ–‡ä»¶ï¼Œè¿™ä¹Ÿæ˜¯ `guestbook` ç±»å‹çš„ CRD é…ç½®æ–‡ä»¶ã€‚
+ `config/rbac/role.yaml` å®šä¹‰çš„æ˜¯ä¸€ä¸ª ClusterRoleï¼Œä»åå­— manager-role ä¸Šå¤§è‡´ä¹Ÿå¯ä»¥çŒœå‡ºè¿™æ˜¯åé¢ Controller éƒ¨ç½²åå°†å……å½“çš„ â€œ**è§’è‰²**â€ï¼Œå®šä¹‰äº†å¯¹ `guestbook` èµ„æºçš„ CURD æ“ä½œã€‚



æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª Kubernetes çš„ç¯å¢ƒï¼Œä¸ç®¡æ˜¯ Kind & minikube & k3s ä½œä¸ºæµ‹è¯•ç¯å¢ƒä¹Ÿå¥½ï¼Œæˆ‘é€‰æ‹©äº† Kindï¼š

```
â¯ kind version
kind v0.18.0-alpha+bc0526729cf900 go1.20.1 linux/amd64
```

âš ï¸ æ§åˆ¶å™¨å°†è‡ªåŠ¨ä½¿ç”¨ kubeconfigæ–‡ä»¶ï¼ˆå³é›†ç¾¤`kubectl cluster-info`æ˜¾ç¤ºçš„ä»»ä½•å†…å®¹ï¼‰ã€‚å¦‚æœå‡ºç°é—®é¢˜æŒ‰ç…§å®˜æ–¹çš„æ–¹æ³•å¯ä»¥è‡ªå·±æ‹·è´ kubeconfig æˆ–è€…æ˜¯ è®¾ç½® ç¯å¢ƒå˜é‡ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨ `make install` å®Œæˆ CRD çš„éƒ¨ç½²è¿‡ç¨‹ï¼š

```bash
â¯ make install
```



ğŸ¯ æˆ‘ä»¬åº”è¯¥æ£€æŸ¥ä¸€ä¸ª Makefile ä¸­çš„ `install target`ï¼Œæˆ–è®¸å¯ä»¥çœ‹å‡ºæ¥ `install target` æ˜¯åŒ…å«  `make manifests` å‘½ä»¤çš„ï¼Œä¸è¿‡æˆ‘ä»¬è¿˜æ˜¯åˆ†å¼€æ“ä½œæœ‰åŠ©äºäº†è§£æ•´ä¸ªè¿‡ç¨‹ã€‚

```bash
# install   Install CRDs into the K8s cluster specified in ~/.kube/config.
.PHONY: install
install: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.
        $(KUSTOMIZE) build config/crd | kubectl apply -f -
      
# kustomize Download kustomize locally if necessary. If wrong version is installed, it will be removed before downloading.
.PHONY: kustomize
kustomize: $(KUSTOMIZE) ## Download kustomize locally if necessary. If wrong version is installed, it will be removed before downloading.
$(KUSTOMIZE): $(LOCALBIN)
        @if test -x $(LOCALBIN)/kustomize && ! $(LOCALBIN)/kustomize version | grep -q $(KUSTOMIZE_VERSION); then \
                echo "$(LOCALBIN)/kustomize version is not expected $(KUSTOMIZE_VERSION). Removing it before installing."; \
                rm -rf $(LOCALBIN)/kustomize; \
        fi
        test -s $(LOCALBIN)/kustomize || { curl -Ss $(KUSTOMIZE_INSTALL_SCRIPT) --output install_kustomize.sh && bash install_kustomize.sh $(subst v,,$(KUSTOMIZE_VERSION)) $(LOCALBIN); rm install_kustomize.sh; }
```



**æ ¡éªŒ**ï¼š

```bash
â¯ k get crd -A
NAME                          CREATED AT
crontabs.stable.example.com   2023-04-07T09:17:55Z
guestbooks.webapp.my.domain   2023-04-07T15:42:22Z
```

`guestbooks.webapp.my.domain` å·²ç»å­˜åœ¨äº†ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰èµ„æºï¼Œæ„å‘³ç€æˆ‘ä»¬å¯ä»¥é€šè¿‡ `k get pod` çš„æ–¹å¼æ¥ `k get guestbooks`

```bash
â¯ k get guestbooks.webapp.my.domain -A
No resources found
```

è¿™é‡Œå¹¶æ²¡å‡ºå‡ºç° error ï¼Œè¯´æ˜æç¤ºçš„æ˜¯ kube-apiserver å·²ç»å¯ä»¥è¯†åˆ«è¿™ä¸ªèµ„æºï¼Œåªä¸è¿‡æ²¡æœ‰è¿™ä¸ªèµ„æºçš„å…·ä½“å®ä¾‹ã€‚



è¿è¡Œä½ çš„æ§åˆ¶å™¨ï¼ˆè¿™å°†åœ¨å‰å°è¿è¡Œï¼Œæ‰€ä»¥åˆ‡æ¢åˆ°ä¸€ä¸ªæ–°çš„ ç»ˆç«¯ï¼ˆå¦‚æœä½ æƒ³è®©å®ƒä¿æŒè¿è¡Œï¼‰ï¼š

```bash
make run
```



### å®‰è£…è‡ªå®šä¹‰èµ„æºçš„å®ä¾‹

å¦‚æœä½ æŒ‰äº†`y`åˆ›å»ºèµ„æº **[y/n]**ï¼Œåˆ™ä½ åœ¨ç¤ºä¾‹ä¸­ä¸ºCRDåˆ›å»ºäº†CRï¼ˆå¦‚æœä½ æ›´æ”¹äº† APIå®šä¹‰ï¼‰ï¼š

```bash
kubectl apply -f config/samples/
```



### åœ¨é›†ç¾¤ä¸Šè¿è¡Œ

æ„å»ºé•œåƒå¹¶å°†å…¶æ¨é€åˆ°`IMG`æŒ‡å®šçš„ä½ç½®ï¼š

```bash
make docker-build docker-push IMG=<some-registry>/<project-name>:tag
```

ä½¿ç”¨`IMG`æŒ‡å®šçš„é•œåƒå°†æ§åˆ¶å™¨éƒ¨ç½²åˆ°é›†ç¾¤ï¼š

```bash
make deploy IMG=<some-registry>/<project-name>:tag
```



### å¸è½½CRD

è¦ä»ç¾¤é›†ä¸­åˆ é™¤CRDï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```bash
make uninstall
```



### å–æ¶ˆéƒ¨ç½² controller

å°†æ§åˆ¶å™¨å–æ¶ˆéƒ¨ç½²åˆ°ç¾¤é›†ï¼š

```bash
make undeploy
```



## CR éƒ¨ç½²

æˆ‘ä»¬ä½¿ç”¨ CRD å®šä¹‰äº†ä¸€ä¸ªèµ„æºï¼ŒCR å°±åƒå†™ä¸€ä¸ª Deployment ä¸€æ ·ï¼Œåˆ›å»º `guestbooks` åŒæ ·éœ€è¦ä¸€ä¸ª `yaml` æ–‡ä»¶ï¼Œå¹¶ä¸”ç¬¦åˆå£°æ˜å¼ã€‚

```yaml
â¯ cat config/samples/webapp_v1_guestbook.yaml
apiVersion: webapp.my.domain/v1
kind: Guestbook
metadata:
  labels:
    app.kubernetes.io/name: guestbook
    app.kubernetes.io/instance: guestbook-sample
    app.kubernetes.io/part-of: guestbook
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/created-by: guestbook
  name: guestbook-sample
spec:
  # TODO(user): Add fields here
  replicas: 3
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

â¯ k apply -f webapp_v1_guestbook.yaml
guestbook.webapp.my.domain/guestbook-sample created

â¯ k get guestbooks.webapp.my.domain
NAME               AGE
guestbook-sample   93s
```

ä¸Šé¢è¡¨ç¤ºåˆ›å»ºå‡ºæ¥äº† `guestbook-sample ` å¯¹è±¡ï¼Œä¸è¿‡è¿™ä¸ªæ—¶å€™è¿˜ä¸å¤Ÿï¼Œå› ä¸º Pod è¿˜æ²¡æœ‰è¢«åˆ›å»ºå‡ºæ¥ï¼Œæˆ‘ä»¬ç»§ç»­å®ç°ç›¸åº”çš„æ§åˆ¶å™¨é€»è¾‘ã€‚



## guestbook ç”Ÿæˆçš„ä»£ç å’Œç»“æ„

æˆ‘ä»¬è¯¦ç»†è§£é‡Šä¸€ä¸‹ kubebuilder åˆå§‹åŒ–CRDä»¥åŠç›¸å…³çš„æ§åˆ¶å™¨æ¡†æ¶åï¼Œéƒ¨åˆ†çš„ä»£ç ç»“æ„å’Œæºç è§£æã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥ä»ä¸»å‡½æ•°å¼€å§‹ï¼š main.go



### cmd

main.go ä½œä¸ºå…¥å£å‡½æ•°ï¼Œæ˜¯æˆ‘ä»¬ä¸»è¦çœ‹çš„ã€‚å½“ç„¶ï¼Œè®¸å¯è¯ä¿¡æ¯çœç•¥æ‰äº†~ æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹å¤´æ–‡ä»¶ï¼š

```go
package main

import (
	"flag"
	"os"

	// Import all Kubernetes client auth plugins (e.g. Azure, GCP, OIDC, etc.)
	// to ensure that exec-entrypoint and run can make use of them.
	_ "k8s.io/client-go/plugin/pkg/client/auth"

	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/healthz"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"

	webappv1 "my.domain/guestbook/api/v1"
	"my.domain/guestbook/internal/controller"
	//+kubebuilder:scaffold:imports
)
```

æ¯ä¸€ç»„æ§åˆ¶å™¨éƒ½éœ€è¦ä¸€ä¸ªSchemeï¼Œå®ƒæä¾›äº†Kindså’Œå®ƒä»¬å¯¹åº”çš„Goç±»å‹ä¹‹é—´çš„æ˜ å°„ã€‚åœ¨ç¼–å†™APIå®šä¹‰æ—¶ï¼Œæˆ‘ä»¬å°†æ›´å¤šåœ°è®¨è®ºKindsï¼Œæ‰€ä»¥ç¨åè¯·è®°ä½è¿™ä¸€ç‚¹ã€‚

```go
var (
	scheme   = runtime.NewScheme()
	setupLog = ctrl.Log.WithName("setup")
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))

	utilruntime.Must(webappv1.AddToScheme(scheme))
	//+kubebuilder:scaffold:scheme
}
```

`runtime.Scheme` æ˜¯ Kubernetes ä¸­å¯¹è±¡çš„ç¼–è§£ç å™¨æ³¨å†Œè¡¨ï¼Œç”¨äºå°†å¯¹è±¡åºåˆ—åŒ–ä¸ºå­—èŠ‚æ•°ç»„ï¼Œå¹¶å°†å­—èŠ‚æ•°ç»„ååºåˆ—åŒ–ä¸ºå¯¹è±¡ã€‚æ‰€æœ‰ Kubernetes API å¯¹è±¡éƒ½å¿…é¡»æ³¨å†Œåˆ° `runtime.Scheme` ä¸­ï¼Œä»¥ä¾¿åœ¨å­˜å‚¨åˆ° etcd æˆ–å‘é€åˆ° API Server æ—¶è¿›è¡Œæ­£ç¡®çš„ç¼–è§£ç ã€‚

å¯ä»¥é€šè¿‡è°ƒç”¨ `scheme.AddKnownTypes()` æ–¹æ³•å‘ `runtime.Scheme` ä¸­æ³¨å†Œæ–°çš„ API å¯¹è±¡ç±»å‹ã€‚

æˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸¤ä¸ªç®¡ç†å™¨ï¼Œå®ƒè·Ÿè¸ªæ‰€æœ‰æ§åˆ¶å™¨çš„è¿è¡Œæƒ…å†µï¼Œå¹¶ä¸ºAPIæœåŠ¡å™¨è®¾ç½®å…±äº«ç¼“å­˜å’Œå®¢æˆ·æœºã€‚

âš ï¸ æ³¨æ„ï¼šè¿™é‡Œæœ‰ `//+kubebuilder:scaffold:scheme` ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒæœ‰æ„æ€çš„äº‹æƒ…ã€‚

å‰©ä¸‹å°±æ˜¯æœ€æ ¸å¿ƒçš„ `main()` å‡½æ•°ï¼š

```go
func main() {
    var metricsAddr string
    var enableLeaderElection bool
    var probeAddr string
    flag.StringVar(&metricsAddr, "metrics-bind-address", ":8080", "The address the metric endpoint binds to.")
    flag.StringVar(&probeAddr, "health-probe-bind-address", ":8081", "The address the probe endpoint binds to.")
    flag.BoolVar(&enableLeaderElection, "leader-elect", false,
        "Enable leader election for controller manager. "+
            "Enabling this will ensure there is only one active controller manager.")
    opts := zap.Options{
        Development: true,
    }
    opts.BindFlags(flag.CommandLine)
    flag.Parse()

    ctrl.SetLogger(zap.New(zap.UseFlagOptions(&opts)))

    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
        Scheme:                 scheme,
        MetricsBindAddress:     metricsAddr,
        Port:                   9443,
        HealthProbeBindAddress: probeAddr,
        LeaderElection:         enableLeaderElection,
        LeaderElectionID:       "80807133.tutorial.kubebuilder.io",
    })
    if err != nil {
        setupLog.Error(err, "unable to start manager")
        os.Exit(1)
    }
```

å‰é¢æ— éå°±æ˜¯å®šä¹‰äº†ä¸€ä¸ª flag å¹¶ä¸”åˆå§‹åŒ–ã€‚å¹¶ä¸”äº¤ç»™`Parse`è§£æ`os.Args[1:]`ä¸­çš„å‘½ä»¤è¡Œæ ‡å¿—ã€‚

è¯·æ³¨æ„ï¼Œ `Manager` å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é™åˆ¶æ‰€æœ‰æ§åˆ¶å™¨å°†ç›‘è§†èµ„æºçš„å‘½åç©ºé—´ï¼š

```go
 mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
     Scheme:                 scheme,
     Namespace:              namespace,
     MetricsBindAddress:     metricsAddr,
     Port:                   9443,
     HealthProbeBindAddress: probeAddr,
     LeaderElection:         enableLeaderElection,
     LeaderElectionID:       "80807133.tutorial.kubebuilder.io",
 })
```

ä¸Šé¢çš„ç¤ºä¾‹å°†é¡¹ç›®çš„èŒƒå›´æ›´æ”¹ä¸ºå•ä¸ª `Namespace` ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿˜å»ºè®®å°†é»˜è®¤çš„ `ClusterRole` å’Œ `ClusterRoleBinding` åˆ†åˆ«æ›¿æ¢ä¸º `Role` å’Œ `RoleBinding` ï¼Œä»è€Œå°†æä¾›çš„æˆæƒé™åˆ¶åœ¨æ­¤å‘½åç©ºé—´ã€‚è¿™æ ·æ¥è¯´æƒé™å°ä¸€äº›ï¼Œåœ¨ RBAC ä¸­ä»‹ç»è¿‡è¿™éƒ¨åˆ†ã€‚

ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ `MultiNamespacedCacheBuilder` æ¥ç›‘è§†ç‰¹æ€§çš„å‘½åç©ºé—´å­é›†ã€‚

```go
var namespaces []string // List of Namespaces

mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
    Scheme:                 scheme,
    NewCache:               cache.MultiNamespacedCacheBuilder(namespaces),
    MetricsBindAddress:     fmt.Sprintf("%s:%d", metricsHost, metricsPort),
    Port:                   9443,
    HealthProbeBindAddress: probeAddr,
    LeaderElection:         enableLeaderElection,
    LeaderElectionID:       "80807133.tutorial.kubebuilder.io",
})
```

**æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œå°±æ˜¯ä¸€äº›é”™è¯¯å¤„ç†ï¼Œå’Œå¼€å§‹æ­å»ºæˆ‘ä»¬çš„APIäº†ï¼**

```go
// +kubebuilder:scaffold:builder

if err := mgr.AddHealthzCheck("healthz", healthz.Ping); err != nil {
    setupLog.Error(err, "unable to set up health check")
    os.Exit(1)
}
if err := mgr.AddReadyzCheck("readyz", healthz.Ping); err != nil {
    setupLog.Error(err, "unable to set up ready check")
    os.Exit(1)
}

setupLog.Info("starting manager")
if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
    setupLog.Error(err, "problem running manager")
    os.Exit(1)
}

```



##  groups, versions, kinds, and resources

å½“æˆ‘ä»¬è°ˆè®ºKubernetesä¸­çš„APIæ—¶ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨4ä¸ªæœ¯è¯­ï¼šç»„ã€ç‰ˆæœ¬ã€ç§ç±»å’Œèµ„æºã€‚

æ²¡é”™ï¼Œæˆ‘ä»¬åœ¨ [https://docker.nsddd.top/Cloud-Native-k8s/](https://docker.nsddd.top/Cloud-Native-k8s/) ä¸­ä»‹ç»è¿‡å¾ˆå¤šå…³äº GVK å’Œ GVR çš„ä»‹ç»ï¼Œä¸ºä»€ä¹ˆåœ¨ Kubebuilder ä¸­å°¤å…¶éœ€è¦å†æä¸€æ¬¡ã€‚

å½“æˆ‘ä»¬åœ¨ä¸€ä¸ªç‰¹å®šçš„ç»„ç‰ˆæœ¬ä¸­å¼•ç”¨ä¸€ä¸ªç§ç±»æ—¶ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºGroupVersionKindï¼Œæˆ–ç®€ç§°ä¸ºGVKã€‚èµ„æºå’ŒGVRä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬å¾ˆå¿«å°±ä¼šçœ‹åˆ°ï¼Œæ¯ä¸ªGVKå¯¹åº”äºåŒ…ä¸­ç»™å®šçš„ root Go typeã€‚èµ°è¿›æºç ï¼Œä½“ä¼šè¿™ç§æ„Ÿè§‰~



### create an API

æˆ‘ä»¬åœ¨å‰é¢åˆ›å»ºè¿‡ API ï¼Œ`kubebuilder create api --group webapp --version v1 --kind Guestbook` å‘½ä»¤åˆ›å»ºäº†ä¸€ä¸ª ç»„ä¸º `webapp`ï¼Œç‰ˆæœ¬ä¸º `v1`ï¼Œç±»å‹ä¸º `Guestbook` çš„API èµ„æºå¯¹è±¡ã€‚

æˆ‘ä»¬ä½¿ç”¨çš„å‘½ä»¤æ˜¯ `create api`ï¼Œæ­¤å‘½ä»¤çš„ç›®æ ‡æ˜¯ä¸ºæˆ‘ä»¬çš„åŒç±»åˆ›å»ºè‡ªå®šä¹‰èµ„æºï¼ˆCRï¼‰å’Œè‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰ã€‚æˆ‘ç¿»å¼€äº† Kubebuilder çš„æºç éƒ¨åˆ†ï¼š

åœ¨ `pkg/cli` ç›®å½•ä¸‹é¢ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†å…¥å£ï¼š

```go
// addSubcommands returns a root command with a subcommand tree reflecting the
// current project's state.
func (c *CLI) addSubcommands() {
	// add the alpha command if it has any subcommands enabled
	c.addAlphaCmd()

	// kubebuilder completion
	// Only add completion if requested
	if c.completionCommand {
		c.cmd.AddCommand(c.newCompletionCmd())
	}

	// kubebuilder create
	createCmd := c.newCreateCmd()
	// kubebuilder create api
	createCmd.AddCommand(c.newCreateAPICmd())
	createCmd.AddCommand(c.newCreateWebhookCmd())
	if createCmd.HasSubCommands() {
		c.cmd.AddCommand(createCmd)
	}

	// kubebuilder edit
	c.cmd.AddCommand(c.newEditCmd())

	// kubebuilder init
	c.cmd.AddCommand(c.newInitCmd())

	// kubebuilder version
	// Only add version if a version string was provided
	if c.version != "" {
		c.cmd.AddCommand(c.newVersionCmd())
	}
}
```

ğŸ“œ å¯¹ä¸Šé¢çš„è§£é‡Šï¼š

> é€šè¿‡ `newCreateCmd` åˆ›å»ºäº†ä¸€ä¸ª `create` å‘½ä»¤ï¼Œå¹¶ä¸”é€šè¿‡ `newCreateAPICmd` å’Œ `newCreateWebhookCmd` ç»‘å®šäº† `api` å’Œ `webhook`.

`webhook` æ˜¯ä¸€ä¸ªé’©å­ï¼Œhook åœ¨ Kubernetes ä¸­éšå¤„å¯è§ï¼Œå¯ä»¥ä½¿ç”¨ Webhook æ¥éªŒè¯ CRD å¯¹è±¡çš„è§„èŒƒæ˜¯å¦æ­£ç¡®ï¼Œå¹¶è®¾ç½®é»˜è®¤å€¼ï¼Œä»¥ç¡®ä¿ CRD å¯¹è±¡çš„æ­£ç¡®æ€§ã€‚



### ä¸ºä»€ä¹ˆè¦åˆ›å»º APIï¼Ÿ

æ–°çš„APIæ˜¯æˆ‘ä»¬å‘Kubernetesæ•™æˆè‡ªå®šä¹‰å¯¹è±¡çš„æ–¹å¼ã€‚Goç»“æ„ä½“ç”¨äºç”Ÿæˆä¸€ä¸ªCRDï¼Œå…¶ä¸­åŒ…æ‹¬æ•°æ®çš„æ¨¡å¼ä»¥åŠè·Ÿè¸ªæ•°æ®ï¼Œä¾‹å¦‚æˆ‘ä»¬çš„æ–°ç±»å‹è¢«ç§°ä¸ºä»€ä¹ˆã€‚

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„è‡ªå®šä¹‰å¯¹è±¡çš„å®ä¾‹ï¼Œè¿™äº›å¯¹è±¡å°†ç”±æˆ‘ä»¬çš„æ§åˆ¶å™¨ç®¡ç†ã€‚

æˆ‘ä»¬çš„APIå’Œèµ„æºä»£è¡¨æˆ‘ä»¬åœ¨é›†ç¾¤ä¸Šçš„è§£å†³æ–¹æ¡ˆã€‚åŸºæœ¬ä¸Šï¼ŒCRDæ˜¯æˆ‘ä»¬å®šåˆ¶å¯¹è±¡çš„å®šä¹‰ï¼Œè€ŒCRæ˜¯å®ƒçš„å®ä¾‹ã€‚å°±æ¯”å¦‚è¯´ä¸Šé¢ æˆ‘ä»¬ é€šè¿‡ kubebuilder å®šä¹‰äº†ä¸€ä¸ª `guestbooks.webapp.my.domain`  çš„ CRDï¼Œç„¶åå†é€šè¿‡ å£°æ˜å¼ yaml æ–‡ä»¶ç¼–å†™ CRï¼Œå¹¶ä¸”åˆ›å»º CRã€‚

> å½“ç„¶å®˜æ–¹æœ‰ä¸€ä¸ªæ›´å¥½ç†è§£çš„ä¾‹å­ï¼šç›®æ ‡æ˜¯è®©åº”ç”¨ç¨‹åºåŠå…¶æ•°æ®åº“åœ¨Kuberneteså¹³å°ä¸Šè¿è¡Œã€‚ç„¶åï¼Œä¸€ä¸ªCRDå¯ä»¥è¡¨ç¤ºAppï¼Œè€Œå¦ä¸€ä¸ªCRDå¯ä»¥è¡¨ç¤ºDBã€‚
>
> é€šè¿‡ä½¿ç”¨ä¸€ä¸ªCRDæè¿°åº”ç”¨ç¨‹åºï¼Œå¦ä¸€ä¸ªCRDæè¿°æ•°æ®åº“ï¼Œæˆ‘ä»¬ä¸ä¼šæŸå®³å°è£…ã€å•ä¸€è´£ä»»åŸåˆ™å’Œå†…èšç­‰æ¦‚å¿µã€‚
>
> ç ´åè¿™äº›æ¦‚å¿µå¯èƒ½ä¼šå¯¼è‡´æ„æƒ³ä¸åˆ°çš„å‰¯ä½œç”¨ï¼Œä¾‹å¦‚éš¾ä»¥æ‰©å±•ã€é‡ç”¨æˆ–ç»´æŠ¤ç­‰ã€‚



### Single Group to Multi-Group

åœ¨`Kubebuilder v2 scaffolding`çš„åˆå§‹ç‰ˆæœ¬ä¸­ï¼ˆä»Kubebuilder v2.0.0å¼€å§‹ï¼‰ä¸å­˜åœ¨å¤šç»„`scaffolding`æ”¯æŒã€‚

è¦æ›´æ”¹é¡¹ç›®çš„å¸ƒå±€ä»¥æ”¯æŒå¤šç»„ï¼Œè¯·è¿è¡Œå‘½ä»¤ `kubebuilder edit --multigroup=true` ã€‚ä¸€æ—¦åˆ‡æ¢åˆ°å¤šç»„å¸ƒå±€ï¼Œæ–°çš„Kindså°†åœ¨æ–°å¸ƒå±€ä¸­ç”Ÿæˆï¼Œä½†éœ€è¦é¢å¤–çš„æ‰‹åŠ¨å·¥ä½œå°†æ—§çš„APIç»„ç§»åŠ¨åˆ°æ–°å¸ƒå±€ã€‚

ç„¶åæˆ‘ä»¬å°±å¯ä»¥æ·»åŠ ä¸€ä¸ª **æ–°çš„ API**



### add new api

```bash
â¯ kubebuilder create api --group batch --version v1 --kind CronJob
```

æŒ‰ä¸‹ `y` é”®ï¼Œé€‰æ‹©â€œåˆ›å»ºèµ„æºâ€å’Œâ€œåˆ›å»ºæ§åˆ¶å™¨â€ã€‚

ç¬¬ä¸€æ¬¡ä¸ºæ¯ä¸ªç»„ç‰ˆæœ¬è°ƒç”¨æ­¤å‘½ä»¤æ—¶ï¼Œå®ƒå°†ä¸ºæ–°çš„ç»„ç‰ˆæœ¬åˆ›å»ºä¸€ä¸ªç›®å½•ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œåˆ›å»ºäº†ä¸ `batch.tutorial.kubebuilder.io/v1` å¯¹åº”çš„ `api/v1/` ç›®å½•ï¼ˆè¿˜è®°å¾—æˆ‘ä»¬ä¸€å¼€å§‹çš„ `--domain` --domainè®¾ç½®å—ï¼Ÿï¼‰ã€‚

å®ƒè¿˜ä¸ºæˆ‘ä»¬çš„ `CronJob` Kindæ·»åŠ äº†ä¸€ä¸ªæ–‡ä»¶ï¼Œ `api/v1/cronjob_types.go` ã€‚æ¯æ¬¡æˆ‘ä»¬è°ƒç”¨ä¸åŒç±»å‹çš„å‘½ä»¤æ—¶ï¼Œå®ƒéƒ½ä¼šæ·»åŠ ä¸€ä¸ªç›¸åº”çš„æ–°æ–‡ä»¶ã€‚

æˆ‘ä»¬çš„å‡ºå‘ç‚¹å¾ˆç®€å•ï¼šæˆ‘ä»¬å¯¼å…¥ `meta/v1` APIç»„ï¼Œå®ƒé€šå¸¸ä¸ä¼šè‡ªå·±æš´éœ²ï¼Œè€Œæ˜¯åŒ…å«æ‰€æœ‰Kubernetes Kindé€šç”¨çš„å…ƒæ•°æ®ã€‚

```go
package v1

import (
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºSpecå’ŒStatus of our Kindå®šä¹‰ç±»å‹ã€‚**Kubernetesçš„åŠŸèƒ½æ˜¯å°†æœŸæœ›çš„çŠ¶æ€ï¼ˆ `Spec` ï¼‰ä¸å®é™…çš„é›†ç¾¤çŠ¶æ€ï¼ˆå…¶ä»–å¯¹è±¡çš„ `Status` ï¼‰å’Œå¤–éƒ¨çŠ¶æ€è¿›è¡Œåè°ƒï¼Œç„¶åè®°å½•å®ƒæ‰€è§‚å¯Ÿåˆ°çš„ï¼ˆ `Status` ï¼‰**ã€‚å› æ­¤ï¼Œæ¯ä¸ªå‡½æ•°å¯¹è±¡éƒ½åŒ…æ‹¬specå’Œstatusã€‚ä¸€äº›ç±»å‹ï¼Œæ¯”å¦‚ `ConfigMap` ï¼Œä¸éµå¾ªè¿™ç§æ¨¡å¼ï¼Œå› ä¸ºå®ƒä»¬ä¸ç¼–ç æ‰€éœ€çš„çŠ¶æ€ï¼Œä½†å¤§å¤šæ•°ç±»å‹éƒ½æ˜¯è¿™æ ·ã€‚

```go
// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
    // Important: Run "make" to regenerate code after modifying this file
}

// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run "make" to regenerate code after modifying this file
}
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰å¯¹åº”äºå®é™…Kindçš„ç±»å‹ï¼Œ `CronJob` å’Œ `CronJobList` ã€‚ `CronJob` æ˜¯æˆ‘ä»¬çš„æ ¹ç±»å‹ï¼Œå®ƒæè¿°äº† `CronJob` ç±»å‹ã€‚åƒæ‰€æœ‰Kuberneteså¯¹è±¡ä¸€æ ·ï¼Œå®ƒåŒ…å« `TypeMeta` ï¼ˆæè¿°APIç‰ˆæœ¬å’ŒKindï¼‰ï¼Œè¿˜åŒ…å« `ObjectMeta` ï¼Œå®ƒåŒ…å«åç§°ï¼Œå‘½åç©ºé—´å’Œæ ‡ç­¾ç­‰å†…å®¹ã€‚

`CronJobList` åªæ˜¯å¤šä¸ª `CronJob` çš„å®¹å™¨ã€‚å®ƒæ˜¯åœ¨æ‰¹é‡æ“ä½œä¸­ä½¿ç”¨çš„ç±»å‹ï¼Œå¦‚LISTã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ä»ä¸ä¿®æ”¹è¿™ä¸¤ä¸ªå±æ€§ä¸­çš„ä»»ä½•ä¸€ä¸ª--æ‰€æœ‰ä¿®æ”¹éƒ½åœ¨Specæˆ–Statusä¸­ã€‚

è¿™ä¸ªå°å°çš„ `+kubebuilder:object:root` æ³¨é‡Šè¢«ç§°ä¸ºæ ‡è®°ã€‚æˆ‘ä»¬å°†åœ¨ç¨åçœ‹åˆ°æ›´å¤šï¼Œä½†è¦çŸ¥é“å®ƒä»¬ä½œä¸ºé¢å¤–çš„å…ƒæ•°æ®ï¼Œå‘Šè¯‰æ§åˆ¶å™¨å·¥å…·ï¼ˆæˆ‘ä»¬çš„ä»£ç å’ŒYAMLç”Ÿæˆå™¨ï¼‰é¢å¤–çš„ä¿¡æ¯ã€‚è¿™ä¸ªç‰¹å®šçš„ç±»å‹å‘Šè¯‰ `object` ç”Ÿæˆå™¨è¿™ä¸ªç±»å‹ä»£è¡¨ä¸€ä¸ªKindã€‚ç„¶åï¼Œ `object` ç”Ÿæˆå™¨ä¸ºæˆ‘ä»¬ç”Ÿæˆ `runtime.Object` æ¥å£çš„å®ç°ï¼Œè¿™æ˜¯æ‰€æœ‰è¡¨ç¤ºKindsçš„ç±»å‹éƒ½å¿…é¡»å®ç°çš„æ ‡å‡†æ¥å£ã€‚

```go
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status

// CronJob is the Schema for the cronjobs API
type CronJob struct {
    metav1.TypeMeta   `json:",inline"`
    metav1.ObjectMeta `json:"metadata,omitempty"`

    Spec   CronJobSpec   `json:"spec,omitempty"`
    Status CronJobStatus `json:"status,omitempty"`
}

//+kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
    metav1.TypeMeta `json:",inline"`
    metav1.ListMeta `json:"metadata,omitempty"`
    Items           []CronJob `json:"items"`
}
```

æœ€åï¼Œæˆ‘ä»¬å°†Goç±»å‹æ·»åŠ åˆ°APIç»„ã€‚è¿™å…è®¸æˆ‘ä»¬å°†æ­¤APIç»„ä¸­çš„ç±»å‹æ·»åŠ åˆ°ä»»ä½• Scheme ä¸­ã€‚

```go
func init() {
    SchemeBuilder.Register(&CronJob{}, &CronJobList{})
}
```



##  controller 

ä¸€å¥è¯æè¿°ï¼šControllers are the core of Kubernetes, and of any operator

æ§åˆ¶å™¨çš„å·¥ä½œæ˜¯ç¡®ä¿ï¼Œå¯¹äºä»»ä½•ç»™å®šçš„å¯¹è±¡ï¼Œä¸–ç•Œçš„å®é™…çŠ¶æ€ï¼ˆåŒ…æ‹¬é›†ç¾¤çŠ¶æ€ï¼Œä»¥åŠæ½œåœ¨çš„å¤–éƒ¨çŠ¶æ€ï¼Œå¦‚Kubeletçš„è¿è¡Œå®¹å™¨æˆ–äº‘æä¾›å•†çš„è´Ÿè½½å‡è¡¡å™¨ï¼‰ä¸å¯¹è±¡ä¸­æ‰€éœ€çš„çŠ¶æ€ç›¸åŒ¹é…ã€‚

æ¯ä¸ªæ§åˆ¶å™¨ä¸“æ³¨äºä¸€ä¸ª root Kindï¼Œä½†å¯ä»¥ä¸å…¶ä»–Kindäº¤äº’ã€‚

æˆ‘ä»¬ç§°è¿™ä¸ªè¿‡ç¨‹ä¸ºå’Œè§£ï¼ˆ *reconciling*ï¼‰ã€‚

åœ¨controller-runtimeä¸­ï¼Œå®ç°ç‰¹å®šç±»å‹åè°ƒçš„é€»è¾‘ç§°ä¸ºReconcilerã€‚reconcilerè·å–å¯¹è±¡çš„åç§°ï¼Œå¹¶è¿”å›æ˜¯å¦éœ€è¦é‡è¯•ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœå‡ºç°é”™è¯¯æˆ–å‘¨æœŸæ€§æ§åˆ¶å™¨ï¼Œå¦‚HorizontalPodAutoscalerï¼‰ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬ä»ä¸€äº›æ ‡å‡†çš„å¯¼å…¥å¼€å§‹ã€‚å’Œå‰é¢ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦æ ¸å¿ƒæ§åˆ¶å™¨è¿è¡Œæ—¶åº“ï¼Œä»¥åŠå®¢æˆ·ç«¯åŒ…å’ŒAPIç±»å‹çš„åŒ…ã€‚

```go
package controllers

import (
    "context"

    "k8s.io/apimachinery/pkg/runtime"
    ctrl "sigs.k8s.io/controller-runtime"
    "sigs.k8s.io/controller-runtime/pkg/client"
    "sigs.k8s.io/controller-runtime/pkg/log"

    batchv1 "tutorial.kubebuilder.io/project/api/v1"
)
```

æ¥ä¸‹æ¥ï¼Œkubebuilderä¸ºæˆ‘ä»¬æ­å»ºäº†ä¸€ä¸ªåŸºæœ¬çš„reconcilerç»“æ„ã€‚å‡ ä¹æ¯ä¸ªåè°ƒå™¨éƒ½éœ€è¦æ—¥å¿—ï¼Œå¹¶ä¸”éœ€è¦èƒ½å¤Ÿè·å–å¯¹è±¡ï¼Œå› æ­¤è¿™äº›éƒ½æ˜¯å¼€ç®±å³ç”¨çš„ã€‚

```go
// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Scheme *runtime.Scheme
}
```

å¤§å¤šæ•°æ§åˆ¶å™¨æœ€ç»ˆéƒ½åœ¨é›†ç¾¤ä¸Šè¿è¡Œï¼Œå› æ­¤å®ƒä»¬éœ€è¦RBACæƒé™ï¼Œæˆ‘ä»¬ä½¿ç”¨controller-tools RBACæ ‡è®°æ¥æŒ‡å®šè¿™äº›æƒé™ã€‚è¿™äº›æ˜¯è¿è¡Œæ‰€éœ€çš„æœ€ä½æƒé™ã€‚å½“æˆ‘ä»¬æ·»åŠ æ›´å¤šåŠŸèƒ½æ—¶ï¼Œæˆ‘ä»¬å°†éœ€è¦é‡æ–°è®¿é—®è¿™äº›ã€‚

```go
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch
```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡controller-genä»ä¸Šè¿°æ ‡è®°ç”Ÿæˆ `config/rbac/role.yaml` å¤„çš„ `ClusterRole` æ¸…å•ï¼š

```bash
make manifests
```



### å®ç° Controller

æˆ‘ä»¬é€šè¿‡å®ç° Controller é€»è¾‘å»åˆ›å»º Podï¼Œåœ¨ `internal/controller/guestbook_controller.go`

æˆ‘ä»¬çš„CronJobæ§åˆ¶å™¨çš„åŸºæœ¬é€»è¾‘æ˜¯è¿™æ ·çš„ï¼š

1. åŠ è½½å‘½åçš„ CronJob
2. åˆ—å‡ºæ‰€æœ‰æ´»åŠ¨ jobs å¹¶æ›´æ–°çŠ¶æ€
3. æ ¹æ®å†å²é™åˆ¶æ¸…ç† history limits
4. æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦è¢«æš‚åœï¼ˆå¦‚æœæ˜¯ï¼Œä¸è¦åšä»»ä½•å…¶ä»–äº‹æƒ…ï¼‰
5. è·å–ä¸‹ä¸€æ¬¡è®¡åˆ’è¿è¡Œ
6. è¿è¡Œä¸€ä¸ªæ–°çš„ jobï¼Œå¦‚æœå®ƒæ˜¯æŒ‰è®¡åˆ’è¿›è¡Œçš„ï¼Œæ²¡æœ‰è¶…è¿‡æˆªæ­¢æ—¥æœŸï¼Œä¹Ÿæ²¡æœ‰è¢«æˆ‘ä»¬çš„å¹¶å‘ç­–ç•¥é˜»å¡
7. å½“æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„ jobï¼ˆè‡ªåŠ¨å®Œæˆï¼‰æˆ–è€…åˆ°äº†ä¸‹ä¸€ä¸ªè®¡åˆ’è¿è¡Œçš„æ—¶é—´æ—¶ï¼Œé‡æ–°æ’é˜Ÿã€‚



æˆ‘ä»¬å…ˆä»è¿›å£å¼€å§‹ã€‚ä¸‹é¢ä½ ä¼šçœ‹åˆ°ï¼Œæˆ‘ä»¬éœ€è¦æ¯”é‚£äº›è„šæ‰‹æ¶æ›´å¤šçš„è¿›å£ã€‚æˆ‘ä»¬å°†åœ¨ä½¿ç”¨æ—¶è®¨è®ºæ¯ä¸€ä¸ªã€‚

```go
package controllers

import (
    "context"
    "fmt"
    "sort"
    "time"

    "github.com/robfig/cron"
    kbatch "k8s.io/api/batch/v1"
    corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/apimachinery/pkg/runtime"
    ref "k8s.io/client-go/tools/reference"
    ctrl "sigs.k8s.io/controller-runtime"
    "sigs.k8s.io/controller-runtime/pkg/client"
    "sigs.k8s.io/controller-runtime/pkg/log"

    batchv1 "tutorial.kubebuilder.io/project/api/v1"
)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ—¶é’Ÿï¼Œè¿™å°†å…è®¸æˆ‘ä»¬åœ¨æµ‹è¯•ä¸­ä¼ªé€ æ—¶é—´ã€‚

```go
// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Scheme *runtime.Scheme
    Clock
}
// Clock
```

æˆ‘ä»¬å°†æ¨¡æ‹Ÿæ—¶é’Ÿï¼Œä»¥ä¾¿åœ¨æµ‹è¯•æ—¶æ›´å®¹æ˜“åœ¨æ—¶é—´ä¸Šè·³è·ƒï¼Œâ€œçœŸå®çš„â€ æ—¶é’Ÿåªæ˜¯è°ƒç”¨ `time.Now` ã€‚

```go
type realClock struct{}

func (_ realClock) Now() time.Time { return time.Now() }

// clock knows how to get the current time.
// It can be used to fake out timing for testing.
type Clock interface {
    Now() time.Time
}
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›RBACæƒé™--å› ä¸ºæˆ‘ä»¬ç°åœ¨æ­£åœ¨åˆ›å»ºå’Œç®¡ç†ä½œä¸šï¼Œæ‰€ä»¥éœ€è¦è¿™äº›æƒé™ï¼Œè¿™æ„å‘³ç€è¦æ·»åŠ ä¸€äº›æ ‡è®°ã€‚

```go
//+kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/finalizers,verbs=update
//+kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=batch,resources=jobs/status,verbs=get
```

ç°åœ¨ï¼Œæˆ‘ä»¬è¿›å…¥æ§åˆ¶å™¨çš„æ ¸å¿ƒ--åè°ƒå™¨é€»è¾‘ã€‚

```go
var (
    scheduledTimeAnnotation = "batch.tutorial.kubebuilder.io/scheduled-at"
)

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
// TODO(user): Modify the Reconcile function to compare the state specified by
// the CronJob object against the actual cluster state, and then
// perform operations to make the cluster state reflect the state specified by
// the user.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.13.0/pkg/reconcile
func (r *CronJobReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    log := log.FromContext(ctx)
```



### æŒ‰åç§°åŠ è½½CronJob

æˆ‘ä»¬å°†ä½¿ç”¨å®¢æˆ·ç«¯è·å–CronJobã€‚æ‰€æœ‰å®¢æˆ·ç«¯æ–¹æ³•éƒ½å°†ä¸Šä¸‹æ–‡ï¼ˆä»¥å…è®¸å–æ¶ˆï¼‰ä½œä¸ºå…¶ç¬¬ä¸€ä¸ªå‚æ•°ï¼Œå¹¶å°†æœ‰é—®é¢˜çš„å¯¹è±¡ä½œä¸ºå…¶æœ€åä¸€ä¸ªå‚æ•°ã€‚Getæœ‰ç‚¹ç‰¹æ®Šï¼Œå®ƒå°† `NamespacedName` ä½œä¸ºä¸­é—´å‚æ•°ï¼ˆå¤§å¤šæ•°æ²¡æœ‰ä¸­é—´å‚æ•°ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹é¢çœ‹åˆ°ï¼‰ã€‚

è®¸å¤šå®¢æˆ·ç«¯æ–¹æ³•ä¹Ÿåœ¨æœ€åé‡‡ç”¨å¯å˜å‚æ•°é€‰é¡¹ã€‚

```go
var cronJob batchv1.CronJob
if err := r.Get(ctx, req.NamespacedName, &cronJob); err != nil {
    log.Error(err, "unable to fetch CronJob")
    // we'll ignore not-found errors, since they can't be fixed by an immediate
    // requeue (we'll need to wait for a new notification), and we can get them
    // on deleted requests.
    return ctrl.Result{}, client.IgnoreNotFound(err)
}
```



### åˆ—å‡ºæ‰€æœ‰æ´»åŠ¨ä½œä¸šï¼Œå¹¶æ›´æ–°çŠ¶æ€

è¦å®Œå…¨æ›´æ–°çŠ¶æ€ï¼Œæˆ‘ä»¬éœ€è¦åˆ—å‡ºæ­¤å‘½åç©ºé—´ä¸­å±äºæ­¤CronJobçš„æ‰€æœ‰å­ä½œä¸šã€‚ä¸Getç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Listæ–¹æ³•åˆ—å‡ºå­ä½œä¸šã€‚

```go
    var childJobs kbatch.JobList
    if err := r.List(ctx, &childJobs, client.InNamespace(req.Namespace), client.MatchingFields{jobOwnerKey: req.Name}); err != nil {
        log.Error(err, "unable to list child Jobs")
        return ctrl.Result{}, err
    }
```

ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰çš„ä½œä¸šï¼Œæˆ‘ä»¬å°†æŠŠå®ƒä»¬åˆ†æˆæ´»åŠ¨çš„ã€æˆåŠŸçš„å’Œå¤±è´¥çš„ä½œä¸šï¼Œè·Ÿè¸ªæœ€è¿‘çš„è¿è¡Œï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨çŠ¶æ€ä¸­è®°å½•å®ƒã€‚

è¯·è®°ä½ï¼ŒçŠ¶æ€åº”è¯¥èƒ½å¤Ÿä»ä¸–ç•Œçš„çŠ¶æ€ä¸­é‡æ–°æ„å»ºï¼Œå› æ­¤é€šå¸¸ä»æ ¹å¯¹è±¡çš„çŠ¶æ€ä¸­è¯»å–ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ç›¸åï¼Œä½ åº”è¯¥åœ¨æ¯æ¬¡è¿è¡Œæ—¶é‡æ–°æ„å»ºå®ƒã€‚è¿™å°±æ˜¯æˆ‘ä»¬è¦åšçš„ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çŠ¶æ€æ¡ä»¶æ£€æŸ¥ä½œä¸šæ˜¯å¦â€œå®Œæˆâ€ä»¥åŠå®ƒæ˜¯æˆåŠŸè¿˜æ˜¯å¤±è´¥ã€‚æˆ‘ä»¬å°†æŠŠè¿™ä¸ªé€»è¾‘æ”¾åœ¨ä¸€ä¸ªå¸®åŠ©å™¨ä¸­ï¼Œä»¥ä½¿æˆ‘ä»¬çš„ä»£ç æ›´æ¸…æ™°ã€‚

```go
    // find the active list of jobs
    var activeJobs []*kbatch.Job
    var successfulJobs []*kbatch.Job
    var failedJobs []*kbatch.Job
    var mostRecentTime *time.Time // find the last run so we can update the status
// isJobFinished
// getScheduledTimeForJob
    for i, job := range childJobs.Items {
        _, finishedType := isJobFinished(&job)
        switch finishedType {
        case "": // ongoing
            activeJobs = append(activeJobs, &childJobs.Items[i])
        case kbatch.JobFailed:
            failedJobs = append(failedJobs, &childJobs.Items[i])
        case kbatch.JobComplete:
            successfulJobs = append(successfulJobs, &childJobs.Items[i])
        }

        // We'll store the launch time in an annotation, so we'll reconstitute that from
        // the active jobs themselves.
        scheduledTimeForJob, err := getScheduledTimeForJob(&job)
        if err != nil {
            log.Error(err, "unable to parse schedule time for child job", "job", &job)
            continue
        }
        if scheduledTimeForJob != nil {
            if mostRecentTime == nil {
                mostRecentTime = scheduledTimeForJob
            } else if mostRecentTime.Before(*scheduledTimeForJob) {
                mostRecentTime = scheduledTimeForJob
            }
        }
    }

    if mostRecentTime != nil {
        cronJob.Status.LastScheduleTime = &metav1.Time{Time: *mostRecentTime}
    } else {
        cronJob.Status.LastScheduleTime = nil
    }
    cronJob.Status.Active = nil
    for _, activeJob := range activeJobs {
        jobRef, err := ref.GetReference(r.Scheme, activeJob)
        if err != nil {
            log.Error(err, "unable to make reference to active job", "job", activeJob)
            continue
        }
        cronJob.Status.Active = append(cronJob.Status.Active, *jobRef)
    }
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®°å½•æˆ‘ä»¬åœ¨ç¨é«˜çš„æ—¥å¿—çº§åˆ«ä¸Šè§‚å¯Ÿåˆ°çš„ä½œä¸šæ•°é‡ï¼Œä»¥ä¾¿è¿›è¡Œè°ƒè¯•ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸æ˜¯ä½¿ç”¨æ ¼å¼å­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ä½¿ç”¨å›ºå®šæ¶ˆæ¯ï¼Œå¹¶é™„åŠ å¸¦æœ‰é¢å¤–ä¿¡æ¯çš„é”®å€¼å¯¹ã€‚è¿™ä½¿å¾—è¿‡æ»¤å’ŒæŸ¥è¯¢æ—¥å¿—è¡Œæ›´åŠ å®¹æ˜“ã€‚

```go
    log.V(1).Info("job count", "active jobs", len(activeJobs), "successful jobs", len(successfulJobs), "failed jobs", len(failedJobs))
```

ä½¿ç”¨æˆ‘ä»¬æ”¶é›†çš„æ•°æ®ï¼Œæˆ‘ä»¬å°†æ›´æ–°CRDçš„çŠ¶æ€ã€‚å°±åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬åˆ©ç”¨æˆ‘ä»¬çš„å®¢æˆ·ã€‚ä¸ºäº†ä¸“é—¨æ›´æ–°çŠ¶æ€å­èµ„æºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®¢æˆ·ç«¯çš„ `Status` éƒ¨åˆ†å’Œ `Update` æ–¹æ³•ã€‚

statuså­èµ„æºå¿½ç•¥å¯¹specçš„æ›´æ”¹ï¼Œå› æ­¤å®ƒä¸å¤ªå¯èƒ½ä¸ä»»ä½•å…¶ä»–æ›´æ–°å†²çªï¼Œå¹¶ä¸”å¯ä»¥å…·æœ‰å•ç‹¬çš„æƒé™ã€‚

```go
    if err := r.Status().Update(ctx, &cronJob); err != nil {
        log.Error(err, "unable to update CronJob status")
        return ctrl.Result{}, err
    }
```

ä¸€æ—¦æˆ‘ä»¬æ›´æ–°äº†æˆ‘ä»¬çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç»§ç»­ç¡®ä¿ä¸–ç•Œçš„çŠ¶æ€ä¸æˆ‘ä»¬åœ¨è§„èŒƒä¸­æƒ³è¦çš„ç›¸åŒ¹é…ã€‚



### æ ¹æ®å†å²é™åˆ¶æ¸…ç†æ—§ä½œä¸š

é¦–å…ˆï¼Œæˆ‘ä»¬å°†åŠªåŠ›æ¸…ç†æ—§çš„å·¥ä½œï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸ä¼šç•™ä¸‹å¤ªå¤šçš„é—²ç½®å·¥ä½œã€‚

```go
// NB: deleting these are "best effort" -- if we fail on a particular one,
    // we won't requeue just to finish the deleting.
    if cronJob.Spec.FailedJobsHistoryLimit != nil {
        sort.Slice(failedJobs, func(i, j int) bool {
            if failedJobs[i].Status.StartTime == nil {
                return failedJobs[j].Status.StartTime != nil
            }
            return failedJobs[i].Status.StartTime.Before(failedJobs[j].Status.StartTime)
        })
        for i, job := range failedJobs {
            if int32(i) >= int32(len(failedJobs))-*cronJob.Spec.FailedJobsHistoryLimit {
                break
            }
            if err := r.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground)); client.IgnoreNotFound(err) != nil {
                log.Error(err, "unable to delete old failed job", "job", job)
            } else {
                log.V(0).Info("deleted old failed job", "job", job)
            }
        }
    }

    if cronJob.Spec.SuccessfulJobsHistoryLimit != nil {
        sort.Slice(successfulJobs, func(i, j int) bool {
            if successfulJobs[i].Status.StartTime == nil {
                return successfulJobs[j].Status.StartTime != nil
            }
            return successfulJobs[i].Status.StartTime.Before(successfulJobs[j].Status.StartTime)
        })
        for i, job := range successfulJobs {
            if int32(i) >= int32(len(successfulJobs))-*cronJob.Spec.SuccessfulJobsHistoryLimit {
                break
            }
            if err := r.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground)); (err) != nil {
                log.Error(err, "unable to delete old successful job", "job", job)
            } else {
                log.V(0).Info("deleted old successful job", "job", job)
            }
        }
    }
```



###  æ£€æŸ¥æ˜¯å¦åœæ­¢

å¦‚æœè¿™ä¸ªå¯¹è±¡è¢«æŒ‚èµ·ï¼Œæˆ‘ä»¬ä¸æƒ³è¿è¡Œä»»ä½•ä½œä¸šï¼Œæ‰€ä»¥æˆ‘ä»¬ç°åœ¨åœæ­¢ã€‚å¦‚æœæˆ‘ä»¬æ­£åœ¨è¿è¡Œçš„ä½œä¸šå‡ºç°æ•…éšœï¼Œå¹¶ä¸”æˆ‘ä»¬å¸Œæœ›æš‚åœè¿è¡Œä»¥è¿›è¡Œè°ƒæŸ¥æˆ–å¤„ç†é›†ç¾¤ï¼Œè€Œä¸åˆ é™¤å¯¹è±¡ï¼Œåˆ™æ­¤åŠŸèƒ½éå¸¸æœ‰ç”¨ã€‚

```go
if cronJob.Spec.Suspend != nil && *cronJob.Spec.Suspend {
    log.V(1).Info("cronjob suspended, skipping")
    return ctrl.Result{}, nil
}
```



### è·å–ä¸‹ä¸€æ¬¡è®¡åˆ’è¿è¡Œ

å¦‚æœæˆ‘ä»¬æ²¡æœ‰æš‚åœï¼Œæˆ‘ä»¬å°†éœ€è¦è®¡ç®—ä¸‹ä¸€ä¸ªè®¡åˆ’çš„è¿è¡Œï¼Œä»¥åŠæˆ‘ä»¬æ˜¯å¦æœ‰ä¸€ä¸ªå°šæœªå¤„ç†çš„è¿è¡Œã€‚

```go
// getNextSchedule
    // figure out the next times that we need to create
    // jobs at (or anything we missed).
    missedRun, nextRun, err := getNextSchedule(&cronJob, r.Now())
    if err != nil {
        log.Error(err, "unable to figure out CronJob schedule")
        // we don't really care about requeuing until we get an update that
        // fixes the schedule, so don't return an error
        return ctrl.Result{}, nil
    }
```

æˆ‘ä»¬å°†å‡†å¤‡æœ€ç»ˆçš„è¯·æ±‚ï¼Œä»¥ä¾¿åœ¨ä¸‹ä¸€ä¸ªä½œä¸šä¹‹å‰é‡æ–°æ’é˜Ÿï¼Œç„¶åç¡®å®šæˆ‘ä»¬æ˜¯å¦å®é™…éœ€è¦è¿è¡Œã€‚

```go
    scheduledResult := ctrl.Result{RequeueAfter: nextRun.Sub(r.Now())} // save this so we can re-use it elsewhere
    log = log.WithValues("now", r.Now(), "next run", nextRun)
```



### å¹¶å‘ç­–ç•¥

**è¿è¡Œä¸€ä¸ªæ–°çš„ä½œä¸šï¼Œå¦‚æœå®ƒæ˜¯æŒ‰è®¡åˆ’è¿›è¡Œçš„ï¼Œæ²¡æœ‰è¶…è¿‡æˆªæ­¢æ—¥æœŸï¼Œä¹Ÿæ²¡æœ‰è¢«æˆ‘ä»¬çš„å¹¶å‘ç­–ç•¥é˜»å¡**

å¦‚æœæˆ‘ä»¬é”™è¿‡äº†ä¸€ä¸ªè¿è¡Œï¼Œå¹¶ä¸”æˆ‘ä»¬ä»ç„¶åœ¨å¼€å§‹å®ƒçš„æˆªæ­¢æ—¥æœŸå†…ï¼Œæˆ‘ä»¬å°†éœ€è¦è¿è¡Œä¸€ä¸ªä½œä¸šã€‚

```go
    if missedRun.IsZero() {
        log.V(1).Info("no upcoming scheduled times, sleeping until next")
        return scheduledResult, nil
    }

    // make sure we're not too late to start the run
    log = log.WithValues("current run", missedRun)
    tooLate := false
    if cronJob.Spec.StartingDeadlineSeconds != nil {
        tooLate = missedRun.Add(time.Duration(*cronJob.Spec.StartingDeadlineSeconds) * time.Second).Before(r.Now())
    }
    if tooLate {
        log.V(1).Info("missed starting deadline for last run, sleeping till next")
        // TODO(directxman12): events
        return scheduledResult, nil
    }
```

å¦‚æœæˆ‘ä»¬å®é™…ä¸Šå¿…é¡»è¿è¡Œä¸€ä¸ªä½œä¸šï¼Œæˆ‘ä»¬éœ€è¦ç­‰å¾…ç°æœ‰çš„ä½œä¸šå®Œæˆï¼Œæ›¿æ¢ç°æœ‰çš„ä½œä¸šï¼Œæˆ–è€…åªæ˜¯æ·»åŠ æ–°çš„ä½œä¸šã€‚å¦‚æœæˆ‘ä»¬çš„ä¿¡æ¯ç”±äºç¼“å­˜å»¶è¿Ÿè€Œè¿‡æœŸï¼Œæˆ‘ä»¬å°†åœ¨è·å¾—æœ€æ–°ä¿¡æ¯æ—¶é‡æ–°æ’é˜Ÿã€‚

```go
    // figure out how to run this job -- concurrency policy might forbid us from running
    // multiple at the same time...
    if cronJob.Spec.ConcurrencyPolicy == batchv1.ForbidConcurrent && len(activeJobs) > 0 {
        log.V(1).Info("concurrency policy blocks concurrent runs, skipping", "num active", len(activeJobs))
        return scheduledResult, nil
    }

    // ...or instruct us to replace existing ones...
    if cronJob.Spec.ConcurrencyPolicy == batchv1.ReplaceConcurrent {
        for _, activeJob := range activeJobs {
            // we don't care if the job was already deleted
            if err := r.Delete(ctx, activeJob, client.PropagationPolicy(metav1.DeletePropagationBackground)); client.IgnoreNotFound(err) != nil {
                log.Error(err, "unable to delete active job", "job", activeJob)
                return ctrl.Result{}, err
            }
        }
    }
```

ä¸€æ—¦æˆ‘ä»¬å¼„æ¸…æ¥šäº†å¦‚ä½•å¤„ç†ç°æœ‰çš„å·¥ä½œï¼Œæˆ‘ä»¬å®é™…ä¸Šå°±ä¼šåˆ›é€ å‡ºæˆ‘ä»¬æƒ³è¦çš„å·¥ä½œ

```go
// constructJobForCronJob
    // actually make the job...
    job, err := constructJobForCronJob(&cronJob, missedRun)
    if err != nil {
        log.Error(err, "unable to construct job from template")
        // don't bother requeuing until we get a change to the spec
        return scheduledResult, nil
    }

    // ...and create it on the cluster
    if err := r.Create(ctx, job); err != nil {
        log.Error(err, "unable to create Job for CronJob", "job", job)
        return ctrl.Result{}, err
    }

    log.V(1).Info("created Job for CronJob run", "job", job)
```



###  å½“æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„ä½œä¸šæˆ–è€…åˆ°äº†ä¸‹ä¸€ä¸ªè®¡åˆ’è¿è¡Œçš„æ—¶é—´æ—¶é‡æ–°æ’é˜Ÿ

æœ€åï¼Œæˆ‘ä»¬å°†è¿”å›ä¸Šé¢å‡†å¤‡çš„ç»“æœï¼Œå®ƒè¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›åœ¨ä¸‹ä¸€æ¬¡è¿è¡Œæ—¶é‡æ–°æ’é˜Ÿã€‚

è¿™æ˜¯ä¸€ä¸ªæœ€å¤§çš„æœŸé™--å¦‚æœåœ¨è¿™æœŸé—´å‘ç”Ÿäº†å…¶ä»–å˜åŒ–ï¼Œæ¯”å¦‚æˆ‘ä»¬çš„å·¥ä½œå¼€å§‹æˆ–ç»“æŸï¼Œæˆ‘ä»¬è¢«ä¿®æ”¹ï¼Œç­‰ç­‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæ›´å¿«åœ°å†æ¬¡åè°ƒã€‚

```go
    // we'll requeue once we see the running job, and update our status
    return scheduledResult, nil
}
```



### å¯åŠ¨ Controller

æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥å¯åŠ¨è¿™ä¸ª Controller æ¥çœ‹ä¸€ä¸‹æ•ˆæœ

ç›´æ¥è¿è¡Œ `make run` å³å¯è¿è¡Œä»£ç ï¼Œå†ç»ˆç«¯ä¸­å¯ä»¥çœ‹åˆ°æ—¥å¿—è¾“å‡ºã€‚

æŸ¥çœ‹ Pod æ˜¯å¦æˆåŠŸåˆ›å»ºå‡ºæ¥äº†ï¼š

```bash
kubectl get pod 
```



### éƒ¨ç½² Controller

æˆ‘ä»¬å°†å…¶éƒ¨ç½²äº†äº†ä¸€ä¸ª ç¤ºä¾‹ï¼Œä¹Ÿè¿è¡Œ Controller çœ‹åˆ°äº†ç›¸åº”çš„ å‰¯æœ¬ Pod è¢«åˆ›å»ºå‡ºæ¥äº†ï¼Œç°åœ¨æˆ‘ä»¬è¿›ä¸€æ­¥æ¨¡æ‹Ÿ Operator å®é™…ä½¿ç”¨æ—¶çš„éƒ¨ç½²æ–¹å¼ï¼ŒæŠŠ Controller æ‰“åŒ…åä»¥ container çš„æ–¹å¼éƒ¨ç½²åˆ° Kubernetes é›†ç¾¤ç¯å¢ƒä¸­ï¼š

```bash
# æ„å»ºé•œåƒ
make docker-build IMG=application-operator:v0.0.1

# æ¨é€åˆ° Kind ç¯å¢ƒ
kind load docker-image

# éƒ¨ç½² controller 
make deploy IMG=application-operator:v0.0.1
```



**è¡¥å……ï¼šæˆ‘ä»¬å¯ä»¥åœ¨ dockerfile ä¸­è§£å†³Goè¯­è¨€çš„ä»£ç†é—®é¢˜ï¼š**

```bash
ENV GOPROXY=https://goproxy.op
```



### èµ„æºæ¸…ç†

ä¸Šé¢æœ‰è®²è¿‡ï¼Œç”¨ Makefile å‘½ä»¤æ¸…ç†è¿˜æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œæˆ‘ä»¬ç›´æ¥æ¸…ç†å°±å¥½äº†ï¼š

```bash
# å¸è½½ controller
make undeploy

# å¸è½½ CRD
make uninstall
```



## Client-go

Kubeilder å·²ç»å±è”½äº† client-go çš„ç»†èŠ‚ï¼Œä½†æ˜¯å¦‚æœå¸Œæœ›æ·±å…¥æŒæ¡ Operator å¼€å‘æœºåˆ¶ï¼Œè¿˜æ˜¯éœ€è¦å¯¹ Client-go ç†Ÿæ‚‰çš„ã€‚

è¿™æ˜¯ä¸€ç¯‡è¿˜æ²¡å…¥é—¨çš„æ¦‚å¿µäº†è§£ç¬”è®°ï¼š

+ [ç¬”è®°éƒ¨åˆ† ~](https://docker.nsddd.top/Cloud-Native-k8s/35.html)

Kubernetes APIæ˜¯ä¸€ç»„REST APIï¼Œç”¨äºä¸Kubernetesé›†ç¾¤äº¤äº’ã€‚è¿™äº›APIå…è®¸å¼€å‘äººå‘˜æ‰§è¡Œå„ç§æ“ä½œï¼ŒåŒ…æ‹¬ç®¡ç†Podã€Deploymentã€Serviceã€Namespaceç­‰ã€‚Kubernetes APIç”±ä¸€ç»„èµ„æºå¯¹è±¡è¡¨ç¤ºï¼Œä¾‹å¦‚Podã€Serviceã€ReplicaSetç­‰ã€‚è¿™äº›èµ„æºå¯¹è±¡ç”±Kubernetes API Serverç®¡ç†ï¼Œå¹¶å¯ä»¥é€šè¿‡kubectlç­‰å·¥å…·è¿›è¡ŒæŸ¥è¯¢å’Œä¿®æ”¹ã€‚

| æ–¹å¼                                                 | ç‰¹ç‚¹                                                         | æ”¯æŒè€…   |
| ---------------------------------------------------- | ------------------------------------------------------------ | -------- |
| Kubernetes dashboard                                 | ç›´æ¥é€šè¿‡Web UIè¿›è¡Œæ“ä½œï¼Œç®€å•ç›´æ¥ï¼Œå¯å®šåˆ¶åŒ–ç¨‹åº¦ä½             | å®˜æ–¹æ”¯æŒ |
| kubectl                                              | å‘½ä»¤è¡Œæ“ä½œï¼ŒåŠŸèƒ½æœ€å…¨ï¼Œä½†æ˜¯æ¯”è¾ƒå¤æ‚ï¼Œé€‚åˆå¯¹å…¶è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†è£…ï¼Œå®šåˆ¶åŠŸèƒ½ï¼Œç‰ˆæœ¬é€‚é…æœ€å¥½ | å®˜æ–¹æ”¯æŒ |
| [client-go](https://github.com/kubernetes/client-go) | ä»kubernetesçš„ä»£ç ä¸­æŠ½ç¦»å‡ºæ¥çš„å®¢æˆ·ç«¯åŒ…ï¼Œç®€å•æ˜“ç”¨ï¼Œä½†éœ€è¦å°å¿ƒåŒºåˆ†kubernetesçš„APIç‰ˆæœ¬ | å®˜æ–¹æ”¯æŒ |

> Kubernetes API Server æä¾›çš„æ˜¯é»˜è®¤çš„ HTTPS æœåŠ¡ï¼Œè€Œä¸”æ˜¯åŒå‘çš„ TLS è®¤è¯ï¼Œè€Œæˆ‘ä»¬ç›®å‰çš„å…³æ³¨ç‚¹æ˜¯ API æœ¬èº«ï¼Œå› æ­¤å…ˆé€šè¿‡ Kubectl æ¥ä»£ç† API Server æœåŠ¡ã€‚

```bash
â¯ kubectl proxy --port=8080
```

> æ¥ä¸‹æ¥å°±å¯ä»¥é€šè¿‡ç®€å•çš„ HTTP è¯·æ±‚æ¥å’Œ API Server äº¤äº’äº†ï¼š

```bash
â¯ curl localhost:8080/version
{
  "major": "1",
  "minor": "19",
  "gitVersion": "v1.19.16",
  "gitCommit": "e37e4ab4cc8dcda84f1344dda47a97bb1927d074",
  "gitTreeState": "clean",
  "buildDate": "2022-10-26T15:40:32Z",
  "goVersion": "go1.15.15",
  "compiler": "gc",
  "platform": "linux/amd64"
}#
```

æˆ‘ä»¬å¯èƒ½è¿˜éœ€è¦ä¸€ä¸ªé…ç½®æ–‡ä»¶æ¥æè¿° Deployment èµ„æºï¼Œåœ¨æœ¬åœ°åˆ›å»ºä¸€ä¸ª `nginx-deploy.yaml` æ–‡ä»¶:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```



æˆ‘ä»¬åˆ›å»ºå®ƒï¼š

```bash
â¯ kubectl create -f nginx-deploy.yaml
deployment.apps/nginx-deployment created
â¯ k get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
my-nginx-app       0/3     3            0           23h
nginx-deployment   3/3     3            3           19s
```



Deployment åˆ›å»ºçš„ API å’Œ åœ¨ Kubernetes ä¸­ API çš„è·¯å¾„ä¸€æ ·ï¼š

```
POST /apis/apps/v1/namespace/{namespace}/deployment
```

> Kubernetesä¸­ä½¿ç”¨çš„ RESTful æ¥å£æ›´æ–°è¿™äº›å¯¹è±¡ï¼ŒåŒ…æ‹¬æ“ä½œï¼š
>
> ```markdown
> - GETï¼šä»æœåŠ¡å™¨è¯»å–ä¸€ä¸ªèµ„æºã€‚
> - POSTï¼šåœ¨æœåŠ¡å™¨ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„èµ„æºã€‚
> - PUTï¼šåœ¨æœåŠ¡å™¨ä¸Šæ›´æ–°ä¸€ä¸ªèµ„æºã€‚
> - DELETEï¼šä»æœåŠ¡å™¨åˆ é™¤ä¸€ä¸ªèµ„æºã€‚
> - HEADï¼šä»æœåŠ¡å™¨è¯»å–ä¸€ä¸ªèµ„æºçš„å¤´ä¿¡æ¯ã€‚
> - PATCHï¼šåœ¨æœåŠ¡å™¨ä¸Šéƒ¨åˆ†æ›´æ–°ä¸€ä¸ªèµ„æºã€‚
> - OPTIONSï¼šåˆ—å‡ºæœåŠ¡å™¨æ”¯æŒçš„æ–¹æ³•å’ŒåŠŸèƒ½ã€‚
> ```

æ—¢ç„¶æƒ³å°è¯• kubectlï¼Œé‚£ä¹ˆæ¥ç‚¹æ–°èŠ±æ ·ï¼š

```bash
â¯ curl localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deploymen
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {

  },
  "status": "Failure",
  "message": "deployments.apps \"nginx-deploymen\" not found",
  "reason": "NotFound",
  "details": {
    "name": "nginx-deploymen",
    "group": "apps",
    "kind": "deployments"
  },
  "code": 404
}#
```



**èµ„æºåˆ›å»ºï¼š**

Depolyment çš„åˆ›å»º API æ˜¯ï¼š`apps/v1` ä¸­çš„ Deploymentã€‚

```bash
POST /apis/apps/v1/namespaces/{namespace}/deployments
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åœ¨ default å‘½åç©ºé—´ä¸‹åˆ›å»ºä¸€ä¸ª deploymentï¼š

```bash
â¯ curl -X POST --header 'Content-Type: application/yaml' --data-binary @nginx-deploy.yaml http://localhost:8080/apis/apps/v1/namespaces/default/deployments
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {

  },
  "status": "Failure",
  "message": "deployments.apps \"nginx-deployment\" already exists",
  "reason": "AlreadyExists",
  "details": {
    "name": "nginx-deployment",
    "group": "apps",
    "kind": "deployments"
  },
  "code": 409
}#
```



## sample-controller

æˆ‘ä»¬å°†ç”¨äºå®éªŒåˆ›å»ºæ“ä½œç¬¦çš„ç¬¬ä¸€ä¸ªå·¥å…·æ˜¯sample-controllerï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼šhttps://github.com/kubernetes/sample-controllerã€‚

è¿™ä¸ªé¡¹ç›®ä¸º `Foo` ç±»å‹å®ç°äº†ä¸€ä¸ªç®€å•çš„æ“ä½œç¬¦ï¼Œå½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰å¯¹è±¡ `foo` æ—¶ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ä¸€äº›å…¬å…±Dockeré•œåƒå’Œç‰¹å®šæ•°é‡å‰¯æœ¬çš„éƒ¨ç½²ã€‚

æˆ‘ä»¬åœ¨ä¸Šé¢å·²ç»ä¸‹è½½è¿‡ï¼Œç°åœ¨æˆ‘å°è¯•ç¼–è¯‘å®ƒï¼š

> go version: ` gvm use go1.20`

```bash
â¯ export SAMPLE=$(pwd)/sample-controller
â¯ git clone https://github.com/cubxxw/sample-controller.git $SAMPLE  && cd $SAMPLE;
â¯ go build -o ctrl .
```

ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª CRDï¼š

```bash
â¯ kubectl apply -f artifacts/examples/crd-status-subresource.yaml
customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io created
â¯ k get CustomResourceDefinition
NAME                           CREATED AT
crontabs.stable.example.com    2023-04-07T09:17:55Z
foos.samplecontroller.k8s.io   2023-04-08T12:09:46Z
guestbooks.webapp.my.domain    2023-04-07T15:42:22Z
```

ç„¶åæˆ‘ä»¬è¿è¡Œæ§åˆ¶å™¨ï¼š

```bash
./ctrl -kubeconfig ~/.kube/config  -logtostderr=true
```

> `-logtostderr=true`ï¼šå°†æ—¥å¿—è®°å½•åˆ°æ ‡å‡†é”™è¯¯è€Œä¸æ˜¯æ–‡ä»¶(é»˜è®¤ä¸ºtrue)

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¦ä¸€ä¸ªç»ˆç«¯ä¸Šæ“ä½œ `Foo` å¯¹è±¡ï¼Œçœ‹çœ‹æ§åˆ¶å™¨ä¼šå‘ç”Ÿä»€ä¹ˆï¼š

```bash
â¯ kubectl apply -f artifacts/examples/example-foo.yaml
foo.samplecontroller.k8s.io/example-foo created
â¯ k get Foo
NAME          AGE
example-foo   9s

# ----- åˆ é™¤ Foo -------
â¯ kubectl delete -f artifacts/examples/example-foo.yaml
foo.samplecontroller.k8s.io "example-foo" deleted
â¯ k get pod | grep -i "Foo"
â¯ k get Foo
No resources found in default namespace.
```

> åœ¨ç¼–å†™å’Œä½¿ç”¨Kubernetes 1.11.0æ—¶ï¼Œæ§åˆ¶å™¨å°†åœ¨åˆ›å»ºéƒ¨ç½²åæ›´æ–° `foo` å¯¹è±¡çš„çŠ¶æ€æ—¶è¿›å…¥æ— é™å¾ªç¯ï¼šåœ¨ `updateFooStatus` å‡½æ•°ä¸­ï¼Œä½ å¿…é¡»é€šè¿‡è°ƒç”¨ `UpdateStatus(fooCopy)` æ¥æ›´æ”¹å¯¹ `Update(fooCopy)` çš„è°ƒç”¨ã€‚

**å¾ˆå¥½ç†è§£ä¸æ˜¯å—ï¼Œapply å£°æ˜å¼åœ¨ controller ä¸­ä¹Ÿæ˜¯é€šè¿‡ for å¾ªç¯ä¸æ–­çš„è¿›è¡Œæ ¡éªŒã€‚æ£€æŸ¥ status å’Œ spec çš„åŒºåˆ«ï¼Œæ˜¯å¦è¾¾æˆä¸€è‡´ã€‚**

æˆ‘æ‰¾å‡ºå®˜æ–¹çš„æœ€ç®€å•çš„æ¡ˆä¾‹æ‹¿å‡ºæ¥ï¼š

```go
for {
  desired := getDesiredState()	// æœŸæœ›çŠ¶æ€
  current := getCurrentState()	// å®é™…çŠ¶æ€
  makeChanges(desired, current) // è°ƒè°è¿‡ç¨‹
}
```

> å¯ä»¥çœ‹å‡ºæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªå¾ªç¯ï¼Œé€šè¿‡ `getDesiredState()` è·å–åˆ° spec ä¸­çš„æœŸæœ›çŠ¶æ€ï¼Œé€šè¿‡ `getCurrentState()` è·å–åˆ° status å½“å‰é›†ç¾¤çš„å®é™…çŠ¶æ€ï¼Œç„¶åè¿›è¡Œè°ƒè°ã€‚

åˆ°ç›®å‰ä¸ºæ­¢ä¸€åˆ‡é¡ºåˆ©ï¼Œæ§åˆ¶å™¨ä½¿å·¥ä½œï¼šå½“æˆ‘ä»¬åˆ›å»º `foo` å¯¹è±¡æ—¶ï¼Œå®ƒä¼šåˆ›å»ºä¸€ä¸ªéƒ¨ç½²ï¼Œå½“æˆ‘ä»¬åˆ é™¤è¯¥å¯¹è±¡æ—¶ï¼Œå®ƒä¼šåœæ­¢éƒ¨ç½²ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥è°ƒæ•´CRDå’Œæ§åˆ¶å™¨ï¼Œä»¥ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„è‡ªå®šä¹‰èµ„æºå®šä¹‰ã€‚

å‡è®¾æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç¼–å†™ä¸€ä¸ª `operator` ï¼Œå®ƒ å°†åœ¨é›†ç¾¤çš„èŠ‚ç‚¹ä¸Šéƒ¨ç½²ä¸€ä¸ªå®ˆæŠ¤è¿›ç¨‹ã€‚å®ƒå°†ä½¿ç”¨ `DaemonSet` å¯¹è±¡æ¥éƒ¨ç½²æ­¤å®ˆæŠ¤è¿›ç¨‹ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤ŸæŒ‡å®šä¸€ä¸ªæ ‡ç­¾ï¼Œä»¥ä¾¿ä»…åœ¨æ ‡è®°æœ‰æ­¤æ ‡ç­¾çš„èŠ‚ç‚¹ä¸Šéƒ¨ç½²å®ˆæŠ¤è¿›ç¨‹ã€‚æˆ‘ä»¬è¿˜å¸Œæœ›èƒ½å¤ŸæŒ‡å®šè¦éƒ¨ç½²çš„ Docker é•œåƒï¼Œè€Œä¸æ˜¯åƒ`sample-controller`é‚£æ ·ä½¿ç”¨é™æ€é•œåƒã€‚

è®©æˆ‘ä»¬é¦–å…ˆä¸º `GenericDaemon` ç±»å‹åˆ›å»ºè‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼š

```yaml
â¯ cat artifacts/generic-daemon/crd.yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: genericdaemons.mydomain.com
spec:
  group: mydomain.com
  version: v1beta1
  names:
    kind: Genericdaemon
    plural: genericdaemons
  scope: Namespaced
  validation:
    openAPIV3Schema:
      properties:
        spec:
          properties:
            label:
              type: string
            image:
              type: string
          required:
            - image
```

ä¸‹é¢æ˜¯è¦éƒ¨ç½²çš„å®ˆæŠ¤è¿›ç¨‹çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼š

```yaml
// artifacts/generic-daemon/syslog.yaml
apiVersion: mydomain.com/v1beta1
kind: Genericdaemon
metadata:
  name: syslog
spec:
  label: logs
  image: mbessler/syslogdocker
```

ç°åœ¨æˆ‘ä»¬å¿…é¡»ä¸ºAPIæ„å»ºgoæ–‡ä»¶ï¼Œä»¥ä¾¿ä»æ“ä½œç¬¦è®¿é—®è¿™ä¸ªæ–°çš„è‡ªå®šä¹‰èµ„æºå®šä¹‰ã€‚ä¸ºæ­¤ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°ç›®å½• `pkg/apis/genericdaemon` ï¼Œæˆ‘ä»¬å°†åœ¨å…¶ä¸­å¤åˆ¶åœ¨ `pkg/apis/samplecontroller` ä¸­æ‰¾åˆ°çš„æ–‡ä»¶ï¼ˆä½†ä¸åŒ…æ‹¬ `zz_generated.deepcopy.go` ï¼‰

```go
â¯  tree pkg/apis/genericdaemon/
pkg/apis/genericdaemon/
â”œâ”€â”€ register.go
â””â”€â”€ v1alpha1
    â”œâ”€â”€ doc.go
    â”œâ”€â”€ register.go
    â””â”€â”€ types.go
```

å¹¶è°ƒæ•´å…¶å†…å®¹ï¼ˆæ›´æ”¹çš„éƒ¨åˆ†ä»¥ç²—ä½“æ˜¾ç¤ºï¼‰ï¼š

```go
////////////////
// register.go
////////////////
package genericdaemon
const (
 GroupName = "mydomain.com"
)
/////////////////////
// v1beta1/doc.go
/////////////////////
// +k8s:deepcopy-gen=package
// Package v1beta1 is the v1beta1 version of the API.
// +groupName=mydomain.com
package v1beta1
/////////////////////////
// v1beta1/register.go
/////////////////////////
package v1beta1
import (
 metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 "k8s.io/apimachinery/pkg/runtime"
 "k8s.io/apimachinery/pkg/runtime/schema"
genericdaemon "k8s.io/sample-controller/pkg/apis/genericdaemon"
)
// SchemeGroupVersion is group version used to register these objects
var SchemeGroupVersion = schema.GroupVersion{Group: genericdaemon.GroupName, Version: "v1beta1"}
// Kind takes an unqualified kind and returns back a Group qualified GroupKind
func Kind(kind string) schema.GroupKind {
 return SchemeGroupVersion.WithKind(kind).GroupKind()
}
// Resource takes an unqualified resource and returns a Group qualified GroupResource
func Resource(resource string) schema.GroupResource {
 return SchemeGroupVersion.WithResource(resource).GroupResource()
}
var (
 SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)
 AddToScheme   = SchemeBuilder.AddToScheme
)
// Adds the list of known types to Scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
 scheme.AddKnownTypes(SchemeGroupVersion,
  &Genericdaemon{},
  &GenericdaemonList{},
 )
 metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
 return nil
}
//////////////////////
// v1beta1/types.go
//////////////////////
package v1beta1
import (
 metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// Genericdaemon is a specification for a Generic Daemon resource
type Genericdaemon struct {
 metav1.TypeMeta   `json:",inline"`
 metav1.ObjectMeta `json:"metadata,omitempty"`
 Spec   GenericdaemonSpec   `json:"spec"`
 Status GenericdaemonStatus `json:"status"`
}
// GenericDaemonSpec is the spec for a GenericDaemon resource
type GenericdaemonSpec struct {
 Label string `json:"label"`
 Image string `json:"image"`
}
// GenericDaemonStatus is the status for a GenericDaemon resource
type GenericdaemonStatus struct {
 Installed int32 `json:"installed"`
}
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// GenericDaemonList is a list of GenericDaemon resources
type GenericdaemonList struct {
 metav1.TypeMeta `json:",inline"`
 metav1.ListMeta `json:"metadata"`
Items []Genericdaemon `json:"items"`
}
```

æˆ‘ä»¬å†æ¥çœ‹çœ‹å®ƒä¸ºæˆ‘ä»¬æä¾›äº†å“ªäº›å¯ç”¨çš„è„šæœ¬:

å®ƒä½¿ç”¨www.example.comä¸­çš„ç”Ÿæˆå™¨[k8s.io/code-generator](https://github.com/kubernetes/code-generator) ç”Ÿæˆä¸€ä¸ªç±»å‹åŒ–çš„å®¢æˆ·ç«¯ã€informersã€listerså’Œdeep-copyå‡½æ•°ã€‚ä½ å¯ä»¥ä½¿ç”¨`./hack/update-codegen.sh`è„šæœ¬è‡ªå·±æ‰§è¡Œæ­¤æ“ä½œã€‚



### ä»£ç ç”Ÿæˆå™¨

`k8s.io/client-go` æä¾›äº†å¯¹ k8s åŸç”Ÿèµ„æºçš„informerå’Œclientsetç­‰ç­‰ï¼Œä½†å¯¹äºè‡ªå®šä¹‰èµ„æºçš„æ“ä½œåˆ™ç›¸å¯¹ä½æ•ˆï¼Œéœ€è¦ä½¿ç”¨ rest api å’Œ `dynamic client` æ¥æ“ä½œï¼Œå¹¶è‡ªå·±å®ç°ååºåˆ—åŒ–ç­‰åŠŸèƒ½ã€‚

code-generator æä¾›äº†ä»¥ä¸‹å·¥å…·ç”¨äºä¸ºk8sä¸­çš„èµ„æºç”Ÿæˆç›¸å…³ä»£ç ï¼Œå¯ä»¥æ›´åŠ æ–¹ä¾¿çš„æ“ä½œè‡ªå®šä¹‰èµ„æºï¼š

`deepcopy-gen`: ç”Ÿæˆæ·±åº¦æ‹·è´å¯¹è±¡æ–¹æ³• ~

**ä½¿ç”¨æ–¹æ³•ï¼š**

+ åœ¨æ–‡ä»¶ä¸­æ·»åŠ æ³¨é‡Š`// +k8s:deepcopy-gen=package`
+ ä¸ºå•ä¸ªç±»å‹æ·»åŠ è‡ªåŠ¨ç”Ÿæˆ`// +k8s:deepcopy-gen=true`
+ ä¸ºå•ä¸ªç±»å‹å…³é—­è‡ªåŠ¨ç”Ÿæˆ`// +k8s:deepcopy-gen=false`

`client-gen`: ä¸ºèµ„æºç”Ÿæˆæ ‡å‡†çš„æ“ä½œæ–¹æ³•(get; list; watch; create; update; patch; delete)

> åœ¨ `pkg/apis/${GROUP}/${VERSION}/types.go`ä¸­ä½¿ç”¨ï¼Œä½¿ç”¨`// +genclient`æ ‡è®°å¯¹åº”ç±»å‹ç”Ÿæˆçš„å®¢æˆ·ç«¯ï¼Œ å¦‚æœä¸è¯¥ç±»å‹ç›¸å…³è”çš„èµ„æºä¸æ˜¯å‘½åç©ºé—´èŒƒå›´çš„(ä¾‹å¦‚PersistentVolume), åˆ™è¿˜éœ€è¦é™„åŠ `// + genclientï¼šnonNamespaced`æ ‡è®°ï¼Œ

+ `// +genclient` - ç”Ÿæˆé»˜è®¤çš„å®¢æˆ·ç«¯åŠ¨ä½œå‡½æ•°ï¼ˆcreate, update, delete, get, list, update, patch, watchä»¥åŠ æ˜¯å¦ç”ŸæˆupdateStatuså–å†³äº.Statuså­—æ®µæ˜¯å¦å­˜åœ¨ï¼‰ã€‚

+ `// +genclient:nonNamespaced` - æ‰€æœ‰åŠ¨ä½œå‡½æ•°éƒ½æ˜¯åœ¨æ²¡æœ‰åç§°ç©ºé—´çš„æƒ…å†µä¸‹ç”Ÿæˆ

+ `// +genclient:onlyVerbs=create,get` - æŒ‡å®šçš„åŠ¨ä½œå‡½æ•°è¢«ç”Ÿæˆ.

+ `// +genclient:skipVerbs=watch` - ç”Ÿæˆwatchä»¥å¤–æ‰€æœ‰çš„åŠ¨ä½œå‡½æ•°.

+ `// +genclient:noStatus` - å³ä½¿.`Status`å­—æ®µå­˜åœ¨ä¹Ÿä¸ç”Ÿæˆ`updateStatus`åŠ¨ä½œå‡½æ•°

`informer-gen`: ç”Ÿæˆ`informer`ï¼Œæä¾›äº‹ä»¶æœºåˆ¶(AddFunc,UpdateFunc,DeleteFunc)æ¥å“åº”kubernetesçš„event

`lister-gen`: ä¸º `get` å’Œ `list` æ–¹æ³•æä¾›åªè¯»ç¼“å­˜å±‚

`conversion-gen`æ˜¯ç”¨äºè‡ªåŠ¨ç”Ÿæˆåœ¨å†…éƒ¨å’Œå¤–éƒ¨ç±»å‹ä¹‹é—´è½¬æ¢çš„å‡½æ•°çš„å·¥å…·

ä¸€èˆ¬çš„è½¬æ¢ä»£ç ç”Ÿæˆä»»åŠ¡æ¶‰åŠä¸‰å¥—ç¨‹åºåŒ…ï¼š

+ ä¸€å¥—åŒ…å«å†…éƒ¨ç±»å‹çš„ç¨‹åºåŒ…ã€‚
+ ä¸€å¥—åŒ…å«å¤–éƒ¨ç±»å‹çš„ç¨‹åºåŒ…ã€‚
+ å•ä¸ªç›®æ ‡ç¨‹åºåŒ…ï¼ˆå³ï¼Œç”Ÿæˆçš„è½¬æ¢å‡½æ•°æ‰€åœ¨çš„ä½ç½®ï¼Œä»¥åŠå¼€å‘äººå‘˜æˆæƒçš„è½¬æ¢åŠŸèƒ½æ‰€åœ¨çš„ä½ç½®ï¼‰ã€‚åŒ…å«å†…éƒ¨ç±»å‹çš„åŒ…åœ¨ `Kubernetes` çš„å¸¸è§„ä»£ç ç”Ÿæˆæ¡†æ¶ä¸­æ‰®æ¼”ç€ç§°ä¸º`peer package`çš„è§’è‰²ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š

+ æ ‡è®°è½¬æ¢å†…éƒ¨è½¯ä»¶åŒ… `// +k8s:conversion-gen=<import-path-of-internal-package>`
+ æ ‡è®°è½¬æ¢å¤–éƒ¨è½¯ä»¶åŒ…`// +k8s:conversion-gen-external-types=<import-path-of-external-package>`
+ æ ‡è®°ä¸è½¬æ¢å¯¹åº”æ³¨é‡Šæˆ–ç»“æ„ `// +k8s:conversion-gen=false`

`defaulter-gen` ç”¨äºç”Ÿäº§Defaulterå‡½æ•°

+ ä¸ºåŒ…å«å­—æ®µçš„æ‰€æœ‰ç±»å‹åˆ›å»ºdefaultersï¼Œ`// +k8s:defaulter-gen=<field-name-to-flag>`
+ æ‰€æœ‰éƒ½ç”Ÿæˆ`// +k8s:defaulter-gen=true|false`

`go-to-protobuf` é€šè¿‡go structç”Ÿæˆ pb idl

`import-boss` åœ¨ç»™å®šå­˜å‚¨åº“ä¸­å¼ºåˆ¶æ‰§è¡Œå¯¼å…¥é™åˆ¶

`openapi-gen` ç”ŸæˆopenAPIå®šä¹‰

ä½¿ç”¨æ–¹æ³•ï¼š

+ `+k8s:openapi-gen=true` ä¸ºæŒ‡å®šåŒ…æˆ–æ–¹æ³•å¼€å¯
+ `+k8s:openapi-gen=false` æŒ‡å®šåŒ…å…³é—­

`register-gen` ç”Ÿæˆ`register`

`set-gen`

code-generatoræ•´åˆäº†è¿™äº›genï¼Œä½¿ç”¨è„šæœ¬[generate-groups.sh](https://github.com/kubernetes/code-generator/blob/master/generate-groups.sh)å’Œ[generate-internal-groups.sh](https://github.com/kubernetes/code-generator/blob/master/generate-internal-groups.sh)å¯ä»¥ä¸ºè‡ªå®šä¹‰èµ„æºç”Ÿäº§ç›¸å…³ä»£ç ã€‚



### è„šæœ¬è‡ªåŠ¨ç”Ÿæˆ

è„šæœ¬ `hack/update-codegen.sh` å¯ç”¨äºå›´ç»•æˆ‘ä»¬ä½¿ç”¨è¿™äº›å…ˆå‰æ–‡ä»¶å®šä¹‰çš„æ–°è‡ªå®šä¹‰èµ„æºå®šä¹‰ç”Ÿæˆä»£ç ã€‚æˆ‘ä»¬å°†ä¸å¾—ä¸è°ƒæ•´è¿™ä¸ªè„šæœ¬æ¥ä¸ºæˆ‘ä»¬çš„æ–° `CRD` ç”Ÿæˆæ–‡ä»¶ï¼š

`update-codegen`è„šæœ¬å°†è‡ªåŠ¨ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶& ç›®å½•ï¼š

+ `pkg/apis/samplecontroller/v1alpha1/zz_generated.deepcopy.go`
+ `pkg/generated/`

```yaml
// hack/update-codegen.sh
#!/usr/bin/env bash
set -o errexit
set -o nounset
set -o pipefail

SCRIPT_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
CODEGEN_PKG=${CODEGEN_PKG:-$(cd "${SCRIPT_ROOT}"; ls -d -1 ./vendor/k8s.io/code-generator 2>/dev/null || echo ../code-generator)}

# generate the code with:
# --output-base    because this script should also be able to run inside the vendor dir of
#                  k8s.io/kubernetes. The output-base is needed for the generators to output into the vendor dir
#                  instead of the $GOPATH directly. For normal projects this can be dropped.
echo "===> Generating code..."
"${CODEGEN_PKG}/generate-groups.sh" "deepcopy,client,informer,lister" \
  k8s.io/sample-controller/pkg/generated \
  k8s.io/sample-controller/pkg/apis \
  samplecontroller:v1alpha1 \
  --output-base "$(dirname "${BASH_SOURCE[0]}")/../../.." \
  --go-header-file "${SCRIPT_ROOT}"/hack/boilerplate.go.txt
# To use your own boilerplate text append:
#   --go-header-file "${SCRIPT_ROOT}"/hack/custom-boilerplate.go.txt

echo "===> Generating genericdaemon code"
"${CODEGEN_PKG}/generate-groups.sh" "deepcopy,client,informer,lister" \
  k8s.io/sample-controller/pkg/generated \
  k8s.io/sample-controller/pkg/apis \
  genericdaemon:v1beta1 \
  --output-base "$(dirname "${BASH_SOURCE[0]}")/../../.." \
  --go-header-file "${SCRIPT_ROOT}"/hack/boilerplate.go.txt
```



ç„¶åæ‰§è¡Œå®ƒï¼š

```bash
â¯ ./hack/update-codegen.sh 
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥è°ƒæ•´æˆ‘ä»¬çš„`operator` ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»å°†æ‰€æœ‰å¯¹å‰ä¸€ä¸ª `Foo` ç±»å‹çš„å¼•ç”¨æ›´æ”¹ä¸º `Genericdaemon` ç±»å‹ã€‚ç¬¬äºŒï¼Œå½“åˆ›å»ºæ–°çš„é€šç”¨å®ˆæŠ¤è¿›ç¨‹æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åˆ›å»ºDaemonsetè€Œä¸æ˜¯éƒ¨ç½²ã€‚



### å°† operator éƒ¨ç½²åˆ°Kubernetesé›†ç¾¤

å½“æˆ‘ä»¬æ ¹æ®éœ€è¦ä¿®æ”¹å®Œsample-controlleråï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶éƒ¨ç½²åˆ°kubernetesé›†ç¾¤ã€‚äº‹å®ä¸Šï¼Œåœ¨è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬å·²ç»é€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„å‡­è¯ä»æˆ‘ä»¬çš„å¼€å‘ç³»ç»Ÿè¿è¡Œå®ƒæ¥æµ‹è¯•å®ƒã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„Dockerfileï¼Œç”¨äºä½¿ç”¨`operator`æ„å»ºDockeré•œåƒï¼ˆä½ å¿…é¡»ä»åŸå§‹çš„`sample-controller`ä¸­åˆ é™¤æ‰€æœ‰ä»£ç æ‰èƒ½æ„å»ºé•œåƒï¼‰ï¼š

```dockerfile
FROM golang
RUN mkdir -p /go/src/k8s.io/sample-controller
ADD . /go/src/k8s.io/sample-controller
WORKDIR /go
RUN go get ./...
RUN go install -v ./...
CMD ["/go/bin/sample-controller"]
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºé•œåƒå¹¶å°†å…¶æ¨é€åˆ°Docker Hubï¼š

```bash
â¯ docker build . -t cubxxw/genericdaemon
â¯ docker push cubxxw/genericdaemon
```

> âš ï¸ æœ€å¼€å§‹ä½¿ç”¨ `buildpacks` æ¥æ„å»ºçš„ï¼Œå¤ªç¦»è°±äº†ï¼Œæ”¾å¼ƒäº†~

æœ€åï¼Œä½¿ç”¨æ­¤æ–°æ˜ åƒå¯åŠ¨éƒ¨ç½²ï¼š

```yaml
// deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sample
  template:
    metadata:
      labels:
        app: sample
    spec:
      containers:
      - name: sample
        image: "cubxxw/genericdaemon:latest"
```

and `kubectl apply -f deploy.yaml`

```bash
â¯ k get deployment
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
my-nginx-app        0/3     3            0           29h
nginx-deployment    3/3     3            3           5h42m
sample-controller   0/1     1            0           34s

â¯ k get pod | grep -i sample-controller
sample-controller-779777db4b-xh74l   0/1     CrashLoopBackOff   5          6m20s
```

`operator` ç°åœ¨æ­£åœ¨è¿è¡Œï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æ£€æŸ¥ pod çš„æ—¥å¿—ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆæƒå­˜åœ¨é—®é¢˜;PODä¸è·å¾—å¯¹ä¸åŒèµ„æºçš„è®¿é—®æƒé™ï¼š

```bash
â¯ kubectl logs sample-controller-779777db4b-xh74l
E0721 14:34:50.499584       1 reflector.go:134] k8s.io/sample-controller/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1beta1.Genericdaemon: genericdaemons.mydomain.com is forbidden: User "system:serviceaccount:default:default" cannot list genericdaemons.mydomain.com at the cluster scope
E0721 14:34:50.500385       1 reflector.go:134] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.DaemonSet: daemonsets.apps is forbidden: User "system:serviceaccount:default:default" cannot list daemonsets.apps at the cluster scope
[...]
```

æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª `ClusterRole` å’Œä¸€ä¸ª `ClusterRoleBinding` æ¥ç»™äºˆ`operator` å¿…è¦çš„æƒé™ï¼š

```yaml
// rbac_role.yaml
kind: ClusterRole
metadata:
  name: operator-role
rules:
- apiGroups:
  - apps
  resources:
  - daemonsets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - mydomain.com
  resources:
  - genericdaemons
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete

// rbac_role_binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: operator-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: operator-role
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
```

éƒ¨ç½²ï¼š

```bash'
â¯ kubectl apply -f rbac_role.yaml
â¯ kubectl delete -f deploy.yaml
â¯ kubectl apply -f deploy.yaml
```



### sample-controller CRD èµ„æºå®šä¹‰æºç 

é¦–å…ˆï¼Œæœ€å¼€å§‹çœ‹çš„è¿˜æ˜¯ CRD çš„èµ„æºå®šä¹‰éƒ¨åˆ†ï¼ˆsample-controller/artifacts/examples/crd.yamlï¼‰ï¼š

+ [å®˜æ–¹æè¿°~](https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/2337-k8s.io-group-protection/README.md)

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # åå­—å¿…éœ€ä¸ä¸‹é¢çš„ spec å­—æ®µåŒ¹é…ï¼Œå¹¶ä¸”æ ¼å¼ä¸º '<åç§°çš„å¤æ•°å½¢å¼>.<ç»„å>'
  name: foos.samplecontroller.k8s.io
spec:
  # ç»„åç§°ï¼Œç”¨äº REST API: /apis/<ç»„>/<ç‰ˆæœ¬>
  group: samplecontroller.k8s.io
  # åˆ—ä¸¾æ­¤ CustomResourceDefinition æ‰€æ”¯æŒçš„ç‰ˆæœ¬
  version: v1alpha1
  names:
    # kind é€šå¸¸æ˜¯å•æ•°å½¢å¼çš„é©¼å³°ç¼–ç ï¼ˆCamelCasedï¼‰å½¢å¼ã€‚ä½ çš„èµ„æºæ¸…å•ä¼šä½¿ç”¨è¿™ä¸€å½¢å¼ã€‚
    kind: Foo
    # åç§°çš„å¤æ•°å½¢å¼ï¼Œç”¨äº URLï¼š/apis/<ç»„>/<ç‰ˆæœ¬>/<åç§°çš„å¤æ•°å½¢å¼>
    plural: foos
    # åç§°çš„å•æ•°å½¢å¼ï¼Œä½œä¸ºå‘½ä»¤è¡Œä½¿ç”¨æ—¶å’Œæ˜¾ç¤ºæ—¶çš„åˆ«å
    singular: crontab
    # shortNames å…è®¸ä½ åœ¨å‘½ä»¤è¡Œä½¿ç”¨è¾ƒçŸ­çš„å­—ç¬¦ä¸²æ¥åŒ¹é…èµ„æº
    shortNames:
    - f
  # å¯ä»¥æ˜¯ Namespaced æˆ– Cluster
  scope: Namespaced
```

+    è¯¥å®šä¹‰æ–‡ä»¶ï¼Œå£°æ˜äº†ä¸€ç§åä¸º Foo çš„èµ„æºï¼Œå‘Šè¯‰ API Serverï¼Œæœ‰ä¸€ç§èµ„æºå«åš Foo
+    è¯¥ Foo èµ„æºå°†è¢« sample-controller æ‰€ç›‘å¬ï¼Œå¹¶å¯¹å…¶ç›¸å…³äº‹ä»¶è¿›è¡Œå¤„ç†
+    CRD å¯ä»¥æ˜¯åå­—ç©ºé—´ä½œç”¨åŸŸçš„ï¼Œä¹Ÿå¯ä»¥ æ˜¯é›†ç¾¤ä½œç”¨åŸŸçš„ï¼Œå–å†³äº CRD çš„ `scope` å­—æ®µè®¾ç½®ã€‚
+    è‡ªåŠ¨ä»£ç ç”Ÿæˆå·¥å…·ï¼ˆä¸Šé¢è®²çš„update-codegenï¼‰ å°†controllerä¹‹å¤–çš„äº‹æƒ…éƒ½åšå¥½äº†ï¼Œæˆ‘ä»¬åªè¦ä¸“æ³¨äºcontrollerçš„å¼€å‘å°±å¥½ã€‚

**è‡ªå·±ç¼–å†™controlleræœ‰ä¸‰æ­¥ï¼š**

+ å®šä¹‰CRD
+ ç”Ÿæˆè‡ªå®šä¹‰èµ„æºçš„Clientsetã€Informersã€Listersç­‰
  + Clientset ç”¨äºå’Œ Kubernetes è¿›è¡Œäº¤äº’
  + Informers æœºåˆ¶ç”¨äº ç›‘å¬ Kubernetes ä¸­ API Server çš„èµ„æºå˜åŒ–ï¼Œä¸€èˆ¬å’Œ WOrksSqueue ç»“åˆä½¿ç”¨ï¼Œä¸€ä¸ªæ˜¯ç›‘å¬ï¼Œä¸€ä¸ªæ˜¯ç¼“å­˜åˆ°æ’é˜Ÿé˜Ÿåˆ—ä¸­ï¼ˆä¸‰ç§æ¥å£å®ç°ï¼‰
  + Listers æ˜¯ Kubernetes ä¸­çš„å¦ä¸€ç§æœºåˆ¶ï¼Œç”¨äºä» Informers ç¼“å­˜ä¸­è·å–èµ„æºå¯¹è±¡çš„åˆ—è¡¨ã€‚

+ ç¼–å†™Controllerç­‰ä»£ç 



### Foo èµ„æºçš„ yaml å®šä¹‰

æºæ–‡ä»¶ï¼š

```yaml
â¯ cat  artifacts/examples/example-foo.yaml
apiVersion: samplecontroller.k8s.io/v1alpha1
kind: Foo
metadata:
  name: example-foo
spec:
  deploymentName: example-foo
  replicas: 1
```

ğŸ“œ å¯¹ä¸Šé¢çš„è§£é‡Šï¼š

+ è¯¥æ–‡ä»¶å£°æ˜è¦åˆ›å»ºçš„èµ„æºä¸º Foo ç±»å‹ï¼Œå‰¯æœ¬æ•°ä¸º 1
+ è¿™ä¸ªåˆ›å»ºäº‹ä»¶ä¼šè¢« `sample-controller` æ‹¦æˆªå’Œå¤„ç†



###  åˆ†æ Controller çš„å®ç°é€»è¾‘

æ—¢ç„¶æ˜¯ä» `main.go` å¼€å§‹çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸»è¦çš„é€»è¾‘åœ¨è¿™é‡Œï¼š

```go
func main() {
	klog.InitFlags(nil)
	flag.Parse()

	// set up signals so we handle the shutdown signal gracefully
	ctx := signals.SetupSignalHandler()
	logger := klog.FromContext(ctx)

	cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)
	if err != nil {
		logger.Error(err, "Error building kubeconfig")
		klog.FlushAndExit(klog.ExitFlushTimeout, 1)
	}

	kubeClient, err := kubernetes.NewForConfig(cfg)
	if err != nil {
		logger.Error(err, "Error building kubernetes clientset")
		klog.FlushAndExit(klog.ExitFlushTimeout, 1)
	}

	exampleClient, err := clientset.NewForConfig(cfg)
	if err != nil {
		logger.Error(err, "Error building kubernetes clientset")
		klog.FlushAndExit(klog.ExitFlushTimeout, 1)
	}

	kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)
	exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30)

	controller := NewController(ctx, kubeClient, exampleClient,
		kubeInformerFactory.Apps().V1().Deployments(),
		exampleInformerFactory.Samplecontroller().V1alpha1().Foos())

	// notice that there is no need to run Start methods in a separate goroutine. (i.e. go kubeInformerFactory.Start(ctx.done())
	// Start method is non-blocking and runs all registered informers in a dedicated goroutine.
	kubeInformerFactory.Start(ctx.Done())
	exampleInformerFactory.Start(ctx.Done())

	if err = controller.Run(ctx, 2); err != nil {
		logger.Error(err, "Error running controller")
		klog.FlushAndExit(klog.ExitFlushTimeout, 1)
	}
}
```

**ğŸ“œ å¤§è‡´æµç¨‹å¯¹ä¸Šé¢çš„è§£é‡Šï¼š**

+ è¯»å– kubeconfig é…ç½®ï¼Œæ„é€ ç”¨äºäº‹ä»¶ç›‘å¬çš„ `Kubernetes Client`ã€‚

  >  è¿™é‡Œåˆ›å»ºäº†ä¸¤ä¸ªï¼Œä¸€ä¸ªç›‘å¬æ™®é€šäº‹ä»¶ï¼Œä¸€ä¸ªç›‘å¬ Foo äº‹ä»¶ã€‚

+ åŸºäº `Client` æ„é€ ç›‘å¬ç›¸å…³çš„ `informer`ã€‚

+  åŸºäº `Client`ã€`Informer` åˆå§‹åŒ–è‡ªå®šä¹‰ `Controller`ï¼Œç›‘å¬ `Deployment` ä»¥åŠ `Foos` èµ„æºå˜åŒ–ã€‚

+ å¼€å¯ `Controller`ã€‚



é‚£ä¹ˆæ¥ä¸‹æ¥å°±æ˜¯æœ€ä¸»è¦çš„ï¼Œå°±æ˜¯ `controller` çš„å®ç°é€»è¾‘äº†(controller.go)

`Controller` çš„ç»“æ„ä½“å®šä¹‰å¦‚ä¸‹ï¼š

```go
// Controller is the controller implementation for Foo resources
type Controller struct {
	// kubeclientset is a standard kubernetes clientset
	kubeclientset kubernetes.Interface
	// sampleclientset is a clientset for our own API group
	sampleclientset clientset.Interface

	deploymentsLister appslisters.DeploymentLister
	deploymentsSynced cache.InformerSynced
	foosLister        listers.FooLister
	foosSynced        cache.InformerSynced

	// workqueue is a rate limited work queue. This is used to queue work to be
	// processed instead of performing it as soon as a change happens. This
	// means we can ensure we only process a fixed amount of resources at a
	// time, and makes it easy to ensure we are never processing the same item
	// simultaneously in two different workers.
	workqueue workqueue.RateLimitingInterface
	// recorder is an event recorder for recording Event resources to the
	// Kubernetes API.
	recorder record.EventRecorder
}
```

ğŸ“œ å¯¹ä¸Šé¢çš„è§£é‡Šï¼š

+  Controller çš„å…³é”®æˆå‘˜å³ä¸¤ä¸ªäº‹ä»¶çš„ Listenerï¼ˆ`appslisters.DeploymentLister`ã€`listers.FooLister`ï¼‰è¿™ä¸¤ä¸ªæˆå‘˜å°†ç”± main å‡½æ•°ä¼ å…¥å‚æ•°è¿›è¡Œåˆå§‹åŒ–
+ æ­¤å¤–ï¼Œä¸ºäº†ç¼“å†²äº‹ä»¶å¤„ç†ï¼Œè¿™é‡Œä½¿ç”¨é˜Ÿåˆ—æš‚å­˜äº‹ä»¶ï¼Œç›¸å…³æˆå‘˜å³ä¸º `workqueue.RateLimitingInterface`
+ `kubeclientset`æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ Kubernetes å®¢æˆ·ç«¯é›†ï¼Œç”¨äºä¸ Kubernetes API è¿›è¡Œäº¤äº’ã€‚
+ `sampleclientset`æ˜¯æˆ‘ä»¬è‡ªå·±çš„ API ç»„çš„å®¢æˆ·ç«¯é›†ï¼Œç”¨äºä¸æˆ‘ä»¬çš„è‡ªå®šä¹‰APIèµ„æºè¿›è¡Œäº¤äº’ã€‚
+ `deploymentsLister`æ˜¯ä¸€ä¸ª Deployment çš„ Lister æ¥å£ï¼Œç”¨äºè·å–Deploymentèµ„æºçš„åˆ—è¡¨ä¿¡æ¯ã€‚
+ `deploymentsSynced`æ˜¯ä¸€ä¸ª Deployment çš„ InformerSynced æ¥å£ï¼Œç”¨äºç¡®å®šDeploymentèµ„æºæ˜¯å¦å·²ç»åŒæ­¥å®Œæ¯•ã€‚
+ `foosLister`æ˜¯ä¸€ä¸ª Foo çš„`Lister`æ¥å£ï¼Œç”¨äºè·å–Fooèµ„æºçš„åˆ—è¡¨ä¿¡æ¯ã€‚
+ `foosSynced`æ˜¯ä¸€ä¸ª Foo çš„`InformerSynced`æ¥å£ï¼Œç”¨äºç¡®å®šFooèµ„æºæ˜¯å¦å·²ç»åŒæ­¥å®Œæ¯•ã€‚
+ `workqueue`æ˜¯ä¸€ä¸ªé€Ÿç‡é™åˆ¶çš„å·¥ä½œé˜Ÿåˆ—ï¼Œç”¨äºå¤„ç†å·¥ä½œé¡¹ã€‚é€šè¿‡ä½¿ç”¨å·¥ä½œé˜Ÿåˆ—ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿ä¸€æ¬¡åªå¤„ç†ä¸€å®šæ•°é‡çš„èµ„æºï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾ç¡®ä¿æˆ‘ä»¬ä¸ä¼šåŒæ—¶åœ¨ä¸¤ä¸ªä¸åŒçš„å·¥ä½œè€…ä¸­å¤„ç†åŒä¸€é¡¹èµ„æºã€‚
+ `recorder`æ˜¯äº‹ä»¶è®°å½•å™¨ï¼Œç”¨äºè®°å½• Event èµ„æºåˆ° Kubernetes API ä¸­ã€‚

> æ§åˆ¶å™¨çš„å®ç°è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº† informer å’Œ workqueue ä¸¤ä¸ªé‡è¦çš„ç»„ä»¶ï¼Œå®ƒä»¬åˆ†åˆ«ç”¨äºç›‘å¬èµ„æºçš„å˜åŒ–å’Œå¤„ç†ä»»åŠ¡ã€‚å½“æŸä¸ªèµ„æºå‘ç”Ÿå˜åŒ–æ—¶ï¼Œinformer ä¼šå°†è¯¥èµ„æºåŠ å…¥ workqueue ä¸­ç­‰å¾…å¤„ç†ã€‚å¤„ç†ä»»åŠ¡æ—¶ï¼ŒsyncHandler å‡½æ•°ä¼šè·å–ä»»åŠ¡å¯¹åº”çš„ Foo å’Œ Deployment èµ„æºï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬çš„çŠ¶æ€æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™è¿›è¡ŒåŒæ­¥ï¼ŒåŒæ­¥æˆåŠŸåæ›´æ–°çŠ¶æ€ã€‚æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº† Kubernetes ä¸­çš„äº‹ä»¶è®°å½•å™¨å°†äº‹ä»¶è®°å½•åˆ° Kubernetes API ä¸­ï¼Œä»¥ä¾¿äºè°ƒè¯•å’Œç›‘æ§ã€‚



**æ¥ç€**ï¼Œåˆ†æ Controller çš„æ„é€ è¿‡ç¨‹ï¼Œä»£ç å¦‚ä¸‹ï¼š

```go
func NewController(
	kubeclientset kubernetes.Interface,
	sampleclientset clientset.Interface,
	deploymentInformer appsinformers.DeploymentInformer,
	fooInformer informers.FooInformer) *Controller {
 
	// Create event broadcaster
	// Add sample-controller types to the default Kubernetes Scheme so Events can be
	// logged for sample-controller types.
	utilruntime.Must(samplescheme.AddToScheme(scheme.Scheme))
	klog.V(4).Info("Creating event broadcaster")
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartLogging(klog.Infof)
	eventBroadcaster.StartRecordingToSink(&typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events("")})
	recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: controllerAgentName})
 
	controller := &Controller{
		kubeclientset:     kubeclientset,
		sampleclientset:   sampleclientset,
		deploymentsLister: deploymentInformer.Lister(),
		deploymentsSynced: deploymentInformer.Informer().HasSynced,
		foosLister:        fooInformer.Lister(),
		foosSynced:        fooInformer.Informer().HasSynced,
		workqueue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "Foos"),
		recorder:          recorder,
	}
 
	klog.Info("Setting up event handlers")
	// Set up an event handler for when Foo resources change
	fooInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.enqueueFoo,
		UpdateFunc: func(old, new interface{}) {
			controller.enqueueFoo(new)
		},
	})
	// Set up an event handler for when Deployment resources change. This
	// handler will lookup the owner of the given Deployment, and if it is
	// owned by a Foo resource will enqueue that Foo resource for
	// processing. This way, we don't need to implement custom logic for
	// handling Deployment resources. More info on this pattern:
	// https://github.com/kubernetes/community/blob/8cafef897a22026d42f5e5bb3f104febe7e29830/contributors/devel/controllers.md
	deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.handleObject,
		UpdateFunc: func(old, new interface{}) {
			newDepl := new.(*appsv1.Deployment)
			oldDepl := old.(*appsv1.Deployment)
			if newDepl.ResourceVersion == oldDepl.ResourceVersion {
				// Periodic resync will send update events for all known Deployments.
				// Two different versions of the same Deployment will always have different RVs.
				return
			}
			controller.handleObject(new)
		},
		DeleteFunc: controller.handleObject,
	})
 
	return controller
}
```

è¿™æ˜¯ä¸€ä¸ªåˆå§‹åŒ–çš„è¿‡ç¨‹ï¼Œä¹Ÿæ˜¯åœ¨ `main.go` ä¸­è°ƒç”¨çš„ã€‚

-  å°† `sample-controller` çš„ç±»å‹ä¿¡æ¯ï¼ˆFooï¼‰æ·»åŠ åˆ°é»˜è®¤ Kubernetes Schemeï¼Œä»¥ä¾¿èƒ½å¤Ÿè®°å½•åˆ°å…¶äº‹ä»¶ã€‚

-  åŸºäºæ–° `Scheme` åˆ›å»ºä¸€ä¸ªäº‹ä»¶è®°å½• `recorder` ï¼Œç”¨äºè®°å½•æ¥è‡ª â€œsample-controllerâ€ çš„äº‹ä»¶ã€‚

-  åŸºäºå‡½æ•°å…¥å‚åŠåˆšåˆšæ„é€ çš„ `recorder`ï¼Œåˆå§‹åŒ– Controllerã€‚

-  è®¾ç½®å¯¹ `Foo` èµ„æºå˜åŒ–çš„äº‹ä»¶å¤„ç†å‡½æ•°ï¼ˆAddã€Update å‡é€šè¿‡ enqueueFoo å¤„ç†ï¼‰

-  è®¾ç½®å¯¹ `Deployment` èµ„æºå˜åŒ–çš„äº‹ä»¶å¤„ç†å‡½æ•°ï¼ˆAddã€Updateã€Delete å‡é€šè¿‡ handleObject å¤„ç†ï¼‰

-  è¿”å›åˆå§‹åŒ–çš„ `Controller`ã€‚



 **è¿›ä¸€æ­¥**ï¼Œåˆ†æ `enqueueFoo` ä»¥åŠ `handleObject` çš„å®ç°

```go
// enqueueFoo takes a Foo resource and converts it into a namespace/name
// string which is then put onto the work queue. This method should *not* be
// passed resources of any type other than Foo.
func (c *Controller) enqueueFoo(obj interface{}) {
	var key string
	var err error
	if key, err = cache.MetaNamespaceKeyFunc(obj); err != nil {
		utilruntime.HandleError(err)
		return
	}
	c.workqueue.AddRateLimited(key)
}
 
// handleObject will take any resource implementing metav1.Object and attempt
// to find the Foo resource that 'owns' it. It does this by looking at the
// objects metadata.ownerReferences field for an appropriate OwnerReference.
// It then enqueues that Foo resource to be processed. If the object does not
// have an appropriate OwnerReference, it will simply be skipped.
func (c *Controller) handleObject(obj interface{}) {
	var object metav1.Object
	var ok bool
	if object, ok = obj.(metav1.Object); !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("error decoding object, invalid type"))
			return
		}
		object, ok = tombstone.Obj.(metav1.Object)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("error decoding object tombstone, invalid type"))
			return
		}
		klog.V(4).Infof("Recovered deleted object '%s' from tombstone", object.GetName())
	}
	klog.V(4).Infof("Processing object: %s", object.GetName())
	if ownerRef := metav1.GetControllerOf(object); ownerRef != nil {
		// If this object is not owned by a Foo, we should not do anything more
		// with it.
		if ownerRef.Kind != "Foo" {
			return
		}
 
		foo, err := c.foosLister.Foos(object.GetNamespace()).Get(ownerRef.Name)
		if err != nil {
			klog.V(4).Infof("ignoring orphaned object '%s' of foo '%s'", object.GetSelfLink(), ownerRef.Name)
			return
		}
 
		c.enqueueFoo(foo)
		return
	}
}
```

ğŸ“œ å¯¹ä¸Šé¢çš„è§£é‡Šï¼š

+ enqueueFoo å°±æ˜¯è§£æ Foo èµ„æºä¸º `namespace/name` å½¢å¼çš„å­—ç¬¦ä¸²ï¼Œç„¶åå…¥é˜Ÿ
+ handleObject ç›‘å¬äº†æ‰€æœ‰å®ç°äº† `metav1` çš„èµ„æºï¼Œä½†åªè¿‡æ»¤å‡º `owner` æ˜¯ `Foo` çš„ï¼Œå°†å…¶è§£æä¸º `namespace/name` å…¥é˜Ÿ

åœ¨æ„é€  Controller æ—¶å°±å·²ç»åˆå§‹åŒ–å¥½äº‹ä»¶æ”¶é›†è¿™éƒ¨åˆ†çš„å·¥ä½œäº†, é‚£å¦‚ä½•å¤„ç†é˜Ÿåˆ—é‡Œçš„è¿™äº›äº‹ä»¶å‘¢ï¼Ÿé‚£å°±æ˜¯ Run å‡½æ•°çš„æ“ä½œè¿‡ç¨‹ï¼š

**Run å‡½æ•°æ“ä½œï¼š**

```go
// Run will set up the event handlers for types we are interested in, as well
// as syncing informer caches and starting workers. It will block until stopCh
// is closed, at which point it will shutdown the workqueue and wait for
// workers to finish processing their current work items.
func (c *Controller) Run(threadiness int, stopCh <-chan struct{}) error {
	defer utilruntime.HandleCrash()
	defer c.workqueue.ShutDown()
 
	// Start the informer factories to begin populating the informer caches
	klog.Info("Starting Foo controller")
 
	// Wait for the caches to be synced before starting workers
	klog.Info("Waiting for informer caches to sync")
	if ok := cache.WaitForCacheSync(stopCh, c.deploymentsSynced, c.foosSynced); !ok {
		return fmt.Errorf("failed to wait for caches to sync")
	}
 
	klog.Info("Starting workers")
	// Launch two workers to process Foo resources
	for i := 0; i < threadiness; i++ {
		go wait.Until(c.runWorker, time.Second, stopCh)
	}
 
	klog.Info("Started workers")
	<-stopCh
	klog.Info("Shutting down workers")
 
	return nil
}// Run will set up the event handlers for types we are interested in, as well
// as syncing informer caches and starting workers. It will block until stopCh
// is closed, at which point it will shutdown the workqueue and wait for
// workers to finish processing their current work items.
func (c *Controller) Run(threadiness int, stopCh <-chan struct{}) error {
	defer utilruntime.HandleCrash()
	defer c.workqueue.ShutDown()
 
	// Start the informer factories to begin populating the informer caches
	klog.Info("Starting Foo controller")
 
	// Wait for the caches to be synced before starting workers
	klog.Info("Waiting for informer caches to sync")
	if ok := cache.WaitForCacheSync(stopCh, c.deploymentsSynced, c.foosSynced); !ok {
		return fmt.Errorf("failed to wait for caches to sync")
	}
 
	klog.Info("Starting workers")
	// Launch two workers to process Foo resources
	for i := 0; i < threadiness; i++ {
		go wait.Until(c.runWorker, time.Second, stopCh)
	}
 
	klog.Info("Started workers")
	<-stopCh
	klog.Info("Shutting down workers")
 
	return nil
}
```

**Run å‡½æ•°çš„æ‰§è¡Œè¿‡ç¨‹å¤§ä½“å¦‚ä¸‹ï¼š**

+ ç­‰å¾… Informer åŒæ­¥å®Œæˆï¼Œ
+ å¹¶å‘ runWorkerï¼Œå¤„ç†é˜Ÿåˆ—å†…äº‹ä»¶



**runWorker çš„å®šä¹‰**

```go
// runWorker is a long-running function that will continually call the
// processNextWorkItem function in order to read and process a message on the
// workqueue.
func (c *Controller) runWorker() {
	for c.processNextWorkItem() {
	}
}

// processNextWorkItem will read a single work item off the workqueue and
// attempt to process it, by calling the syncHandler.
func (c *Controller) processNextWorkItem() bool {
	obj, shutdown := c.workqueue.Get()
 
	if shutdown {
		return false
	}
 
	// We wrap this block in a func so we can defer c.workqueue.Done.
	err := func(obj interface{}) error {
		// We call Done here so the workqueue knows we have finished
		// processing this item. We also must remember to call Forget if we
		// do not want this work item being re-queued. For example, we do
		// not call Forget if a transient error occurs, instead the item is
		// put back on the workqueue and attempted again after a back-off
		// period.
		defer c.workqueue.Done(obj)
		var key string
		var ok bool
		// We expect strings to come off the workqueue. These are of the
		// form namespace/name. We do this as the delayed nature of the
		// workqueue means the items in the informer cache may actually be
		// more up to date that when the item was initially put onto the
		// workqueue.
		if key, ok = obj.(string); !ok {
			// As the item in the workqueue is actually invalid, we call
			// Forget here else we'd go into a loop of attempting to
			// process a work item that is invalid.
			c.workqueue.Forget(obj)
			utilruntime.HandleError(fmt.Errorf("expected string in workqueue but got %#v", obj))
			return nil
		}
		// Run the syncHandler, passing it the namespace/name string of the
		// Foo resource to be synced.
		if err := c.syncHandler(key); err != nil {
			// Put the item back on the workqueue to handle any transient errors.
			c.workqueue.AddRateLimited(key)
			return fmt.Errorf("error syncing '%s': %s, requeuing", key, err.Error())
		}
		// Finally, if no error occurs we Forget this item so it does not
		// get queued again until another change happens.
		c.workqueue.Forget(obj)
		klog.Infof("Successfully synced '%s'", key)
		return nil
	}(obj)
 
	if err != nil {
		utilruntime.HandleError(err)
		return true
	}
 
	return true
}
```

**processNextWorkItem çš„å¤„ç†æµç¨‹å¤§ä½“å¦‚ä¸‹ï¼š**

+ ä»é˜Ÿåˆ—å–å‡ºå¾…å¤„ç†å¯¹è±¡
+ è°ƒç”¨ `syncHandler` å¤„ç†



**å†æ¥åˆ†æ syncHandler çš„å¤„ç†ç»†èŠ‚:**

`syncHandler` å°±æ˜¯ä¸€ä¸ªæœ€æ ¸å¿ƒçš„å®ç°äº†ï¼Œè¿™ä¸ªå®ç°ä¹Ÿæ˜¯æ¯”è¾ƒæœ‰æ„æ€çš„ï¼Œè¯¥å‡½æ•°æ¯”è¾ƒå®é™…çŠ¶æ€å’ŒæœŸæœ›çŠ¶æ€ï¼Œå¹¶å°è¯•å°†ä¸¤è€…èåˆã€‚ç„¶åï¼Œå®ƒæ›´æ–° Foo èµ„æºçš„ Status å—ä»¥åæ˜ èµ„æºçš„å½“å‰çŠ¶æ€ã€‚æ‰€ä»¥æ ¸å¿ƒæ­¥éª¤å°±æ˜¯ä¸¤ä¸ªæ­¥éª¤

```go
// syncHandler compares the actual state with the desired, and attempts to
// converge the two. It then updates the Status block of the Foo resource
// with the current status of the resource.
func (c *Controller) syncHandler(key string) error {
	// Convert the namespace/name string into a distinct namespace and name
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("invalid resource key: %s", key))
		return nil
	}
 
	// Get the Foo resource with this namespace/name
	foo, err := c.foosLister.Foos(namespace).Get(name)
	if err != nil {
		// The Foo resource may no longer exist, in which case we stop
		// processing.
		if errors.IsNotFound(err) {
			utilruntime.HandleError(fmt.Errorf("foo '%s' in work queue no longer exists", key))
			return nil
		}
 
		return err
	}
 
	deploymentName := foo.Spec.DeploymentName
	if deploymentName == "" {
		// We choose to absorb the error here as the worker would requeue the
		// resource otherwise. Instead, the next time the resource is updated
		// the resource will be queued again.
		utilruntime.HandleError(fmt.Errorf("%s: deployment name must be specified", key))
		return nil
	}
 
	// Get the deployment with the name specified in Foo.spec
	deployment, err := c.deploymentsLister.Deployments(foo.Namespace).Get(deploymentName)
	// If the resource doesn't exist, we'll create it
	if errors.IsNotFound(err) {
		deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Create(newDeployment(foo))
	}
 
	// If an error occurs during Get/Create, we'll requeue the item so we can
	// attempt processing again later. This could have been caused by a
	// temporary network failure, or any other transient reason.
	if err != nil {
		return err
	}
 
	// If the Deployment is not controlled by this Foo resource, we should log
	// a warning to the event recorder and ret
	if !metav1.IsControlledBy(deployment, foo) {
		msg := fmt.Sprintf(MessageResourceExists, deployment.Name)
		c.recorder.Event(foo, corev1.EventTypeWarning, ErrResourceExists, msg)
		return fmt.Errorf(msg)
	}
 
	// If this number of the replicas on the Foo resource is specified, and the
	// number does not equal the current desired replicas on the Deployment, we
	// should update the Deployment resource.
	if foo.Spec.Replicas != nil && *foo.Spec.Replicas != *deployment.Spec.Replicas {
		klog.V(4).Infof("Foo %s replicas: %d, deployment replicas: %d", name, *foo.Spec.Replicas, *deployment.Spec.Replicas)
		deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Update(newDeployment(foo))
	}
 
	// If an error occurs during Update, we'll requeue the item so we can
	// attempt processing again later. THis could have been caused by a
	// temporary network failure, or any other transient reason.
	if err != nil {
		return err
	}
 
	// Finally, we update the status block of the Foo resource to reflect the
	// current state of the world
	err = c.updateFooStatus(foo, deployment)
	if err != nil {
		return err
	}
 
	c.recorder.Event(foo, corev1.EventTypeNormal, SuccessSynced, MessageResourceSynced)
	return nil
}
```

syncHandler çš„å¤„ç†é€»è¾‘å¤§ä½“å¦‚ä¸‹ï¼š

-  æ ¹æ® `namespace/name` è·å– foo èµ„æº

-  æ ¹æ® `foo`ï¼Œè·å–å…¶ `Deployment` åç§°ï¼Œè¿›è€Œè·å– deployment èµ„æºï¼ˆæ²¡æœ‰å°±ä¸ºå…¶åˆ›å»ºï¼‰

-  æ ¹æ® `foo` çš„ `Replicas` æ›´æ–° `deployment` çš„ `Replicas`ï¼ˆå¦‚æœä¸åŒ¹é…ï¼‰

-  æ›´æ–° `foo` èµ„æºçš„çŠ¶æ€ä¸ºæœ€æ–° `deployment` çš„çŠ¶æ€ï¼ˆå…¶å®å°±æ˜¯ `AvailableReplicas`ï¼‰

ç”±æ­¤ï¼Œå¯çŸ¥ foo çš„å®ç°å®ä½“å…¶å®å°±æ˜¯ `deployment`

> è¿™é‡Œä¹Ÿæœ‰ä¸€ä¸ªæ¯”è¾ƒæœ‰æ„æ€çš„ç°è±¡ï¼Œæˆ‘ä»¬å­¦ä¹  Kubernetes çš„æ—¶å€™ä¹ŸçŸ¥é“ Deployment æ§åˆ¶ Pod æ˜¯é€šè¿‡ä¸¤å±‚æ§åˆ¶å™¨æ¥å®ç°çš„ï¼ŒDeployment æ§åˆ¶å™¨ä½¿ç”¨ ReplicaSet æ§åˆ¶å™¨æ¥ç¡®ä¿æŒ‡å®šæ•°é‡çš„ Pod å§‹ç»ˆåœ¨è¿è¡Œã€‚å½“ Deployment æ§åˆ¶å™¨æ£€æµ‹åˆ° Pod æ•°é‡ä¸è¶³æˆ– Pod ä¸å­˜åœ¨æ—¶ï¼Œå®ƒä¼šå¯åŠ¨ä¸€ä¸ªæ–°çš„ ReplicaSetï¼Œä»¥ä¾¿ç¡®ä¿æŒ‡å®šæ•°é‡çš„ Pod å§‹ç»ˆåœ¨è¿è¡Œã€‚ç„¶åï¼Œå®ƒé€æ­¥å°†æ–°çš„ ReplicaSet ä¸­çš„ Pod æ›¿æ¢ä¸ºæ—§çš„ ReplicaSet ä¸­çš„ Podï¼Œç›´åˆ°æ—§çš„ ReplicaSet ä¸­çš„æ‰€æœ‰ Pod éƒ½è¢«æ›¿æ¢ä¸ºæ­¢ã€‚è¿™æ ·ï¼ŒDeployment æ§åˆ¶å™¨å°±å¯ä»¥å®ç°æ— å®•æœºæ›´æ–°å’Œå›æ»šæ“ä½œï¼Œä»è€Œç¡®ä¿åº”ç”¨ç¨‹åºçš„é«˜å¯ç”¨æ€§å’Œå¯é æ€§ã€‚



**çœ‹ä¸‹ `deployment` çš„å®ç°ä»£ç ï¼š**

> è¿™é‡Œä¹Ÿå°±æ˜¯ controller çš„æœ€ç»ˆéƒ¨åˆ†äº†ï¼Œåˆ›å»ºäº†ä¸€ä¸ª Deployment å¯¹è±¡ï¼Œå½“ç„¶åœ¨åé¢ä¼šæ›´æ–°å®é™…çŠ¶æ€ã€‚

```go
// newDeployment creates a new Deployment for a Foo resource. It also sets
// the appropriate OwnerReferences on the resource so handleObject can discover
// the Foo resource that 'owns' it.
func newDeployment(foo *samplev1alpha1.Foo) *appsv1.Deployment {
	labels := map[string]string{
		"app":        "nginx",
		"controller": foo.Name,
	}
	return &appsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      foo.Spec.DeploymentName,
			Namespace: foo.Namespace,
			OwnerReferences: []metav1.OwnerReference{
				*metav1.NewControllerRef(foo, schema.GroupVersionKind{
					Group:   samplev1alpha1.SchemeGroupVersion.Group,
					Version: samplev1alpha1.SchemeGroupVersion.Version,
					Kind:    "Foo",
				}),
			},
		},
		Spec: appsv1.DeploymentSpec{
			Replicas: foo.Spec.Replicas,
			Selector: &metav1.LabelSelector{
				MatchLabels: labels,
			},
			Template: corev1.PodTemplateSpec{
				ObjectMeta: metav1.ObjectMeta{
					Labels: labels,
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "nginx",
							Image: "nginx:latest",
						},
					},
				},
			},
		},
	}
}
```

**ç®€å•é€»è¾‘å°±æ˜¯æ ¹æ® foo èµ„æºçš„ namespaceã€nameã€deploymentnameã€replicas ä¿¡æ¯**

åˆ›å»º `nginx deployment` è€Œå·²ã€‚

 éœ€è¦æ³¨æ„çš„æ˜¯ OwnerReferences é‡Œéœ€è¦ä¸ Foo ç±»å‹ç»‘å®šï¼ˆGroupã€Versionã€Kindï¼‰ï¼Œä¸»è¦æ˜¯è¦ä¸é‡‡é›†å¤„åŒ¹é…ï¼Œå› ä¸º handleObject ä¸­çš„ç­›é€‰ Foo èµ„æºä»£ç æ˜¯æ ¹æ® Kind å€¼åšçš„

```go
if ownerRef.Kind != "Foo" {
	return
}
```

**è‡ªå®šä¹‰ Controller æ˜¯å¦‚ä½•ä¸ crd.yaml å®šä¹‰å…³è”çš„ï¼Ÿ**

æˆ‘ä»¬çŸ¥é“ï¼Œä¸€å¼€å§‹æ˜¯é€šè¿‡ `crd.yaml` æ¥é€šå‘Š kubernetes æˆ‘ä»¬è‡ªå®šä¹‰èµ„æºçš„ scheme çš„ï¼Œè¿™é‡Œåœ¨è¡¥å……ä¸€ç‚¹ï¼Œè™½ç„¶ä»£ç ç”Ÿæˆå™¨çš„é€»è¾‘å®ç°ä¸€ç›´æœ‰é—®é¢˜ï¼Œä½†æ˜¯å¤ªåº•å±‚çš„åŠŸèƒ½ï¼Œå¤§éƒ¨åˆ†æ—¶å€™éƒ½ä¸ä¼šç›´æ¥å»ç”¨ã€‚é€‚å½“çš„ç†è§£åº•å±‚å®ç°ï¼ˆinformerï¼‰çš„å·§å¦™ä¹‹åï¼Œçœ‹ä¸€ä¸‹ controller-runtimeçš„ä»£ç ï¼ŒçŸ¥é“æ€ä¹ˆç”¨å®ƒå°è£…çš„cacheã€‚ç„¶åå°±æ˜¯å…³æ³¨åœ¨ åº”ç”¨è¦åšçš„äº‹æƒ…ä¸Šï¼Œåšè¿™éƒ¨åˆ†çš„å®ç°å°±å¤Ÿäº†ã€‚æœ‰ä¸€ä¸ªç‰¹æ®Šæƒ…å†µå°±æ˜¯ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä½ çš„ä»£ç è¦ç”¨å¦å¤–çš„CRDï¼Œè¿™ä¸ªæ—¶å€™ä¼šéº»çƒ¦ä¸€ç‚¹ã€‚æ—©æœŸçš„æ—¶å€™ï¼Œéœ€è¦æå‡ æ­¥ç”Ÿæˆä»£ç çš„æ“ä½œã€‚ä½†æ˜¯ç°åœ¨åªè¦æ˜¯kubebuilderæ¡†æ¶ç”Ÿæˆçš„CRDï¼Œæ¡†æ¶ç”Ÿæˆçš„ä»£ç é‡Œä¼šæœ‰ addtoscheme çš„å®ç°ï¼Œåªéœ€è¦import ä¹‹åï¼Œregistryä¸€ä¸‹ï¼Œå°±å®Œäº‹å„¿äº†ã€‚

+ é‚£æ˜¯å¦‚ä½•ä¸ Controller å…³è”çš„å‘¢ï¼Ÿå…¶å®å°±åœ¨äº `pkg/apis` ç›®å½•ä¸‹ï¼Œæ»¡æ»¡ä¹Ÿéƒ½æ˜¯ Kubernetes çš„å‘³é“ï¼Œåœ¨èµ„æºå®šä¹‰ä¸­ï¼Œä¸ç®¡æ˜¯ k3sã€k0sã€è¿˜æ˜¯ Kubernetesï¼Œæ€»æ˜¯ä¸å¯å¿½è§†çš„æ˜¯ pkg/apis ç›®å½•ã€‚
+ `pkg/apis` ä¸‹å®šä¹‰äº†è‡ªå®šä¹‰èµ„æºçš„ç›¸å…³å±æ€§ä¿¡æ¯ï¼Œæˆ‘ä»¬ç®€å•çœ‹ä¸‹ï¼š
+ `pkg/samplecontroller/v1alpha1/register.go`ï¼ˆå¤„ç†ç±»å‹ Schemaï¼‰

```go
// SchemeGroupVersion is group version used to register these objects
var SchemeGroupVersion = schema.GroupVersion{Group: samplecontroller.GroupName, Version: "v1alpha1"}
 
// Kind takes an unqualified kind and returns back a Group qualified GroupKind
func Kind(kind string) schema.GroupKind {
	return SchemeGroupVersion.WithKind(kind).GroupKind()
}
 
// Resource takes an unqualified resource and returns a Group qualified GroupResource
func Resource(resource string) schema.GroupResource {
	return SchemeGroupVersion.WithResource(resource).GroupResource()
}
 
var (
	SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)
	AddToScheme   = SchemeBuilder.AddToScheme
)
 
// Adds the list of known types to Scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&Foo{},
		&FooList{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}
```

**ä¸ä¹‹å‰çš„ crd å®šä¹‰å¯¹æ¯”ä¸‹:**

```yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: foos.samplecontroller.k8s.io
spec:
  group: samplecontroller.k8s.io
  version: v1alpha1
  names:
    kind: Foo
    plural: foos
  scope: Namespaced
```

ä¼šå‘ç° controller ä¸ crd ä¸¤è€…çš„ groupã€version éƒ½æ˜¯ä¸€è‡´çš„ï¼Œè¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

```bash
var SchemeGroupVersion = schema.GroupVersion{Group: samplecontroller.GroupName, Version: "v1alpha1"}
```

åœ¨ Kubernetes ä¸­ï¼Œnamespace å’Œ name å¯ä»¥å”¯ä¸€æ ‡è¯†ä¸€ä¸ªèµ„æºï¼Œé€šè¿‡ group å’Œ versionï¼Œå¯ä»¥å”¯ä¸€æ ‡è¯† Kubernetes API ä¸­çš„ä¸€ä¸ª API èµ„æºã€‚

è€Œä¸” metadata çš„ name æ˜¯ç¬¦åˆ `<plural>.<group>` è§„èŒƒçš„ï¼Œé‚£ä¹ˆå›åˆ°å¼€å¤´çš„è§£é‡Šï¼Œåœ¨ k8s ç³»ç»Ÿä¸­ï¼Œä¸€æ—¦åˆ›å»ºäº† CRDï¼Œå¯¹è¯¥ CRD çš„å¢åˆ æ”¹æŸ¥å…¶å®å°±å·²ç»è¢«æ”¯æŒäº†ï¼Œæˆ‘ä»¬çš„ Controller åªæ˜¯ç›‘å¬è‡ªå·±æ„Ÿå…´è¶£çš„èµ„æºäº‹ä»¶ï¼Œåšå‡ºçœŸå®çš„éƒ¨ç½²ã€æ›´æ–°ã€ç§»é™¤ç­‰åŠ¨ä½œã€‚

åœ¨æœ€åï¼Œæˆ‘ä»¬ä¹Ÿä¼šæ‰©å±•è‡ªå·±çš„ ç±»å‹ blogã€‚



## Kubebuilder

å¿«é€Ÿåˆ›å»ºä¸€ä¸ª mydemo:

```bash
â¯ go mod init mydemo
go: creating new go.mod: module mydemo
â¯ kubebuilder init --domain mydomain.com
â¯ tree -L 2
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ PROJECT
â”œâ”€â”€ README.md
â”œâ”€â”€ cmd
â”‚   â””â”€â”€ main.go
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ default
â”‚   â”œâ”€â”€ manager
â”‚   â”œâ”€â”€ prometheus
â”‚   â””â”€â”€ rbac
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ hack
    â””â”€â”€ boilerplate.go.txt
```



**æœ€ååˆ›å»ºCRDï¼š**

```bash
â¯ kubebuilder create api --group mygroup --version v1beta1 --kind GenericDaemon
```

`kubebuilder` ä¸ºæˆ‘ä»¬åˆ›å»ºäº†APIçš„æºï¼Œä»¥è®¿é—® `api/v1beta1` ä¸‹çš„CRDã€‚æ‚¨å¯ä»¥çœ‹åˆ°åˆ›å»ºçš„æ–‡ä»¶ä¸æˆ‘ä»¬ä¹‹å‰åœ¨ sample-controller ä¸­ç¼–è¾‘çš„æ–‡ä»¶ç±»ä¼¼ã€‚



**å†™ä¸€äº›ä»£ç :**

æˆ‘ä»¬éœ€è¦ä¿®æ”¹ `GenericDaemon` çš„ç»“æ„ï¼Œä¸ºæˆ‘ä»¬çš„å¯¹è±¡æ·»åŠ å¿…è¦çš„å­—æ®µã€‚ä¸è¦å¿˜è®°è®°å½•å­—æ®µï¼Œä»¥ä¾¿æ–‡æ¡£ç”Ÿæˆå™¨å¯ä»¥åˆ›å»ºè‰¯å¥½çš„æ–‡æ¡£ï¼š

```go
// api/v1beta1/genericdaemon_types.go
[...]
// GenericDaemonSpec defines the desired state of GenericDaemon
type GenericDaemonSpec struct {
  // Label is the value of the 'daemon=' label to set on a node that should run the daemon
  Label string `json:"label"`
  // Image is the Docker image to run for the daemon
  Image string `json:"image"`
}
// GenericDaemonStatus defines the observed state of GenericDaemon
type GenericDaemonStatus struct {
  // Count is the number of nodes the daemon is deployed to
  Count int32 `json:"count"`
}
[...]
```

ç„¶åè®©æˆ‘ä»¬æŒ‰ç…§ `genericdaemon_controller.go` æ–‡ä»¶ä¸­çš„TODOè¯´æ˜è¿›è¡Œæ“ä½œã€‚é¦–å…ˆåœ¨ `add` å‡½æ•°ä¸­ï¼Œè®©æˆ‘ä»¬å¬ `DaemonSet` è€Œä¸æ˜¯ `Deployment` ï¼š

```go
// pkg/controller/genericdaemon/genericdaemon_controller.go
func add(mgr manager.Manager, r reconcile.Reconciler) error {
  [...]
  // watch a Daemonset created by GenericDaemon
  err = c.Watch(&source.Kind{Type: &appsv1.DaemonSet{}}, &handler.EnqueueRequestForOwner{
    IsController: true,
    OwnerType:    &mygroupv1beta1.GenericDaemon{},
  })
  [...]
}
```



ç¬¬äºŒï¼Œè®©æˆ‘ä»¬ç¼–å†™ `Reconcile` å‡½æ•°çš„ä»£ç ã€‚

```go
// ...
```



**éƒ¨ç½²å’Œ play**

```
â¯ make
â¯ make docker-build IMG=cubxxw/genericdaemon
â¯ make docker-push IMG=cubxxw/genericdaemon
â¯ make deploy
```

å¦‚æœæ‚¨æ£€æŸ¥ `make deploy` å‘½ä»¤çš„è¾“å‡ºï¼Œæ‚¨å¯ä»¥çœ‹åˆ°è¯¥å‘½ä»¤ä¸º`operator` éƒ¨ç½²äº†CRDã€RBACè§’è‰²å’Œè§’è‰²ç»‘å®šä»¥è®¿é—®å¿…è¦çš„å¯¹è±¡ï¼Œä¸º`operator` åˆ›å»ºäº†å‘½åç©ºé—´ï¼Œä¸º`operator` åˆ›å»ºäº†æœåŠ¡å’Œstatefulsetã€‚

æ­¤æ—¶ï¼Œ`operator` åº”è¿è¡Œï¼š

```
â¯ kubectl get pods --namespace=mygroup-system
â¯ kubectl logs mygroup-controller-manager-6bdb7f7f88-vnhhb --namespace=mygroup-system
```



æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸ªæ€§åŒ–ç”Ÿæˆçš„ `GenericDaemon` æ ·æœ¬ï¼š

```yaml
â¯ cat config/samples/mygroup_v1beta1_genericdaemon.yaml
apiVersion: mygroup.mydomain.com/v1beta1
kind: GenericDaemon
metadata:
  labels:
    app.kubernetes.io/name: genericdaemon
    app.kubernetes.io/instance: genericdaemon-sample
    app.kubernetes.io/part-of: mygroup
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/created-by: mygroup
  name: genericdaemon-sample
spec:
  image: httpd
  label: http
```



å¹¶åˆ›å»ºå®ƒï¼š

```
â¯ kubectl apply -f config/samples/mygroup_v1beta1_genericdaemon.yaml
genericdaemon.mygroup.mydomain.com/genericdaemon-sample created
â¯ kubectl get genericdaemon
NAME                   AGE
genericdaemon-sample   11s
```



## åšå®¢å…ƒæ•°æ®

å¯ä»¥å°†Sample-Controllerç”¨äºç®¡ç† æˆ‘çš„åšå®¢ï¼ˆ[docker.nsddd.top](https://docker.nsddd.top/) OR  [go.nsddd.top](https://go.nsddd.top/)ï¼‰ çš„å…ƒæ•°æ®ã€‚

+ [å¯¹åº”çš„ PR è¯·æ±‚](https://github.com/cubxxw/sample-controller/pull/7)
+ [ä»“åº“åœ°å€](https://github.com/cubxxw/sample-controller/)

æŒ‰ç…§ä¸Šé¢å­¦ä¹ ç¼–å†™ controller çš„é€»è¾‘ï¼Œæˆ‘æŠŠæ­¥éª¤åˆ†ä¸ºä¸‰æ­¥ï¼ˆé’ˆå¯¹ kubebuilder ï¼‰ï¼š

1. Create CRD and CR object
2. Write  controller code
3. Deploy controller

ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘å¸Œæœ›å°† Kubebuilder å’Œ code-generator ç›¸ç»“åˆï¼Œä½¿ç”¨Kubebuilderç”ŸæˆCRDå’Œä¸€æ•´å¥—æ§åˆ¶å™¨æ¶æ„ï¼Œå†ä½¿ç”¨ code-generator ç”Ÿæˆ `informers`ã€`listers`ã€`clientsets`ç­‰ã€‚

é’ˆå¯¹é€šè¿‡ä»£ç ç”Ÿæˆå™¨å†™ controller:

+ å®šä¹‰CRD
+ ç”Ÿæˆè‡ªå®šä¹‰èµ„æºçš„Clientsetã€Informersã€Listersç­‰
+ ç¼–å†™Controllerç­‰ä»£ç 



### å®šä¹‰è‡ªå®šä¹‰æè¿°

å–åä¸ºï¼š`cat crd-blog.yaml`

```yaml
â¯ cat artifacts/examples/crd-blog.yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: blogs.controller.nsddd.top
spec:
  group: controller.nsddd.top
  version: v1beta1
  names:
    kind: Blog
    plural: blogs
  scope: Namespaced
```



**éƒ¨ç½²è¯¥èµ„æºå®šä¹‰:**

```bash
â¯ k apply -f  artifacts/examples/crd-blog.yaml
customresourcedefinition.apiextensions.k8s.io/blogs.controller.nsddd.top created
â¯ k get crd
NAME                                  CREATED AT
blogs.controller.nsddd.top            2023-04-09T04:13:15Z
```



**æ„å»º examples `example-blog.yaml`ï¼š**

```bash
â¯ cat example-blog.yaml
apiVersion: controller.nsddd.top/v1beta1
kind: Blog
metadata:
  name: example-blog
spec:
  deploymentName: example-blog
  replicas: 1
  title: "example blog"
  author: "Xinwei Xiong"
  content: "blog content"
  lastUpdate: "2023-04-09"
  prev: ""
  next: ""

â¯ k apply -f  artifacts/examples/example-blog.yaml
blog.controller.nsddd.top/example-blog created

â¯ k get Blog
NAME           AGE
example-blog   61s
```

å½“ç„¶è¿™è¿˜è¿œè¿œä¸å¤Ÿï¼Œæ²¡æœ‰ controllerã€‚



### å¼€å‘å…³äº blog èµ„æºæ“æ§çš„ controller ç«¯ä»£ç 

é¦–å…ˆåœ¨ `pkg/apis` ä¸‹åˆ›å»ºç›®å½• `nsdddcontroller`ï¼Œç„¶åå°† `samplecontroller` é‡Œçš„æ‰€æœ‰æ–‡ä»¶å¤åˆ¶åˆ° `nsdddcontroller`ï¼Œæˆ‘ä»¬åœ¨å­¦ä¹  `sample-controller` çš„æ—¶å€™æ¼”ç¤ºè¿‡æ¡ˆä¾‹`genericdaemon`ã€‚

> ä¸ºäº† ç¬¦åˆ Kubernetes çš„ç‰ˆæœ¬è§„èŒƒï¼Œæˆ‘ä»¬å°†å…¶ä½œä¸º bate ç‰ˆæœ¬ï¼Œå¯ä»¥å¯¹å¤–æä¾›~

```bash
â¯ tree nsdddcontroller
nsdddcontroller
â”œâ”€â”€ register.go
â””â”€â”€ v1beta1
    â”œâ”€â”€ doc.go
    â”œâ”€â”€ register.go
    â””â”€â”€ types.go
```



ä¿®æ”¹æ³¨å†Œè¡¨çš„é…ç½®ï¼š

+ `pkg/apis/nsdddcontroller/register.go`

```go
package nsdddcontroller

// GroupName is the group name used in this package
const (
	GroupName = "nsdddcontroller.k8s.io"
)
```



ä¿®æ”¹ type æ–‡ä»¶ï¼š

+ `pkg/apis/nsdddcontroller/v1bate1/types.go`

```go
package v1beta1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// Blog is a specification for a Blog resource
type Blog struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   BlogSpec   `json:"spec"`
	Status BlogStatus `json:"status"`
}

// BlogSpec is the spec for a Blog resource
type BlogSpec struct {
	DeploymentName string `json:"deploymentName"`
	Replicas       *int32 `json:"replicas"`
	Title          string `json:"title"`
	Author         string `json:"author"`
	Content        string `json:"content"`
	LastUpdate     string `json:"lastUpdate"`
	Prev           string `json:"prev"`
	Next           string `json:"next"`
}

// BlogStatus is the status for a Blog resource
type BlogStatus struct {
	AvailableReplicas int32 `json:"availableReplicas"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// BlogList is a list of Blog resources
type BlogList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata"`

	Items []Blog `json:"items"`
}
```



### ä»£ç ç”Ÿæˆå™¨

 ä¿®æ”¹ `hack/update-codegen.sh`ï¼Œå¢åŠ å¯¹ Blog çš„ä»£ç ç”Ÿæˆå‘½ä»¤ï¼ˆå¯¹ nsdddcontroller åŒ…çš„ä»£ç ç”Ÿæˆï¼‰

> åœ¨æ­¤ä¹‹å‰ä½ åº”è¯¥ç”¨ git å¤‡ä»½ä¸€ä¸‹ï¼Œå¹¶ä¸”å‡†å¤‡ä»£ç ç”Ÿæˆå™¨è„šæœ¬æˆ–è€…äºŒè¿›åˆ¶ `code-generator`
>
> ```bash
> â¯ go mod vendor
> â¯ chmod +x vendor/k8s.io/code-generator/generate-groups.sh
> ```

```bash
echo "===> Generating genericdaemon code for Blog"
"${CODEGEN_PKG}"/generate-groups.sh "deepcopy,client,informer,lister" \
  k8s.io/sample-controller/pkg/client_Blog \ 
  k8s.io/sample-controller/pkg/apis \
  nsdddcontroller:v1Bate1 \
  --output-base "$(dirname "${BASH_SOURCE[0]}")/../../.." \
  --go-header-file "${SCRIPT_ROOT}"/hack/boilerplate.go.txt
```



âš ï¸ è¿™é‡Œæœ‰ä¸ªå‘è®°å½•ä¸‹ï¼Œæˆ‘ç”¨ä»£ç ç”Ÿæˆå™¨ç”Ÿæˆçš„ä¸€ç›´æœ‰é—®é¢˜ï¼Œä½¿ç”¨çš„æ˜¯ `GO111MODULE=on` æ¨¡å—ã€‚ä¿®æ”¹åçš„è„šæœ¬å¦‚ä¸‹ï¼š

+ [è¿™é‡Œæˆ‘åœ¨ Kubernetes ä¸­æ issue çš„è®°å½•ï¼Œä¼šè®°å½•æ‰€æœ‰ ä»£ç ç”Ÿæˆå™¨ å‡ºç°çš„é—®é¢˜ï¼Œè¿™ä¸ªæ–‡ç« å°±ä¸è¡¥å……äº†ã€‚](https://github.com/kubernetes/kubernetes/issues/117181)

è·å– è®¸å¯è¯å¤´æ–‡ä»¶çš„å˜é‡å’Œ è·å–åŒ…çš„è·¯å¾„ï¼š

```bash
SCRIPT_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
CODEGEN_PKG=${CODEGEN_PKG:-$(cd "${SCRIPT_ROOT}"; ls -d -1 ./vendor/k8s.io/code-generator 2>/dev/null || echo ../code-generator)}
```

> è„šæœ¬å°†`SCRIPT_ROOT`å˜é‡è®¾ç½®ä¸ºè„šæœ¬æ–‡ä»¶çš„ç›®å½•åç§°ï¼Œç„¶åå°†`CODEGEN_PKG`å˜é‡è®¾ç½®ä¸º`code-generator`è½¯ä»¶åŒ…çš„è·¯å¾„ã€‚å¦‚æœæœªè®¾ç½®`CODEGEN_PKG`ç¯å¢ƒå˜é‡ï¼Œåˆ™è„šæœ¬ä¼šæ£€æŸ¥`SCRIPT_ROOT`è·¯å¾„çš„vendorç›®å½•ä¸­æ˜¯å¦å®‰è£…äº†`code-generator`è½¯ä»¶åŒ…ï¼Œå¹¶ç›¸åº”åœ°è®¾ç½®`CODEGEN_PKG`å˜é‡ã€‚

```bash
â¯ pwd
/root/workspaces/sample-controller

â¯ vendor/k8s.io/code-generator/generate-groups.sh \
"deepcopy,client,informer,lister" \
  k8s.io/sample-controller/pkg/generated_blog \
  k8s.io/sample-controller/pkg/apis \
  nsdddcontroller:v1Bate1 \
  --output-base "/root/workspaces" \
  --go-header-file "hack/boilerplate.go.txt"
```



###  å†™ controller

è§„èŒƒæ€§çš„å°† controller æ”¾å…¥åˆ° `pkg` ä¸­ï¼Œè€Œä¸æ˜¯ `rootfs`ï¼Œå–åä¸º `pkg/blog_controller.go`

æ•´ä¸ªæ§åˆ¶å™¨é€»è¾‘éƒ½å†™å‡ºæ¥ä¼šæœ‰äº›æ··ä¹±ï¼Œä¸ºäº†æœ‰åŠ©äºç†è§£ï¼Œæˆ‘ä»¬ç»§æ‰¿ç¬¬ä¸€æ¬¡é˜…è¯» controller.go çš„æ—¶å€™çš„é€»è¾‘ï¼Œå†å¯¹ controller è¿›è¡Œä¸€æ¬¡æ·±å…¥é˜…è¯»å’Œç¼–å†™ã€‚

Controller ç»“æ„ä½“çš„å®šä¹‰ï¼š

```go
// BlogController is the controller implementation for Blog resources
type BlogController struct {
	// kubeclientset is a standard kubernetes clientset
	kubeclientset kubernetes.Interface
	// shidaclientset is a clientset for our own API group
	nsdddclientset clientset.Interface
	deploymentsLister appslisters.DeploymentLister
	deploymentsSynced cache.InformerSynced
	blogsLister        listers.BlogLister
	blogsSynced        cache.InformerSynced
	workqueue workqueue.RateLimitingInterface
	recorder record.EventRecorder
}
```

è¿™é‡Œå®šä¹‰äº†ä¸€äº› Clientsetï¼Œç”¨æ¥å¯¹ Kubernetes çš„ API Server è¿›è¡Œäº¤äº’ï¼Œè¿™é‡Œä¹Ÿå®šä¹‰äº†ä¸€äº› Listerï¼Œç”¨æ¥è·å– Kubernetes çš„ API çš„ä¿¡æ¯ï¼Œåœ¨ Lister ä¸­åˆ©ç”¨ç¼“å­˜ä¹Ÿèƒ½å‡å°‘å¯¹ API Server çš„è®¿é—®å‹åŠ›ã€‚

workqueue å’Œ Informer å…±åŒå®Œæˆ Kubernetes controller çš„æ ¸å¿ƒï¼šè°ƒè°çš„æ­¥éª¤



ç»§ç»­ï¼Œæ‰¾åˆ°åˆå§‹åŒ– Controller çš„åœ°æ–¹(`NewBlogController`)ï¼Œè¿™ä¸ªé€»è¾‘ä½œä¸ºå…¥å£ä¾› main.go åˆå§‹åŒ–ï¼Œæ‰€ä»¥å’Œ `Run()` ç”¨å¤§å†™è°ƒç”¨ã€‚

```go
func NewBlogController(
	kubeclientset kubernetes.Interface,
	nsdddcclientset clientset.Interface,
	deploymentInformer appsinformers.DeploymentInformer,
	blogInformer informers.BlogInformer) *BlogController {
	klog.V(4).Info("Creating event broadcaster")
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartLogging(klog.Infof)
	eventBroadcaster.StartRecordingToSink(&typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events("")})
	recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: blogControllerAgentName})

	controller := &BlogController{
		kubeclientset:     kubeclientset,
		nsdddcclientset:   nsdddcclientset,
		deploymentsLister: deploymentInformer.Lister(),
		deploymentsSynced: deploymentInformer.Informer().HasSynced,
		blogsLister:        blogInformer.Lister(),
		blogsSynced:        blogInformer.Informer().HasSynced,
		workqueue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "Blogs"),
		recorder:          recorder,
	}

	klog.Info("Setting up event handlers")
	blogInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.enqueueBlog,
		UpdateFunc: func(old, new interface{}) {
			controller.enqueueBlog(new)
		},
	})
	deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.handleObject,
		UpdateFunc: func(old, new interface{}) {
			newDepl := new.(*appsv1.Deployment)
			oldDepl := old.(*appsv1.Deployment)
			if newDepl.ResourceVersion == oldDepl.ResourceVersion {
				// Periodic resync will send update events for all known Deployments.
				// Two different versions of the same Deployment will always have different RVs.
				return
			}
			controller.handleObject(new)
		},
		DeleteFunc: controller.handleObject,
	})

	return controller
}
```

`AddToScheme` ç”¨äºå‘ `runtime.Scheme` ä¸­æ·»åŠ æ–°çš„ API å¯¹è±¡ã€‚`runtime.Scheme` æ˜¯ä¸€ä¸ªå­˜å‚¨ Kubernetes API å¯¹è±¡ Schema çš„å¯¹è±¡ï¼Œç”¨äºè·¨ API ç‰ˆæœ¬å’Œ API ç»„å…±äº«ç±»å‹ä¿¡æ¯ã€‚é€šè¿‡ä½¿ç”¨ AddToScheme æ–¹æ³•ï¼Œæ‚¨å¯ä»¥å°†è‡ªå®šä¹‰ API å¯¹è±¡æ·»åŠ åˆ° `runtime.Scheme` ä¸­ï¼Œä»¥ä¾¿åœ¨ Kubernetes API ä¸­ä½¿ç”¨è‡ªå®šä¹‰ API å¯¹è±¡ã€‚

> Scheme æ˜¯ä¸€ä¸ªç”¨äºå­˜å‚¨ Kubernetes API å¯¹è±¡ Schema çš„å¯¹è±¡ï¼Œå®ƒæ˜¯ Kubernetes API çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºè·¨ API ç‰ˆæœ¬å’Œ API ç»„å…±äº«ç±»å‹ä¿¡æ¯ã€‚å®ƒå…è®¸æ³¨å†Œå’Œç®¡ç†è‡ªå®šä¹‰ API å¯¹è±¡ï¼Œå¹¶ä¸”è¿˜æä¾›äº†ä¸€äº›è¾…åŠ©æ–¹æ³•ï¼Œä¾‹å¦‚åºåˆ—åŒ–å’Œååºåˆ—åŒ– Kubernetes å¯¹è±¡ã€‚

æˆ‘ä»¬çŸ¥é“äº† `Informer` æ˜¯ç”¨äºInformer ç›‘è§†æŸäº›èµ„æºçš„å¯¹è±¡ï¼Œå®ƒä¼šåœ¨èµ„æºå‘ç”Ÿå˜åŒ–æ—¶é€šçŸ¥æ§åˆ¶å™¨ã€‚é‚£ä¹ˆ å®šä¹‰çš„ `HasSynced` æ˜¯ä»€ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥çœ‹ä¸‹å®šä¹‰ï¼š

> HasSynced æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ£€æŸ¥ Informer æ˜¯å¦å·²ç»å®Œæˆäº†èµ„æºçš„åŒæ­¥ã€‚å½“è°ƒç”¨ Informer çš„ HasSynced å‡½æ•°æ—¶ï¼Œå¦‚æœæ‰€æœ‰ç›‘æ§çš„èµ„æºéƒ½å·²ç»åŒæ­¥å®Œæˆï¼Œå‡½æ•°å°†è¿”å› trueã€‚

`AddEventHandler` æ˜¯ä¸€ä¸ªä¸º Informer æ³¨å†Œäº‹ä»¶å¤„ç†å‡½æ•°ã€‚

å…¶ä»–çš„å®šä¹‰éƒ¨åˆ†ï¼š

::: details å±•å¼€ä»£ç å—
```go
/*
Copyright 2017 The Kubernetes Authors.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package main

import (
	"context"
	"fmt"
	"time"

	appsv1 "k8s.io/api/apps/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/apimachinery/pkg/util/wait"
	appsinformers "k8s.io/client-go/informers/apps/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/kubernetes/scheme"
	typedcorev1 "k8s.io/client-go/kubernetes/typed/core/v1"
	appslisters "k8s.io/client-go/listers/apps/v1"
	"k8s.io/client-go/tools/cache"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/workqueue"
	"k8s.io/klog/v2"

	nsdddv1beta1 "k8s.io/sample-controller/pkg/apis/nsdddcontroller/v1beta1"
	clientset "k8s.io/sample-controller/pkg/generated_blog/clientset/versioned"
	samplescheme "k8s.io/sample-controller/pkg/generated_blog/clientset/versioned/scheme"
	informers "k8s.io/sample-controller/pkg/generated_blog/informers/externalversions/nsdddcontroller/v1beta1"
	listers "k8s.io/sample-controller/pkg/generated_blog/listers/nsdddcontroller/v1beta1"
)

const blogControllerAgentName = "nsddd-controller"

const (
	// BlogSuccessSynced is used as part of the Event 'reason' when a Blog is synced
	BlogSuccessSynced = "Synced"
	// BlogErrResourceExists is used as part of the Event 'reason' when a Blog fails
	// to sync due to a Deployment of the same name already existing.
	BlogErrResourceExists = "ErrResourceExists"

	// BlogMessageResourceExists is the message used for Events when a resource
	// fails to sync due to a Deployment already existing
	BlogMessageResourceExists = "Resource %q already exists and is not managed by Blog"
	// BlogMessageResourceSynced is the message used for an Event fired when a Blog
	// is synced successfully
	BlogMessageResourceSynced = "Blog synced successfully"
)

// BlogController is the controller implementation for Blog resources
type BlogController struct {
	// kubeclientset is a standard kubernetes clientset
	kubeclientset kubernetes.Interface
	// nsdddclientset is a clientset for our own API group
	nsdddclientset clientset.Interface

	deploymentsLister appslisters.DeploymentLister
	deploymentsSynced cache.InformerSynced
	blogsLister       listers.BlogLister
	blogsSynced       cache.InformerSynced

	// workqueue is a rate limited work queue. This is used to queue work to be
	// processed instead of performing it as soon as a change happens. This
	// means we can ensure we only process a fixed amount of resources at a
	// time, and makes it easy to ensure we are never processing the same item
	// simultaneously in two different workers.
	workqueue workqueue.RateLimitingInterface
	// recorder is an event recorder for recording Event resources to the
	// Kubernetes API.
	recorder record.EventRecorder
}

// NewBlogController returns a new sample controller
func NewBlogController(
	kubeclientset kubernetes.Interface,
	nsdddclientset clientset.Interface,
	deploymentInformer appsinformers.DeploymentInformer,
	blogInformer informers.BlogInformer) *BlogController {

	// Create event broadcaster
	// Add sample-controller types to the default Kubernetes Scheme so Events can be
	// logged for sample-controller types.
	utilruntime.Must(samplescheme.AddToScheme(scheme.Scheme))
	klog.V(4).Info("Creating event broadcaster")
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartLogging(klog.Infof)
	eventBroadcaster.StartRecordingToSink(&typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events("")})
	recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: blogControllerAgentName})

	controller := &BlogController{
		kubeclientset:     kubeclientset,
		nsdddclientset:    nsdddclientset,
		deploymentsLister: deploymentInformer.Lister(),
		deploymentsSynced: deploymentInformer.Informer().HasSynced,
		blogsLister:       blogInformer.Lister(),
		blogsSynced:       blogInformer.Informer().HasSynced,
		workqueue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "Blogs"),
		recorder:          recorder,
	}

	klog.Info("Setting up event handlers")
	// Set up an event handler for when Blog resources change
	blogInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.enqueueBlog,
		UpdateFunc: func(old, new interface{}) {
			controller.enqueueBlog(new)
		},
	})
	// Set up an event handler for when Deployment resources change. This
	// handler will lookup the owner of the given Deployment, and if it is
	// owned by a Blog resource will enqueue that Blog resource for
	// processing. This way, we don't need to implement custom logic for
	// handling Deployment resources. More info on this pattern:
	// https://github.com/kubernetes/community/blob/8cafef897a22026d42f5e5bb3f104febe7e29830/contributors/devel/controllers.md
	deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.handleObject,
		UpdateFunc: func(old, new interface{}) {
			newDepl := new.(*appsv1.Deployment)
			oldDepl := old.(*appsv1.Deployment)
			if newDepl.ResourceVersion == oldDepl.ResourceVersion {
				// Periodic resync will send update events for all known Deployments.
				// Two different versions of the same Deployment will always have different RVs.
				return
			}
			controller.handleObject(new)
		},
		DeleteFunc: controller.handleObject,
	})

	return controller
}

// Run will set up the event handlers for types we are interested in, as well
// as syncing informer caches and starting workers. It will block until stopCh
// is closed, at which point it will shutdown the workqueue and wait for
// workers to finish processing their current work items.
func (c *BlogController) Run(threadiness int, stopCh <-chan struct{}) error {
	defer utilruntime.HandleCrash()
	defer c.workqueue.ShutDown()

	// Start the informer factories to begin populating the informer caches
	klog.Info("Starting Blog controller")

	// Wait for the caches to be synced before starting workers
	klog.Info("Waiting for informer caches to sync")
	if ok := cache.WaitForCacheSync(stopCh, c.deploymentsSynced, c.blogsSynced); !ok {
		return fmt.Errorf("failed to wait for caches to sync")
	}

	klog.Info("Starting workers")
	// Launch two workers to process Blog resources
	for i := 0; i < threadiness; i++ {
		go wait.Until(c.runWorker, time.Second, stopCh)
	}

	klog.Info("Started workers")
	<-stopCh
	klog.Info("Shutting down workers")

	return nil
}

// runWorker is a long-running function that will continually call the
// processNextWorkItem function in order to read and process a message on the
// workqueue.
func (c *BlogController) runWorker() {
	for c.processNextWorkItem() {
	}
}

// processNextWorkItem will read a single work item off the workqueue and
// attempt to process it, by calling the syncHandler.
func (c *BlogController) processNextWorkItem() bool {
	obj, shutdown := c.workqueue.Get()

	if shutdown {
		return false
	}

	// We wrap this block in a func so we can defer c.workqueue.Done.
	err := func(obj interface{}) error {
		// We call Done here so the workqueue knows we have finished
		// processing this item. We also must remember to call Forget if we
		// do not want this work item being re-queued. For example, we do
		// not call Forget if a transient error occurs, instead the item is
		// put back on the workqueue and attempted again after a back-off
		// period.
		defer c.workqueue.Done(obj)
		var key string
		var ok bool
		// We expect strings to come off the workqueue. These are of the
		// form namespace/name. We do this as the delayed nature of the
		// workqueue means the items in the informer cache may actually be
		// more up to date that when the item was initially put onto the
		// workqueue.
		if key, ok = obj.(string); !ok {
			// As the item in the workqueue is actually invalid, we call
			// Forget here else we'd go into a loop of attempting to
			// process a work item that is invalid.
			c.workqueue.Forget(obj)
			utilruntime.HandleError(fmt.Errorf("expected string in workqueue but got %#v", obj))
			return nil
		}
		// Run the syncHandler, passing it the namespace/name string of the
		// Blog resource to be synced.
		if err := c.syncHandler(key); err != nil {
			// Put the item back on the workqueue to handle any transient errors.
			c.workqueue.AddRateLimited(key)
			return fmt.Errorf("error syncing '%s': %s, requeuing", key, err.Error())
		}
		// Finally, if no error occurs we Forget this item so it does not
		// get queued again until another change happens.
		c.workqueue.Forget(obj)
		klog.Infof("Successfully synced '%s'", key)
		return nil
	}(obj)

	if err != nil {
		utilruntime.HandleError(err)
		return true
	}

	return true
}

// syncHandler compares the actual state with the desired, and attempts to
// converge the two. It then updates the Status block of the Blog resource
// with the current status of the resource.
func (c *BlogController) syncHandler(key string) error {
	// Convert the namespace/name string into a distinct namespace and name
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("invalid resource key: %s", key))
		return nil
	}

	// Get the Blog resource with this namespace/name
	blog, err := c.blogsLister.Blogs(namespace).Get(name)
	if err != nil {
		// The Blog resource may no longer exist, in which case we stop
		// processing.
		if errors.IsNotFound(err) {
			utilruntime.HandleError(fmt.Errorf("blog '%s' in work queue no longer exists", key))
			return nil
		}

		return err
	}

	deploymentName := blog.Spec.DeploymentName
	if deploymentName == "" {
		// We choose to absorb the error here as the worker would requeue the
		// resource otherwise. Instead, the next time the resource is updated
		// the resource will be queued again.
		utilruntime.HandleError(fmt.Errorf("%s: deployment name must be specified", key))
		return nil
	}

	// Get the deployment with the name specified in Blog.spec
	deployment, err := c.deploymentsLister.Deployments(blog.Namespace).Get(deploymentName)
	// If the resource doesn't exist, we'll create it
	if errors.IsNotFound(err) {
		deployment, err = c.kubeclientset.AppsV1().Deployments(blog.Namespace).Create(context.TODO(), newBlogDeployment(blog), metav1.CreateOptions{})
	}

	// If an error occurs during Get/Create, we'll requeue the item so we can
	// attempt processing again later. This could have been caused by a
	// temporary network failure, or any other transient reason.
	if err != nil {
		return err
	}

	// If the Deployment is not controlled by this Blog resource, we should log
	// a warning to the event recorder and ret
	if !metav1.IsControlledBy(deployment, blog) {
		msg := fmt.Sprintf(BlogMessageResourceExists, deployment.Name)
		c.recorder.Event(blog, corev1.EventTypeWarning, BlogErrResourceExists, msg)
		return fmt.Errorf(msg)
	}

	// If this number of the replicas on the Blog resource is specified, and the
	// number does not equal the current desired replicas on the Deployment, we
	// should update the Deployment resource.
	if blog.Spec.Replicas != nil && *blog.Spec.Replicas != *deployment.Spec.Replicas {
		klog.V(4).Infof("Blog %s replicas: %d, deployment replicas: %d", name, *blog.Spec.Replicas, *deployment.Spec.Replicas)
		deployment, err = c.kubeclientset.AppsV1().Deployments(blog.Namespace).Update(context.TODO(), newBlogDeployment(blog), metav1.UpdateOptions{})
	}

	// If an error occurs during Update, we'll requeue the item so we can
	// attempt processing again later. THis could have been caused by a
	// temporary network failure, or any other transient reason.
	if err != nil {
		return err
	}

	// Finally, we update the status block of the Blog resource to reflect the
	// current state of the world
	err = c.updateBlogStatus(blog, deployment)
	if err != nil {
		return err
	}

	c.recorder.Event(blog, corev1.EventTypeNormal, BlogSuccessSynced, BlogMessageResourceSynced)
	return nil
}

func (c *BlogController) updateBlogStatus(blog *nsdddv1beta1.Blog, deployment *appsv1.Deployment) error {
	// NEVER modify objects from the store. It's a read-only, local cache.
	// You can use DeepCopy() to make a deep copy of original object and modify this copy
	// Or create a copy manually for better performance
	blogCopy := blog.DeepCopy()
	blogCopy.Status.AvailableReplicas = deployment.Status.AvailableReplicas
	// If the CustomResourceSubresources feature gate is not enabled,
	// we must use Update instead of UpdateStatus to update the Status block of the Blog resource.
	// UpdateStatus will not allow changes to the Spec of the resource,
	// which is ideal for ensuring nothing other than resource status has been updated.
	_, err := c.nsdddclientset.ControllerV1beta1().Blogs(blog.Namespace).UpdateStatus(context.TODO(), blogCopy, metav1.UpdateOptions{})
	return err
}

// enqueueBlog takes a Blog resource and converts it into a namespace/name
// string which is then put onto the work queue. This method should *not* be
// passed resources of any type other than Blog.
func (c *BlogController) enqueueBlog(obj interface{}) {
	var key string
	var err error
	if key, err = cache.MetaNamespaceKeyFunc(obj); err != nil {
		utilruntime.HandleError(err)
		return
	}
	c.workqueue.AddRateLimited(key)
}

// handleObject will take any resource implementing metav1.Object and attempt
// to find the Blog resource that 'owns' it. It does this by looking at the
// objects metadata.ownerReferences field for an appropriate OwnerReference.
// It then enqueues that Blog resource to be processed. If the object does not
// have an appropriate OwnerReference, it will simply be skipped.
func (c *BlogController) handleObject(obj interface{}) {
	var object metav1.Object
	var ok bool
	if object, ok = obj.(metav1.Object); !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("error decoding object, invalid type"))
			return
		}
		object, ok = tombstone.Obj.(metav1.Object)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("error decoding object tombstone, invalid type"))
			return
		}
		klog.V(4).Infof("Recovered deleted object '%s' from tombstone", object.GetName())
	}
	klog.V(4).Infof("Processing object: %s", object.GetName())
	if ownerRef := metav1.GetControllerOf(object); ownerRef != nil {
		// If this object is not owned by a Blog, we should not do anything more
		// with it.
		if ownerRef.Kind != "Blog" {
			return
		}

		blog, err := c.blogsLister.Blogs(object.GetNamespace()).Get(ownerRef.Name)
		if err != nil {
			klog.V(4).Infof("ignoring orphaned object '%s' of blog '%s'", object.GetSelfLink(), ownerRef.Name)
			return
		}

		c.enqueueBlog(blog)
		return
	}
}

// newBlogDeployment creates a new Deployment for a Blog resource. It also sets
// the appropriate OwnerReferences on the resource so handleObject can discover
// the Blog resource that 'owns' it.
func newBlogDeployment(blog *nsdddv1beta1.Blog) *appsv1.Deployment {
	labels := map[string]string{
		"app":        "nginx",
		"controller": blog.Name,
	}
	return &appsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      blog.Spec.DeploymentName,
			Namespace: blog.Namespace,
			OwnerReferences: []metav1.OwnerReference{
				*metav1.NewControllerRef(blog, schema.GroupVersionKind{
					Group:   nsdddv1beta1.SchemeGroupVersion.Group,
					Version: nsdddv1beta1.SchemeGroupVersion.Version,
					Kind:    "Blog",
				}),
			},
		},
		Spec: appsv1.DeploymentSpec{
			Replicas: blog.Spec.Replicas,
			Selector: &metav1.LabelSelector{
				MatchLabels: labels,
			},
			Template: corev1.PodTemplateSpec{
				ObjectMeta: metav1.ObjectMeta{
					Labels: labels,
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "nginx",
							Image: "nginx:latest",
						},
					},
				},
			},
		},
	}
}
```

:::



### å¯åŠ¨æ§åˆ¶å™¨

æœ€åï¼Œåœ¨ `pkg/main.go` é‡Œåˆ›å»ºæˆ‘ä»¬çš„ Blog Controllerï¼Œå¹¶å¯åŠ¨æ§åˆ¶å™¨

```go
/*
Copyright 2017 The Kubernetes Authors.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package main

import (
	"flag"
	"time"

	kubeinformers "k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/klog/v2"

	// Uncomment the following line to load the gcp plugin (only required to authenticate against GKE clusters).
	// _ "k8s.io/client-go/plugin/pkg/client/auth/gcp"

	clientset "k8s.io/sample-controller/pkg/generated/clientset/versioned"
	informers "k8s.io/sample-controller/pkg/generated/informers/externalversions"

	blogclientset "k8s.io/sample-controller/pkg/generated_blog/clientset/versioned"
	bloginformers "k8s.io/sample-controller/pkg/generated_blog/informers/externalversions"
	"k8s.io/sample-controller/pkg/signals"
)

var (
	masterURL  string
	kubeconfig string
)

func main() {
	flag.Parse()

	// set up signals so we handle the shutdown signal gracefully
	ctx := signals.SetupSignalHandler()
	logger := klog.FromContext(ctx)

	cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)
	if err != nil {
		logger.Error(err, "Error building kubeconfig")
		klog.Fatalf("Error building kubeconfig: %s", err.Error())
	}

	kubeClient, err := kubernetes.NewForConfig(cfg)
	if err != nil {
		logger.Error(err, "Error building kubernetes clientset")
		klog.Fatalf("Error building kubernetes clientset: %s", err.Error())
	}

	exampleClient, err := clientset.NewForConfig(cfg)
	if err != nil {
		logger.Error(err, "Error building exampleClient is clientset")
		klog.Fatalf("Error building example clientset: %s", err.Error())
	}

	blogClient, err := blogclientset.NewForConfig(cfg)
	if err != nil {
		logger.Error(err, "Error building blogClient is clientset")
		klog.Fatalf("Error building blog clientset: %s", err.Error())
	}

	kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)
	exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30)
	blogInformerFactory := bloginformers.NewSharedInformerFactory(blogClient, time.Second*30)

	blogController := NewBlogController(kubeClient, blogClient,
		kubeInformerFactory.Apps().V1().Deployments(),
		blogInformerFactory.Controller().V1beta1().Blogs())

	// notice that there is no need to run Start methods in a separate goroutine. (i.e. go kubeInformerFactory.Start(ctx.Done())
	// Start method is non-blocking and runs all registered informers in a dedicated goroutine.
	kubeInformerFactory.Start(ctx.Done())
	exampleInformerFactory.Start(ctx.Done())
	blogInformerFactory.Start(ctx.Done())

	go func() {
		err = blogController.Run(2, ctx.Done())
		if err != nil {
			klog.Fatalf("Error running controller: %s", err.Error())
		}
	}()

	if err = blogController.Run(2, ctx.Done()); err != nil {
		klog.Fatalf("Error running controller: %s", err.Error())
	}
}

func init() {
	flag.StringVar(&kubeconfig, "kubeconfig", "", "Path to a kubeconfig. Only required if out-of-cluster.")
	flag.StringVar(&masterURL, "master", "", "The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.")
}
```



### éªŒè¯

**é‡æ–°ç¼–è¯‘ sample-controller é¡¹ç›®ï¼Œå¹¶è¿è¡Œ sample-controller CRD æ§åˆ¶å™¨**

```bash
# assumes you have a working kubeconfig, not required if operating in-cluster
go build -o sample-controller .
./sample-controller -kubeconfig=$HOME/.kube/config

# create a CustomResourceDefinition
kubectl create -f artifacts/examples/crd-status-subresource.yaml

# create a custom resource of type Foo
kubectl create -f artifacts/examples/example-foo.yaml

# check deployments created through the custom resource
kubectl get deployments
```



## ä½¿ç”¨ Kubebuilder æ„å»º





## END é“¾æ¥

<ul><li><div><a href = '65.md' style='float:left'>â¬†ï¸ä¸Šä¸€èŠ‚ğŸ”—  </a><a href = '67.md' style='float: right'>  ï¸ä¸‹ä¸€èŠ‚ğŸ”—</a></div></li></ul>

+ [â“‚ï¸å›åˆ°ç›®å½•ğŸ ](../README.md)

+ [**ğŸ«µå‚ä¸è´¡çŒ®ğŸ’â¤ï¸â€ğŸ”¥ğŸ’–**](https://nsddd.top/archives/contributors))

+ âœ´ï¸ç‰ˆæƒå£°æ˜ &copy; ï¼šæœ¬ä¹¦æ‰€æœ‰å†…å®¹éµå¾ª[CC-BY-SA 3.0åè®®ï¼ˆç½²å-ç›¸åŒæ–¹å¼å…±äº«ï¼‰&copy;](http://zh.wikipedia.org/wiki/Wikipedia:CC-by-sa-3.0åè®®æ–‡æœ¬) 



**å…³äºCRDæœ‰ä¸€äº›é“¾æ¥:**

+ å®˜æ–¹æ–‡æ¡£ï¼šhttps://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/
+ å®˜æ–¹è§£é‡Šï¼šhttps://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition
+ CRD Yamlçš„Schemaï¼šhttps://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#customresourcedefinition-v1beta1-apiextensions-k8s-io
+ https://kubernetes.feisky.xyz/cha-jian-kuo-zhan/api/customresourcedefinition
+ https://book.kubebuilder.io/
+ [kubernetes write controller](https://github.com/kubernetes/community/blob/8cafef897a22026d42f5e5bb3f104febe7e29830/contributors/devel/controllers.md)
+ ä¹¦ç±ï¼šã€ŠKubernetes Operator å¼€å‘è¿›é˜¶ - èƒ¡æ¶›ã€‹ ä½†ä¸æ¨èè´­ä¹°~



**è¿™ç¯‡æ–‡ç« å‚è€ƒçš„åšå®¢è¿æ¥ï¼š**

+ [ä½¿ç”¨code-generatorç”Ÿæˆcrdçš„clientsetã€informerã€listers](https://xieys.club/code-generator-crd/)
+ [kubernetes1.9ç®¡ä¸­çª¥è±¹-CRDæ¦‚å¿µã€ä½¿ç”¨åœºæ™¯åŠå®ä¾‹](https://sq.163yun.com/blog/article/174980128954048512)
+ [ç»“åˆKubebuilderä¸code-generatorå¼€å‘Operator](https://segmentfault.com/a/1190000039706356)
+ [kubebuilder èƒ½å¦ç”Ÿæˆç±»ä¼¼ client-go çš„ sdk?](https://lailin.xyz/post/operator-kubebuilder-clientset.html)
+ [k8sè‡ªå®šä¹‰controllerä¸‰éƒ¨æ›²ä¹‹ä¸€:åˆ›å»ºCRDï¼ˆCustom Resource Definitionï¼‰](https://blog.csdn.net/boling_cavalry/article/details/88917818)
+ [sample-controller å®ç°è‡ªå®šä¹‰ CRD](http://www.asznl.com/post/43)

+ [sample-controller](https://itnext.io/building-an-operator-for-kubernetes-with-the-sample-controller-b4204be9ad56)

> + è¿™æ˜¯ç¬¬ä¸€ç¯‡æ–‡ç« ï¼Œå°†æ¢è®¨sample-controllerã€‚
> + [æœ¬ç³»åˆ—çš„ç¬¬äºŒç¯‡æ–‡ç« ](https://medium.com/p/17cbd3f07761)å°†æ¢ç´¢kubebuilderã€‚
> + [æœ¬ç³»åˆ—çš„ç¬¬ä¸‰ç¯‡æ–‡ç« ](https://medium.com/p/40a029ea056)å°†æ¢è®¨operator-sdkã€‚
